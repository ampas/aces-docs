{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#what-is-aces","title":"What is ACES","text":"<p>The Academy Color Encoding System (ACES) is an industry standard for managing color and digital files throughout the life cycle of most any media production, from motion picture, to television, video game, or immersive storytelling project. From image capture through editing, VFX, mastering, public presentation, archiving and future remastering, ACES ensures a consistent color experience that preserves the creators\u2019 vision. It also \u201cfuture proofs\u201d your project and simplifies the creation of the many deliverables that are required of today\u2019s productions.</p>"},{"location":"#why-use-aces","title":"Why use ACES?","text":""},{"location":"#how-to-use-aces","title":"How to use ACES?","text":""},{"location":"#contribute-to-aces","title":"Contribute to ACES","text":""},{"location":"glossary/","title":"Terms and Definitions","text":""},{"location":"glossary/#a","title":"A","text":"Academy Color Encoding Specification (ACES) RGB color encoding for exchange of image data that have not been color rendered, between and throughout production and postproduction, within the Academy Color Encoding System. ACES is specified in SMPTE ST 2065-1. ACES RGB relative exposure values Relative responses to light of the ACES Reference Image Capture Device, determined by the integrated spectral responsivities of its color channels and the spectral radiances of scene stimuli. ACES unity neutral A triplet of ACES RGB relative exposure values all of which have unity magnitude. ACES Metadata File (AMF) Metadata \u201csidecar\u201d XML-based file that contains information describing a collection of image files color-managed using the Academy Color Encoding System (ACES). ACES Encodings Color encoding specifications specified as part of the Academy Color Encoding System, e.g., ACES2065-1, ACEScc, etc. ACES File Formats Digital data containers specified as part of the Academy Color Encoding System, e.g., ACES Metadata Files, ACES Image Container (SMPTE ST2065-4), etc. ACES Product Partners Companies that integrate ACES concepts and components into their products and/or services. ACES System Complete set of components that comprise the Academy Color Encoding System. ACES System Release Published ACES System. ACES Transforms Color transformations specified as part of the Academy Color Encoding System, e.g., Reference Rendering Transform (RRT), Output Device Transforms (ODT), etc. ACES Viewing Transform Combined RRT and ACES Output Device Transform. American Society of Cinematographers Color Decision List ASC CDL A set of file formats for the exchange of basic primary color grading information between equipment and software from different manufacturers. ASC CDL provides for Slope, Offset and Power operations applied to each of the red, green and blue channels and for an overall Saturation operation affecting all three."},{"location":"glossary/#b","title":"B","text":""},{"location":"glossary/#c","title":"C","text":"chromatic adaptation process by which the visual mechanism adjusts in response to the radiant energy to which the eyes are exposed. chromaticity property of a color stimulus defined by the ratios of each tristimulus value of the color stimulus to their sum. Color Transform Langage (CTL) small open-source programming language, consisting of an interpreter and one or more CLT modules, that has been designed to serve as a building block for digital color management systems. CTL modules (files) files containing Color Transformation Language code. Note CTL serves as the reference implmentation of ACES, hence, CTL modules are the primary documentation for ACES transforms.  If any written documentation conflicts with a CTL module, the CTL module should be considered correct.  If you believe the CTL module contains a bug, please file a bug report on github."},{"location":"glossary/#d","title":"D","text":"DateTime (reference: ISO 8601:2004) timestamp format The DateTime is specified in the following form <code>YYYY-MM-DDThh:mm:ss{offset}</code> where: <ul> <li><code>YYYY</code> indicates the year</li> </ul> <ul> <li><code>MM</code> indicates the month</li> </ul> <ul> <li><code>DD</code> indicates the day</li> </ul> <ul> <li><code>T</code> indicates the start of the required time section </li> </ul> <ul> <li><code>hh</code> indicates the hour</li> </ul> <ul> <li><code>mm</code> indicates the minute</li> </ul> <ul> <li><code>ss</code> indicates the second</li> </ul> <ul> <li><code>{offset}</code> time zone offset from UTC</li> </ul> Note <p>All components are required.</p> Example <p><code>2014-11-20T12:24:13-8:00</code></p>"},{"location":"glossary/#e","title":"E","text":"Edit Decision List (EDL) list used in the post-production process of film editing and video editing containing an ordered sequence of reel and timecode data representing where each video clip can be obtained in order to conform to the final cut. Extensible Markup Language (XML) a markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable"},{"location":"glossary/#f","title":"F","text":""},{"location":"glossary/#g","title":"G","text":""},{"location":"glossary/#h","title":"H","text":""},{"location":"glossary/#i","title":"I","text":"Implementation Transforms ACES System transforms implemented by ACES Product Partners, likely as a Color Look-up Table or as GPU or CPU code. Internet Engineering Task Force (IETF) an open standards organization, which develops and promotes voluntary Internet standards, in particular the technical standards that comprise the Internet protocol suite (TCP/IP)."},{"location":"glossary/#j","title":"J","text":""},{"location":"glossary/#k","title":"K","text":""},{"location":"glossary/#l","title":"L","text":"Look Transform (LMT) Look Modification Transform (deprecated term) ACES transform which applies a global look or other modification to scene-referred image data."},{"location":"glossary/#m","title":"M","text":""},{"location":"glossary/#n","title":"N","text":""},{"location":"glossary/#o","title":"O","text":""},{"location":"glossary/#p","title":"P","text":""},{"location":"glossary/#q","title":"Q","text":""},{"location":"glossary/#r","title":"R","text":"Reference Gamut Compression (RGC) ACES LMT which compresses scene-referred image data to within the AP1 gamut. Reference Rendering Transform (RRT) Core ACES transform that converts scene-referred image data that conforms to SMPTE ST 2065-1:2012 to output-referred image data. RFN technical specification or organizational note published by the Internet Engineering Task Force (IETF)"},{"location":"glossary/#s","title":"S","text":""},{"location":"glossary/#t","title":"T","text":"TransformIDs Transform Identifiers string identifying the ACES transform.   Note Please see the ACES System Versioning Specification for more information on the format to use for TransformIDs."},{"location":"glossary/#u","title":"U","text":"Universal(ly) Unique Identifier (UUID) 128-bit label used for information in computer systems published in various standards including ISO/IEC 11578:1996 \"Information technology \u2013 Open Systems Interconnection \u2013 Remote Procedure Call (RPC)\""},{"location":"glossary/#v","title":"V","text":""},{"location":"glossary/#w","title":"W","text":""},{"location":"glossary/#x","title":"X","text":""},{"location":"glossary/#y","title":"Y","text":""},{"location":"glossary/#z","title":"Z","text":""},{"location":"guides/amf/","title":"ACES Metadata File Implementation Guidelines and Best Practices","text":""},{"location":"guides/amf/#scope","title":"Scope","text":"<p>This document is a guide that recommends implementation guidelines and best practices related to the usage of the ACES Metadata File (AMF) in various workflows. These workflows may involve one or more tools that support the AMF specification and this guide attempts to help both implementers and users in order to facilitate interoperability.</p>"},{"location":"guides/amf/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>Academy S-2019-001, ACES Metadata File (AMF)</li> <li>IETF RFC 4122, A Universally Unique IDentifier (UUID) URN Namespace</li> </ul>"},{"location":"guides/amf/#introduction","title":"Introduction","text":"<p>The Academy Color Encoding System (ACES) is a color processing framework that enables the mix of various sources within a standardized color space in order to produce one or more outputs.</p> <p>While ACES is a living framework and is actively developed and adopted, it also comes with various points that can be configured. These points of configuration are either related to the sources used (Input Transforms), a creative look (Look Transforms), the desired outputs (Output Transforms), or the Version Number (i.e. ACES v1.1) of the core transforms built into the ACES system.</p> <p>ACES does not specify these configuration points directly or associate them with actual images or shots during production, and this is the very reason why AMF exists.</p> <p>AMF is the configuration file that allows a precise setup for an ACES pipeline. Besides this basic goal, AMF is also the tool of choice to transmit and exchange configuration parameters in order to ensure consistency within a workflow and across the entire ecosystem of tools that are used within that workflow.</p>"},{"location":"guides/amf/#target-audience","title":"Target Audience","text":"<p>AMF is a sidecar file specified using the XML markup language, and as such it can be processed by machines and at the same time created/modified by users.</p> <p>This document targets both AMF users and AMF implementers because both groups need the same level of understanding in order to design AMF-enabled workflows and tools that support those workflows.</p>"},{"location":"guides/amf/#what-is-amf","title":"What is AMF","text":"<p>AMF is an XML specification that describes the configuration of an ACES color pipeline, together with the various input transforms, look transforms and output transforms.</p> <p>AMF is a \"sidecar\" element, usually accompanying some visual material such as a video file, a sequence of frames, or a whole timeline. In the case of a timeline, more than one AMF file can be used if the timeline requires different configurations of the ACES pipeline. It is also worth mentioning that several AMF files can reference the same visual material.</p> <p>The opposite is equally true as all these visual elements can share a single AMF file or a whole set of them. This of course is entirely dependent on the workflow, and tools implementing AMF should be prepared to deal with this flexibility.</p> <p>In general, the relationship between the visual elements and the AMF files can be described as a \"many to many\" relationship.</p>"},{"location":"guides/amf/#why-is-amf-needed","title":"Why is AMF needed","text":"<p>The ACES framework is expanding and becoming richer in terms of input, look, and output transforms. AMF describes the exact list of these different transforms, in the order in which they have been or should be applied to obtain the desired result.</p> <p><pre><code>graph LR\nA1[Input Media] --&gt; B(Input Transform)\nsubgraph AMF Complete Processing Path Description\n    B --&gt; C1(Look Transform)\n    C1 --&gt; D1(Output Transform)\nend\nA2[ACES Material] ---&gt; C2(Look Transform)\nsubgraph AMF Partial Processing Path Description\n    C2 --&gt; D2(Output Transform)\nend</code></pre> AMF Processing Path Description </p> <p>This is a powerful feature because it can describe both configurations that must be used to create a specific output, or configurations that have been used to create a specific output.</p> <pre><code>graph TB\nB1  --&gt; A1(AMF &lt;br/&gt;Look Modification Transform &lt;br/&gt; Chain)\nB2  --&gt; A1\nB3  --&gt; A1\nsubgraph  \n    B1(\"ASC-CDL&lt;br/&gt;or&lt;br/&gt;External LMT&lt;br/&gt;(active)\") \n    B2(\"ASC-CDL&lt;br/&gt;or&lt;br/&gt;External LMT&lt;br/&gt;(disabled)\")\n    B3(\"ASC-CDL&lt;br/&gt;or&lt;br/&gt;External LMT&lt;br/&gt;(active)\")\nend\n\nclassDef disabled opacity: 0.5\nclass B2 disabled</code></pre> Each block in the chain can be an ASC-CDL or an external LUT.   Blocks can be enabled or disabled <p>Finally, another feature of AMF is the ability to document a \"change log\" in an ACES color pipeline. This is called the \"archived pipeline\" and will be discussed later in the document.</p>"},{"location":"guides/amf/#the-applied-attribute","title":"The \u201capplied\u201d attribute","text":"<p>Each transform in an AMF can be tagged with the attribute called <code>applied</code> - which indicates whether a transform has already been applied (<code>applied=true</code>), in which case the transform has already been baked into the image, or if the transform has not been applied (<code>applied=false</code>), in which case the transform should be loaded as part of the viewing pipeline.</p> <p>One use case of this might be when using the ACES Gamut Compression transform, which may be baked into SMPTE ST 2065-1 ACES image data, and it is essential to communicate to downstream software that it has already been applied, as to not double-apply the transform, or invert it if necessary.</p>"},{"location":"guides/amf/#lifecycle-of-amf","title":"Lifecycle of AMF","text":"<p>This section describes the life cycle of an AMF and how it could be used within each production stage.</p>"},{"location":"guides/amf/#camera","title":"Camera","text":"<p>While on set, AMF could be imported in-camera and used to apply color pipeline settings for video and file output, or exported to a camera card when using a camera\u2019s ACES viewing pipeline. See section 7 for more on cameras reading/writing AMF.</p>"},{"location":"guides/amf/#monitor","title":"Monitor","text":"<p>Some professional monitors allow import of LUTs to apply looks in-device. AMF could replace proprietary or uncommon LUT formats for improved interoperability.</p>"},{"location":"guides/amf/#on-set-live-grading","title":"On-set live grading","text":"<p>An AMF may be read by on-set live grading software for the purpose of on-set monitoring and color grading within ACES.</p> <p>If anything is altered within the domain of the pipeline defined in the AMF, a new AMF is created to reflect those modifications. For example, an ACES pipeline is established in an AMF before production, then CDL adjustments are created during production to create an updated AMF accordingly.</p>"},{"location":"guides/amf/#dailies","title":"Dailies","text":"<p>In a dailies tool, a pre-created AMF could be read and associated with OCF (Original Camera Files) to apply pipeline settings (for viewing and rendering). This could be done either by manual association or automatically.</p> <p>In the process of creating the dailies, the color pipeline coming from an existing AMF may be modified and updated. AMFs are written out with media to be passed to editorial software. Commonly used interchange files (e.g. EDL or ALE) can be used to conform AMF files with OCF, see below for more details.</p>"},{"location":"guides/amf/#editorial","title":"Editorial","text":"<p>Editorial software can apply pipeline settings provided by AMF(s) when importing media to automatically set up viewing and rendering.</p>"},{"location":"guides/amf/#vfx","title":"VFX","text":"<p>Read AMF(s) when importing plates into VFX software and apply pipeline settings for viewing. Given the prevalence of OpenColorIO across VFX software, it is likely that a translation from AMF to OpenColorIO (OCIO) would be required.</p>"},{"location":"guides/amf/#color-grading","title":"Color Grading","text":"<p>When in color grading, AMF could be conformed to a timeline and associated with OCF to apply pipeline settings (for viewing and rendering). These applications should also allow for look development in ACES and subsequent exporting of AMF.</p>"},{"location":"guides/amf/#review","title":"Review","text":"<p>Review software could automatically apply ACES pipeline settings for viewing purposes by reading AMF(s) when importing media or by manually applying AMF(s) to imported media.</p>"},{"location":"guides/amf/#mastering-and-archiving","title":"Mastering and Archiving","text":"<p>Read AMF(s) when importing media and apply pipeline settings for viewing.</p> <p>Consolidate AMF(s) to meet specific archival delivery requirements.</p>"},{"location":"guides/amf/#considerations-on-readingwriting-amf","title":"Considerations on reading/writing AMF","text":"<p>This section outlines various scenarios related to the reading and writing of AMF files.</p> Scenario Read - An AMF is read when importing media and used to populate a color pipeline for viewing and rendering. Write - A new AMF is written in order to be passed along to the next production stage. If a new AMF is done from a previous AMF, the previous pipeline might be archived in the  section. RAW Clips AMF does not include any metadata for demosaic settings. Implementations need to ensure that the image is demosaicked to the appropriate color space before the Input Transform defined in the AMF is applied. User input may be required.If software chooses to directly demosaic a RAW image to ACES, the Input Transform defined in the AMF must be ignored. n/a Input Transform Conflict A clip has already been loaded into the software, and an Input Transform is already applied. Default behaviour should be to override that Input Transform with what\u2019s specified in the AMF, but the user should be prompted. n/a Output Transform Conflict If the AMF specifies an Output Transform that is in conflict with the respective shot\u2019s Output Transform, then this conflict must be handled. The default behaviour should be to stick with the project-wide Output Transform, but it may be useful to indicate a conflict to the user.Example: An AMF is generated from a software platform that uses the Rec709_100nits transform, and is then read by a software platform that is using an HDR Output Transform. The Output Transform that was indeed used for viewing should be specified in the AMF. Manual AMF Batch Import/Export Consider the use of commonly used interchange files (e.g. EDL or ALE) to batch import AMF\u2019s to a timeline.Ideally, the software would allow for a partial import of only the Input, Look, or Output Transforms of a given AMF. When exporting a batch of AMF\u2019s for an entire timeline, consider exporting commonly used interchange files (e.g. EDL or ALE) to create an association between Clips and the exported AMF files. Inter-AMF Output Transform Conflict If AMF\u2019s are batch imported into a single timeline, and at least two of them have different Output Transforms defined, consider prompting the user if this inconsistency exists, and provide appropriate options to address. This has the assumption that the user will only be using one Output Transform at a time for an entire timeline. n/a Pipeline Override If an AMF has already been applied to a shot, or if project/timeline settings apply, and a subsequent AMF is read for that shot, consider prompting the user before overriding pipeline.Ideally, the software would allow for a partial import of only the Input, Look, or Output Transforms of a given AMF. Consider including multiple AMF transform pipelines by making use of AMF\u2019s <code>aces:archivedPipeline</code> element. Updating Look Transforms n/a Local changes to transforms in the AMF should not affect any other transforms if the pipeline structure doesn\u2019t change, e.g. when only updating CDL values, but none of the other transforms change, the AMF structure should not be changed and only the CDL values should be updated. Any changes to the transform pipeline will result in a new AMF and global values, e.g. the <code>dateTime</code> element, are therefore expected to change. If CLF(s) are used in the pipeline, consider using CLF UUID and/or its hash to make sure that the CLF has not changed. Look Transform Support If an AMF references a Look Transform with a format that the application does not support reading, consider notifying the user that the format is unsupported, so it is clear the error is unrelated to the AMF itself. When producing a new Look Transform for an AMF export, consider defaulting to CLF, a format that ensures high interoperability,, especially for any operations other than ASC CDL. It\u2019s important to consider what to do when using CDL operations vs other grading operations and how these should be reflected in the AMF document: in elaborated pipeline where CDL are \u201cin between\u201d other more sophisticated grading operations, it might be required to let the user identify and decide over what CDL operations should be treated as such and which ones can be baked with other operations into a consolidated CLF Cameras Import AMF in-camera and apply pipeline settings for video and file output.An AMF loaded in camera could specify over SDI how to treat the incoming signal (ie Output Transform)There are reasonable expectations that any in-camera processing, for the foreseeable future, will be done utilizing small 3D LUTs at the highest complexity. Therefore, applications of an ACES pipeline in-camera may be limited in precision. When should a camera generate an AMF?If a camera generates an AMF, where should it be written?#1 Preferred Method: Embedded in the OCF, e.g.REDCODE RAW R3D #2 Preferred Method: AMF should live in the same directory as its associated clip#3 Preferred Method:* A single folder with all AMF files Metadata Population Parse the AMF for its filename and <code>aces:uuid</code> and write these to the appropriate metadata fields for each clip. If the AMF associated with a clip changes, the value relative to the AMF metadata fields within the editorial software should change and adopt the new values. So when writingcommonly used interchange files (e.g. EDL or ALE) the correspondent values are correct. Applied Tag When reading an AMF file that has the <code>applied=true</code> attribute for a specific transform, the software should NOT apply the transform to the file, since it has already been applied to the image itself. Consider reporting it to the user if applicable (e.g. a \u201chistory\u201d log of the transforms is accessible for each clip) When exporting AMF\u2019s from a timeline of clips that have not been rendered yet, each transform in the AMF should be tagged as <code>applied=false</code>.However, when rendering new files, consider having the ability to export new AMF\u2019s files simultaneously as part of the same deliverable and, in this case, each transform that is actually baked in should be tagged as <code>applied=true</code> in the AMF (e.g. the Input Transform if rendering OpenEXR ACES 2065-1 VFX pulls, or everything when exporting 709 proxies for editorial). Archived Pipelines Consider allowing the user to toggle between different ACES pipelines that are recorded in the <code>aces:archivedPipeline</code> element.Otherwise, <code>aces:archivedPipeline</code> elements should be preserved for any new AMF\u2019s subsequently created for the same shots. If the software is updating a pre-existing AMF, the written AMF should include the appropriate <code>aces:archivedPipeline</code> element."},{"location":"guides/amf/#structure-of-amf","title":"Structure of AMF","text":"<p>AMF is a specification based on the XML markup language. It is fully described in Academy Specification S-2019-001. The specification also comes with an XML Schema that can be used to validate the AMF documents.</p> <p>The XML Schema is publicly available here: https://github.com/ampas/aces-dev/tree/master/formats/amf</p> <p>The guidelines and best practices in this document are provided to help both implementers and users to take full advantage of AMF.</p> <p>It is strongly recommended to use the specification as a reference in order to better understand the concepts described here.</p>"},{"location":"guides/amf/#amf-document-sections","title":"AMF document sections","text":"<p>AMF documents are mainly divided in 3 sections:</p> <ol> <li><code>aces:amfInfo</code> - this section provides descriptive information about the AMF     itself. </li> <li><code>aces:clipId</code> - this section provides a reference to the visual material     (OCF or rendered images) that this AMF is applicable for. </li> <li><code>aces:pipeline</code> - this section describes the ACES pipeline configuration.</li> </ol>"},{"location":"guides/amf/#general-descriptive-information","title":"General descriptive information","text":"<p>The <code>aces:amfInfo</code> element contains various sub-elements that provide descriptive information about the AMF document but also a mechanism to help identification. More specifically two sub-elements deserve some consideration:</p> <ul> <li><code>aces:dateTime</code> </li> <li><code>aces:uuid</code></li> </ul> <p>The mandatory <code>aces:dateTime</code> element contains the creation and modification date. The <code>aces:uuid</code> element is optional and is designed to carry a Universally Unique Identifier (also known as Globally Unique Identifier, or GUID on some systems). The Universally Unique Identifier is specified in IETF RFC 4122 as well as other ISO and ITU documents.</p> <p>Both <code>aces:dateTime</code> and <code>aces:uuid</code> elements are not filled in by a human operator but rather automatically generated by the tool used to create the AMF document.</p>"},{"location":"guides/amf/#amf-naming-and-identification","title":"AMF naming and identification","text":"<p>In general, the most common method that everyone uses to distinguish between two files is by comparing file names and/or their creation and/or modification date in the file system. However, this method quickly reveals itself ineffective when files are exchanged between various computers and operating systems because these file properties can easily be changed without any sort of warning.</p> <p>As explained above, AMF files usually come in large numbers and are moved across various systems and processed by various tools during their life cycle. Because of this situation, a better approach is to make good use of the information contained in the document itself.</p> <p>However, to avoid common pitfalls like overwriting files, the following file naming convention is recommended:</p> <p>AMF files should conform to the following file naming convention:</p> <p><code>&lt;description&gt;_&lt;date&gt;_&lt;time&gt;.amf</code></p> <p><code>&lt;description&gt;</code> should describe the following, if applicable:</p> <ul> <li>Purpose: the use case of the AMF file (eg. \u201cdailies\u201d, \u201cSDR_709\u201d, \u201cVFX-Pull\u201d) </li> <li>Clip: Clip ID as in the AMF specification </li> <li>Show Name: Title or other identifiers of the associated show </li> <li>Author: Author of the AMF</li> </ul> <p><code>&lt;date&gt;</code> is the date of creation, using the format YYYY-MM-DD <code>&lt;time&gt;</code> is the time of creation, using the format HHMMSSZ (trailing \u201cZ\u201d indicating \u201cZulu\u201d time, see below)</p> <p>Values for <code>&lt;date&gt;</code> and <code>&lt;time&gt;</code> are determined at the start of the operation that results in the creation of the AMF file and the values are represented using the Coordinated Universal Time (UTC) standard.</p> <p>Example: <code>Dailies_ShowName_A002R2EC_2019-01-07_080228Z.amf</code></p>"},{"location":"guides/amf/#using-date-and-time-mechanism","title":"Using date and time mechanism","text":"<p>As mentioned above, the <code>aces:dateTime</code> is a mandatory element and it is defined using the standard XML type <code>xs:dateTime.</code> Because this definition is very flexible, it is strongly recommended for the tools to always use the most explicit form that includes the year, month and day, the time of the day in hours, minutes and seconds as well as the time zone. This practice ensures that the creation and modification dates and times are giving a good indication on the location where the document was created/modified.</p>"},{"location":"guides/amf/#using-the-unique-identifier-mechanism","title":"Using the unique identifier mechanism","text":"<p>A more elaborate identification mechanism can also be used, by taking advantage of the <code>aces:uuid</code> element. Since this element is optional, one cannot count on its presence, however it is strongly recommended to use it. When doing so, the UUID becomes a much safer tool to distinguish between to AMF documents. UUIDs are automatically generated and they shall never be hand-crafted.</p>"},{"location":"guides/amf/#combining-several-identification-mechanisms","title":"Combining several identification mechanisms","text":"<p>In order to improve the identification mechanism, one can combine both the UUID checks and creation/modification times. This might be helpful if two AMF documents contain the same UUID but have different creation/modification dates.</p> <p>In practice, when using dedicated tools to create and manage AMF files, such situations should not occur, but AMF files can still be manually altered. If this is the case, further inspection of the AMF documents can help to distinguish them. Such advanced methods will be discussed later in the document.</p> <p>It is worth mentioning here that there are situations where two or more AMF documents can have the same unique identifier but have different creation dates and time. It is then recommended that tools encountering this situation switch to the most recent version of the AMF document based on the date and time.</p>"},{"location":"guides/amf/#informing-the-user-and-logging-conflicts","title":"Informing the user and logging conflicts","text":"<p>Because of the large number of AMF documents involved in a workflow, it might not be practical to inform the user of every error encountered. However these errors should be logged by the tools using AMF and options should be offered to select the various identification rules, e.g. unique identifier first (if available), then the creation date and time.</p>"},{"location":"guides/amf/#clip-information-and-association","title":"Clip information and association","text":"<p>As described in the previous sections, AMF can be used with different targets, i.e. single file video clips, image sequences, compositions, etc.</p> <p>This flexibility implies that the AMF specification does not prescribe a specific way to create the connection with the target material. Instead, the specification offers different connection mechanisms via the <code>aces:clipId,</code> an optional structure that in turn contains child elements to help with the handling of the various situations.</p> <p>The first important observation to make is that the <code>aces:clipId</code> element itself is defined as optional. In this context, optional does not mean that the presence or absence of the <code>aces:clipId</code> element does not affect the workflow and how tools that support AMF behave. The term optional must be understood as a switch between two categories of workflow: the first does not connect an AMF file to a specific visual material and the second does connect an AMF file to a specific visual material.</p> <p>Depending on the workflow in use, an implementation must handle the presence or absence correctly and report errors if necessary. Typically, the XML validation only will not be enough to distinguish between a valid AMF file and an invalid one, since the <code>aces:clipId</code> element is optional.</p> <p>In other words, the <code>aces:clipId</code> does not dictate how the AMF document is handled. It is the workflow that dictates the behavior.</p>"},{"location":"guides/amf/#acesclipid-is-not-present","title":"<code>aces:clipId</code> is not present","text":"<p>The absence of the <code>aces:clipId</code> element is important when the connection between the AMF document and the visual material is handled by a higher level protocol.</p> <p>The simplest higher level protocol that comes immediately to mind is the use of the file system and some sort of naming convention. For instance, a folder can contain a video clip and the related AMF file like this:</p> <pre><code>./myVideoClip.mxf \n./myVideoClip.amf\n</code></pre> <p>In this simple situation, an implementation that can read the the myVideoClip file could also look for a secondary file named myVideoClip.amf and if it is present and if it is a valid AMF document consider that there is a \"connection\" between the two files and act accordingly.</p> <p>While this seems to be a natural thing to do, it is certainly something to avoid. First of all, this kind of \"connection\" would work in a limited number of situations and then it would also prevent more elaborated workflows from existing. </p> <p>Consider the following modified example:</p> <p><pre><code>./myVideoClip.mxf \n./myVideoClip.mov \n./myVideoClip.amf\n</code></pre> In this variant, it's impossible to guess if myVideoClip.amf is related to myVideoClip.mxf or to myVideoClip.mov or to both files.</p> <p>To solve this problem, the <code>aces:clipId</code> element must be used to establish the desired connection between the AMF document and the correct targeted visual material.</p> <p>A single AMF document can be \"shared\" by multiple video clips or image sequences or even compositions. While it's certainly possible to invent a solution based on the file system naming capabilities via a fixed folder/file structure and naming convention, it is not recommended.</p> <p>In practice, workflows that involve multiple visual material elements, and one or more AMF documents, shared or not, make use of a control file that acts like a database, describing the complex relationships that may exist.</p> <p>This handbook defines the use of AMF in conjunction with some popular commonly used interchange files:</p> <ol> <li>Avid Log Exchange (ALE)</li> <li>CMX3600 Edit Decision List (EDL)</li> </ol> <p>The AMF Implementation Group explored the use of AMF with higher level protocols as well and those will eventually be described in a future version of this handbook.  A future version may also describe the use of AMF with OpenTimelineIO.</p>"},{"location":"guides/amf/#acesclipid-is-present","title":"<code>aces:clipId</code> is present","text":"<p>As briefly described before, the <code>aces:clipId</code> is a complex element, containing the following sub-elements:</p> <ul> <li> <p><code>aces:clipName</code>  and one of the following:</p> </li> <li> <p><code>aces:file</code> </p> </li> <li><code>aces:sequence</code> </li> <li><code>aces:uuid</code>  All these sub-elements are mandatory when <code>aces:clipId</code> is used, but it's important to remember that <code>aces:sequence,</code> aces:file and <code>aces:uuid</code> cannot coexist. They are mutually excluding each other and therefore are used for specific variants in a workflow.</li> </ul>"},{"location":"guides/amf/#acesclipname","title":"<code>aces:clipName</code>","text":"<p>The <code>aces:clipName</code> is used to carry the name of the target visual material element, but not the file name of that element. Typically <code>aces:clipName</code> is the same as the file name but without the file extension or the frame number digits in the case of a file sequence.</p>"},{"location":"guides/amf/#acesfile","title":"<code>aces:file</code>","text":"<p>The <code>aces:file</code> element is used to carry the actual file name of the target visual material element. It can carry the full absolute path and the file name, a relative path and the file name or simply the file name (base name and extension) of the target visual material element.</p> <p>As it is the case with file names in general, path information and special characters supported or not supported by various file systems must be taken into account. The goal here is not to describe all the possibilities, but rather to recommend some best practices: * If the path (absolute or relative) is used in the file name, it should be limited to cases when the AMF document is only used within a closed system where the rules can be clearly defined. * Special characters or Unicode names can be used, but in general they might be a source of problems. While not forbidden, their use should be tested in the context of the desired workflow to ensure that all the tools and operating systems involved correctly support the selected convention.</p> <p>A good practice however would be to stick with ASCII characters only and avoid using path-like structures in file names.</p>"},{"location":"guides/amf/#acessequence","title":"<code>aces:sequence</code>","text":"<p>The <code>aces:sequence</code> is similar to <code>aces:file,</code> however it is primarily designed to handle image sequences. Image sequences usually follow a file name pattern and the only difference between two files of the same sequence, is a number which indicates the file's position in the sequence. Moreover, the number is using a fixed number of digits where the unused digits are replaced with zeroes. <code>aces:sequence</code>  requires three different attributes to fully define a sequence of files:</p> <ul> <li><code>idx</code>: a special character that the filename pattern uses to represent digits (e.g.#) </li> <li><code>min</code>: a number that represents the first file in the sequence </li> <li><code>max</code>: a number that represents the last file in the sequence</li> </ul> <p>In other words, min and max define a range of frame numbers and they are both part of the sequence (included).</p>"},{"location":"guides/amf/#acesuuid","title":"<code>aces:uuid</code>","text":"<p>The last method for connecting the AMF document to a visual material element is by using <code>aces:uuid.</code> In this particular case, the connection between the AMF document and the actual visual material element is clearly handled elsewhere and not at the file system level. Various workflows will be described later that make use of the <code>aces:uuid</code> instead of <code>aces:file</code> or <code>aces:sequence.</code> However it's important to note that using UUID is probably the safest method, especially when the workflow is distributed across multiple tools, operating systems and even geographic locations.</p>"},{"location":"guides/amf/#aces-pipeline-configuration","title":"ACES pipeline configuration","text":"<p>The ACES pipeline section is a list of ordered elements that define various steps of the ACES color pipeline. The pipeline is described by the <code>aces:pipeline</code> element. In turn, this element contains a list of sub-elements that describe the configuration of the various color processing stages that exist in the ACES color processing framework. Below is the list of sub-elements that can be found in the <code>aces:pipeline</code> element:</p> <ul> <li><code>aces:pipelineInfo</code> </li> <li><code>aces:inputTransform</code> </li> <li><code>aces:lookTransform</code> </li> <li><code>aces:outputTransform</code> </li> </ul> <p>These elements must appear in this exact order.</p> <p>Although these steps are described separately, this does not imply that a system has to process all pixels in a frame of visual material one step at a time. Some systems might do it while some others might need to crunch the various processing steps into a single transform, typically a 3D Lookup Table (3D LUT). Moreover a system may choose to optimize the processing of the various steps, depending on the given situation. The only constraint is that the color processing must follow the steps in the order described above.</p>"},{"location":"guides/amf/#acespipelineinfo","title":"<code>aces:pipelineInfo</code>","text":"<p>The <code>aces:pipelineInfo</code> element extends the set of properties found in the AMF document identification by adding an element to define the ACES system version.</p> <p>The role of this element is to specify the ACES system version targeted by this AMF file in order to produce the correct output. The system version is a crucial piece of information as it allows us to achieve interoperability and archivability.</p> <p>The <code>aces:pipelineInfo</code> element can (and should) be used to validate the AMF document itself. The following sections that describe the use of the input transforms, look transforms and output transforms mention the use of transform identifiers. Transform identifiers are also \"tagged\" with the ACES system version to ensure a match between the pipeline system version and the various transform identifiers.</p> <p>The validity of transform identifiers within the scope of a given ACES system version will be described later in a dedicated section.</p>"},{"location":"guides/amf/#acesinputtransform","title":"<code>aces:inputTransform</code>","text":"<p>This element defines the input transform that is optionally applied to the source material referenced by <code>aces:clipId.</code>  The input transforms can be either standard transforms defined within the ACES framework or custom transforms.</p> <p>Custom transforms can be referenced by their transform ID or referenced as external files/resources.</p> <p>Standard transforms can only be referenced by their transform ID.</p> <pre><code>graph BT\nB1(ACES Input Transform Identifier)   --&gt; A1(AMF Input Transform)\nB2(Custom Input Transform Identifier) --&gt; A1\nB3(\"External Input Transform (e.g. CLF)\")  --&gt; A1</code></pre> AMF Input Transform SupportAMF can describe one Input Transform as an identifieror external reference <p>An important observation must be made here: the <code>aces:inputTransform</code> is entirely optional. This implies that an AMF document can work in different environments, i.e with sources made of raw material, color pre-processed material and ACES only material. These different use cases will be discussed later in this document.</p> <p>If an <code>aces:inputTransform</code> element is present, then it must also define the \"applied\" attribute that will allow an AMF-aware tool to know if the input transform is provided for information purposes only or if it needs to be executed.</p>"},{"location":"guides/amf/#aceslooktransform","title":"<code>aces:lookTransform</code>","text":"<p>This element is repeated for every step that defines a custom color processing in the ACES color space (e.g. color grading). There are 3 kinds of look transforms:</p> <ol> <li>Standard transforms defined within the ACES framework (such as Gamut Compression) \u25cb Standard transforms can only be referenced by their transform id.</li> <li>Embedded ASC-CDL transforms<ul> <li>Embedded ASC-CDL transforms carry their parameters within the file and do not rely on any external information.</li> </ul> </li> <li>External transforms stored in various formats (ASC CDL XML, CLF, etc)<ul> <li>External transforms can be referenced by either a unique ID or by a file name, described later in this document.  <code>aces:lookTransform</code> elements are optional, and therefore AMF documents do not mandate any color processing beyond the processing provided by the ACES color processing framework.</li> </ul> </li> </ol> <p>If an <code>aces:lookTransform</code> element is present, then it must also define the \"applied\" attribute that will allow an AMF-aware tool to know if the look transform is provided for information purposes only or if it needs to be executed.</p>"},{"location":"guides/amf/#acesoutputtransform","title":"<code>aces:outputTransform</code>","text":"<p>Finally, this element closes the list and defines both the RRT and ODT (or a combined Output Transform) to use in order to produce a presentable result.</p> <p>The RRT and ODT can be either specified independently of each other:</p> <pre><code>&lt;aces:outputTransform&gt; \n    &lt;aces:referenceRenderingTransform&gt;\n        &lt;aces:transformId&gt;urn:ampas:aces:transformId:v1.5:RRT.a1.0.3&lt;/aces:transformId&gt;\n    &lt;/aces:referenceRenderingTransform&gt;\n    &lt;aces:outputDeviceTransform&gt;\n        &lt;aces:transformId&gt;urn:ampas:aces:transformId:v1.5:ODT.Academy.P3D60_48nits.a1.0.3&lt;/aces:transformId&gt;\n    &lt;/aces:outputDeviceTransform&gt;\n&lt;/aces:outputTransform&gt;\n</code></pre> <p>or combined:</p> <pre><code>&lt;aces:outputTransform&gt;\n&lt;aces:transformId&gt;urn:ampas:aces:transformId:v1.5:RRTODT.Academy.Rec2020_1000nits_15nits_ST2084.a1.1.0&lt;/aces:transformId&gt;\n&lt;/aces:outputTransform&gt;\n</code></pre> <p>The RRT and ODT (as well as the combined versions) are standard color transforms defined within the ACES framework. </p>"},{"location":"guides/amf/#amf-external-lmt-referencing-rules","title":"AMF &amp; external LMT referencing rules","text":""},{"location":"guides/amf/#using-acesfile","title":"Using <code>&lt;aces:file&gt;</code>","text":"<p>The simplest way to reference external LMTs is to use the <code>aces:file</code> element. However, some care must be taken, depending on the workflow and also on the system or device generating the AMF document.</p> <p>The <code>aces:file</code> element is defined as an XML standard type called <code>xs:anyURI.</code> This type allows a very large set of possibilities by using the Uniform Resource Identifier (URI) syntax: file access on a local or remote file system, HTTP or FTP access and much more. All of these possibilities are identified by a scheme, which is a predefined syntax to allow unambiguous interpretation of the URI. Although there are situations where this might not be possible.This document will mainly focus on the file access on computer file systems or embedded file systems (e.g. in-camera).</p> <p>File access is accomplished by the use of the file:// scheme as a prefix to the file location. It is assumed that in a file system centric workflow, the omission of the file:// scheme means that the URI is the actual file name of the external resource, i.e. the LMT. This is probably the most common use.</p> <p>Resolving the file location by the means of the file name may still be problematic, especially because of how various file systems identify disks or volumes. In order to simplify the file name resolution, the following rules are recommended:</p> <ol> <li>Avoid the use absolute file names, i.e. file names that contain the full path from the root of the disk or volume</li> <li>Avoid using external LMTs in folders that exist at a higher level in the file system hierarchy than the location of the AMF document</li> <li>Avoid the use of \"current path\" and \"one level up\" path segments as they might not be interpreted correctly by the systems and/or devices that need to work with the AMF document and its externally referenced resources.</li> </ol> <p>Example:</p> <p>For an AMF file with the following location:</p> <p><code>C:\\MyAMFDocuments\\myAMF.amf</code></p> <p>The external resources, i.e. LMTs, should be located either at the same level, like this:</p> <pre><code>C:\\MyAMFDocuments\\myAMF.amf \nC:\\MyAMFDocuments\\myLookTransform.clf\n</code></pre> <p>or in a sub-folder like this:</p> <pre><code>C:\\MyAMFDocuments\\myAMF.amf \nC:\\MyAMFDocuments\\myAMFLooks\\myLookTransform_First.clf \nC:\\MyAMFDocuments\\myAMFLooks\\myLookTransform_Second.clf\n</code></pre> <p>In addition to the recommendations listed above, it is also highly recommended to avoid deep hierarchies for the sub-folders as these can easily cause trouble when the files are moved to a file system with limitations on the file path length.</p> <p>If the external resources cannot be stored in the same folder as the AMF document or in sub-folders relative to the AMF document's location, then the system/device working with the AMF document should provide some user interface means to allow the selection of the location. If automation, without user intervention, is desired, the system/device should provide a configuration file to specify the location.</p>"},{"location":"guides/amf/#file-name-characters","title":"File name characters","text":"<p>As stated before, modern file systems are very permissive in terms of file naming. Moreover, most systems have either no limit on the file name length or a very large limit that exceeds easily most of the use cases. Taking advantage of these file system features might not be a good practice though. These limitations differ among the file systems in use and migrating files from one file system to another might result in errors or even truncated file names.</p> <p>In order to avoid problems at the file system level, consider following these rules:</p> <ol> <li>Keep files name lengths under 128 characters, file name extension included</li> <li>Restrict the file name extension to 3 characters</li> <li>Use only alphabetical characters for the file name extension</li> <li>Use the native or recommended file name extension for the external resource (e.g. CLF or clf for the Common LUT Format)</li> </ol>"},{"location":"guides/amf/#file-naming-conventions","title":"File naming conventions","text":"<p>AMF does not impose a strict file naming convention on the external resources. However it is also highly recommended that a proper and meaningful one is adopted when naming those resources.</p> <p>Proper file naming conventions not only ease the inspection of the files in a file system but also can provide a better reading when displayed in the graphical user interface of the system/device used to manage them. Since these systems/devices can have a limited display room, short names should be considered.</p>"},{"location":"guides/amf/#retrieving-external-lmts-via-http","title":"Retrieving external LMTs via HTTP","text":"<p>The resources identified by a URI using the \"http\" or \"https\" schemes can be retrieved as the response to a GET request to the URI. When working with CLF-based LMTs, care must be taken to clearly indicate the content type in the HTTP headers. For instance AMF and CLF are XML-based specifications and HTTP allows the content type to signal XML in many different ways. Two popular ones are:</p> <ol> <li><code>text/xml</code></li> <li><code>application/xml</code></li> </ol> <p>These should be preferred in a HTTP transaction when working with AMF and CLF.</p> <p>HTTP transactions can require authentication in order to access the AMF and the LMTs. Authentication and encryption topics are outside the scope of this document. Nevertheless it's important to consider these issues in a workflow that is distributed around various locations as not all systems/devices support the HTTP security features</p>"},{"location":"guides/amf/#using-acesuuid","title":"Using <code>&lt;aces:uuid&gt;</code>","text":"<p>CLF ProcessList root element shall have the id attribute set with the sameUUID, e.g:</p> <p>AMF <pre><code>&lt;aces:uuid&gt;urn:uuid:1258F89C-0ED7-4A79-0E2-36F97E8FF9F1&lt;/aces:uuid&gt;\n</code></pre> CLF <pre><code>&lt;ProcessList\nxmlns=\"urn:AMPAS:CLF:v3.0\" id=\"urn:uuid:1258F89C-0ED7-4A79-B0E2-36F97E8FF9F1\" compCLFversion=\"3.0\"&gt;\n&lt;/ProcessList&gt;\n</code></pre> The CLF files can be located anywhere and the product supporting AMF+CLF must provide the configuration options to locate the CLFassets, or,search for the CLF files in the local folder for the corresponding CLF files\u2022recursive search in subfolders should be supported (option)</p>"},{"location":"guides/amf/#annex","title":"Annex","text":""},{"location":"guides/amf/#avid-log-exchange-ale-support","title":"Avid Log Exchange (ALE) support","text":"<p>The Avid Log Exchange (ALE) format supports custom metadata elements through the definition of dedicated columns in the ALE table. In order to support AMF linkage through ALE, the following columns are defined:</p> <pre><code>AMF_UUID \nAMF_NAME\n</code></pre> <p>These two columns enable the linkage of AMF files, independently for every clip listed in the ALE file. Both <code>AMF_UUID</code> and <code>AMF_NAME</code> are defined as optional. The linkage rules are described below.</p>"},{"location":"guides/amf/#amf_uuid","title":"<code>AMF_UUID</code>","text":"<p>The <code>AMF_UUID</code> column shall be used to convey the AMF UUID from the amf:Info/uuid element. The format of the column entries must use the canonical textual representation, the 16 octets of a UUID are represented as 32 hexadecimal (base-16) digits, displayed in 5 groups separated by hyphens, in the form 8-4-4-4-12 for a total of 36 characters (32 alphanumeric characters and 4 hyphens):</p> <p><code>afe122be-59d3-4360-ad69-33c10108fa7a</code></p> <p>The <code>AMF_UUID</code> column is optional.</p>"},{"location":"guides/amf/#amf_name","title":"AMF_NAME","text":"<p>AMF_NAME shall be used to convey the AMF file name located in the same folder as the .ale source file:</p> <p><code>clip_001.amf</code></p> <p>The <code>AMF_NAME</code> is optional. When present, it should indicate the file name of the AMF document related to the clip. The AMF file must reside locally in the same folder as the ALE file. No sub-folder structure is permitted. While AMF files can have any name, it is recommended to follow the restrictions imposed by the ALE Specification, i.e. to use the UNC Path syntax.</p>"},{"location":"guides/amf/#linkage-rules","title":"Linkage Rules","text":"<p>Since both <code>AMF_UUID</code> and <code>AMF_NAME</code> are optional, there are four possible combinations that can occur:</p> <p><code>AMF_UUID</code> and <code>AMF_NAME</code> are both absent:</p> <p>In this case, no AMF file can be associated with the clip and is treated like a regular ALE file <code>AMF_UUID</code> is present and <code>AMF_NAME</code> is absent:</p> <p>In this case the host product must look for the corresponding AMF files into a database, using the UUID as a key to match the AMF file and the corresponding clip. Please note that the word \"database\" does not imply any specific implementation. This feature many not be supported by the host product</p> <p><code>AMF_UUID</code> is absent and <code>AMF_NAME</code> is present:</p> <p>In this case, the <code>AMF_NAME</code> column contains file names for AMF files that should be located at the same level in the file system (i.e. same folder) as the ALE file, or in a subfolder. The linkage is based on the file name and the UUID of the AMF files (if present) is ignored</p> <p><code>AMF_UUID</code> and <code>AMF_NAME</code> are both present:</p> <p>In this case, the host product can select between the methods described in 2) and 3). However, it is recommended to rely on the UUID in priority. The host product can provide an option to select the matching rule (UUID or file name). It is desirable to also provide a matching rule that checks both the UUID and file name.</p>"},{"location":"guides/amf/#remarks","title":"Remarks","text":"<p>Since the ALE file can reference a large number of clips, it is recommended that the host product presents the issues encountered during the linkage and validation process as a log.</p> <p>ALE files can carry inline ASC parameters. When using AMF with ALEs, the inline ASC parameters should be absent to avoid confusion, or ignored if present.</p> <p>AMF files can have an optional <code>aces:clipId</code> element that is used to identify the clip that the AMF is related to. The <code>aces:clipId</code> element can carry a reference using different methods (e.g. file name, UUID, etc). It is strongly recommended that the clip identification method used in AMF correlates with the method used in the ALE files (e.g. file name).</p> <p>If the same AMF file is shared by multiple clips, it is recommended to avoid the use of <code>aces:clipId</code> or ignore it.</p> <p>A validation process can log any differences and present the results to the user of the product/tool processing the ALE+AMF files</p>"},{"location":"guides/amf/#edit-decision-list-edl-support","title":"Edit Decision List (EDL) support","text":"<p>The CMX3600 Edit Decision List (EDL) format supports custom extensions through the definition of dedicated directives following the edit statements in the decision list. In order to support AMF linkage through EDL, the following directives are defined:</p> <pre><code>AMF_UUID\nAMF_NAME\n</code></pre> <p>These two directives enable the linkage of AMF files, independently for every clip listed in the EDL file. Both <code>AMF_UUID</code> and <code>AMF_NAME</code> are defined as optional. The linkage rules are described below.</p>"},{"location":"guides/amf/#amf_uuid_1","title":"<code>AMF_UUID</code>","text":"<p>The <code>AMF_UUID</code> column shall be used to convey the AMF UUID from the amf:Info/uuid element. The format of the column entries must use the canonical textual representation, the 16 octets of a UUID are represented as 32 hexadecimal (base-16) digits, displayed in 5 groups separated by hyphens, in the form 8-4-4-4-12 for a total of 36 characters (32 alphanumeric characters and 4 hyphens):</p> <p><code>afe122be-59d3-4360-ad69-33c10108fa7a</code></p> <p>The <code>AMF_UUID</code> column is optional. When present, it should indicate a path to the AMF file that is relative to the folder where the ALE file is located. The path hierarchy MUST not contain the parent folder or local folder distinguished values, i.e. \"..\" and \".\" to avoid any confusion. The path and AMF file name must use characters from the set a-z, A-Z, 0-9, - (dash), _ (underscore) and \".\". No path segment shall use more than 128 characters and the total length shall not exceed 1024 characters.</p>"},{"location":"guides/amf/#amf_name_1","title":"<code>AMF_NAME</code>","text":"<p><code>AMF_NAME</code> shall be used to convey the AMF file name located in the same folder as the .edl source file:</p> <p><code>clip_001.amf</code></p> <p>The <code>AMF_NAME</code> is optional. When present, it should indicate the file name of the AMF document related to the clip. The AMF file must reside locally in the same folder as the EDL file. No sub-folder structure is permitted. While AMF file can have any name, it is recommended to use the same base name as the clip file that the AMF document relates to. Moreover to ensure portability across file systems and operating systems it is recommended to use characters from the set a-z, A-Z, 0-9, - (dash), _ (underscore) and \".\". The AMF file name should use no more than 1024 characters.</p>"},{"location":"guides/amf/#linkage-rules_1","title":"Linkage Rules","text":"<p>Since both <code>AMF_UUID</code> and <code>AMF_NAME</code> are optional, there are four possible combinations that can occur:</p> <p><code>AMF_UUID</code> and <code>AMF_NAME</code> are both absent</p> <p>In this case, no AMF file can be associated with the clip <code>AMF_UUID</code> is present and <code>AMF_NAME</code> is absent</p> <p>In this case the host product must look for the corresponding AMF files into a database, using the UUID as a key to match the AMF file and the corresponding clip. Please note that the word \"database\" does not imply any specific implementation. This feature many not be supported by the host product</p> <p><code>AMF_UUID</code> is absent and <code>AMF_NAME</code> is present</p> <p>In this case, the <code>AMF_NAME</code> column contains file names for AMF files that should be located at the same level in the file system (i.e. same folder) as the EDL file, or in a subfolder. The linkage is based on the file name and the UUID of the AMF files (if present) is ignored</p> <p><code>AMF_UUID</code> and <code>AMF_NAME</code> are both present</p> <p>In this case, the host product can select between the methods described in 2) and 3). However, it is recommended to rely on the UUID in priority. The host product can provide an option to select the matching rule (UUID or file name). It is desirable to also provide a matching rule that checks both the UUID and file name.</p>"},{"location":"guides/amf/#edl-event-example","title":"EDL event example","text":"<pre><code>...\n010 Clip1 V C 05:40:12:18 05:40:14:09 01:00:29:16 01:00:31:07 \n* AMF_NAME clip_001.amf\n* AMF_UUID afe122be-59d3-4360-ad69-33c10108fa7a\n...\n</code></pre>"},{"location":"guides/amf/#remarks_1","title":"Remarks","text":"<p>Since each entry in the EDL file can use any of the combinations of <code>AMF_UUID</code> and <code>AMF_NAME</code> described above, it is recommended that the host product presents the issues encountered during the linkage and validation process as a log.</p> <p>EDL files can carry inline ASC parameters. When using AMF with EDLs, the inline ASC parameters should be absent to avoid confusion, or ignored if present.</p> <p>AMF files can have an optional <code>aces:clipId</code> element that is used to identify the clip that the AMF is related to. The <code>aces:clipId</code> element can carry a reference using different methods (e.g. file name, UUID, etc). It is strongly recommended that the clip identification method used in AMF correlates with the method used in the EDL files (e.g. file name).</p> <p>If the same AMF file is shared by multiple clips, it is recommended to avoid the use of <code>aces:clipId</code> or ignore it.</p> <p>A validation process can log any differences and present the results to the user of the product/tool processing the EDL+AMF files.</p>"},{"location":"guides/clf/","title":"Common LUT Format (CLF) Implementation Guide","text":""},{"location":"guides/clf/#introduction","title":"Introduction","text":"<p>Look-up tables, or LUTs, are a common method for communicating color transformations. Many software and hardware providers develop LUT formats uniquely designed for use in their systems. Since these formats were designed to work in specific use cases, they often prove inadequate for interchangeability between applications or systems. To further complicate matters, some LUT formats use the same file extensions which make them appear to be compatible when they are not. If there are already a dozen or more confusing LUT formats, why should you as a developer consider adding support for yet another one?</p> <p>While the myriad LUT formats already available are fundamentally useful in theory, each lacks one or more features that can be critical in meeting the demands of today\u2019s sophisticated workflows. Existing formats can lack the quality, versatility, and metadata required to meet the demands of modern systems. </p> <p>The Common LUT Format (CLF) provides flexibility to enclose transforms from simple to complex. Due to a lack of interchangeability of color transforms between tools, LUTs are frequently abused as a catch-all. Even simple color-space transformations, such as the application of a matrix or a logarithmic shaper function are often \u201cbaked\u201d to crude LUT formats resulting in unfortunate losses in precision. As a solution, CLF allows for a range of common mathematical operators to be specified precisely, in addition to supporting traditional 1D- and 3D-LUTs in the file. </p> <p>Because CLF files are floating-point capable, extremely flexible, and well documented, they are an excellent candidate for use in modern workflows. CLFs are also ideal for archival purposes because the format is well-specified and documented. There is also a high-quality, open source implementation freely available on GitHub.</p>"},{"location":"guides/clf/#format-comparison-table","title":"Format Comparison Table","text":"Features/Formats CLF 3dl Adobe (Iridas) cube Resolve cube Truelight cube Cinespace cube ASC CDL Imageworks spi3d ICC Profile Provider Academy / ASC Discreet Adobe Blackmagic Filmlight Rising Sun ASC Imageworks ICC Maintained public documentation \u2705 \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u274c \u2705 Implementation guide \u2705 \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u2705 Allows shaper LUT \u2705 \u274c \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 Is not limited to log or video data on input \u2705 \u274c \u274c \u2705 \u2705 \u2705 \u274c \u274c \u274c Unconstrained ordering of processing elements \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c Floating-point table values \u2705 \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Rich metadata \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u2705 Test suite provided \u2705 \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u2705 Text-based \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c Can define operations in linear floating-point space \u2705 \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u2705 GUID support \u2705 \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u2705 Supports mathematical operators \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c"},{"location":"guides/clf/#target-audience","title":"Target Audience","text":"<p>This document is primarily intended for application developers who want to add CLF support to their product. It defines requirements, tests, and provides recommendations that, if followed, will lead to robust support for CLF. Implementers who follow the guidance in this document can have confidence that their product is implementing the specification correctly.</p> <p>The document may also be of interest to those using CLF to author transforms and who want to understand how the CLFs will be used by applications.</p> <p>This guide should be read in conjunction with the CLF Specification (v 3.0). </p> <p>Note</p> <p>The specification was previously referred to by its document number - \"S-2014-006\". Although the spec had \"2014\" in the name, the document has been updated more recently than that. Version 3 of CLF was introduced with the release of ACES 1.2 in 2020 and the most recent editorial updates were in 2021. The current version of the specification now lives at the above link.</p>"},{"location":"guides/clf/#a-quick-introduction-to-clf","title":"A Quick Introduction to CLF","text":"<p>Below is a basic example of a simple CLF file. Despite the word 'LUT' in the name of the format, these very simple examples do not contain any type of LUT whatsoever. Instead, the CLF is being used to communicate a set of ASC CDL adjustments (Example 1), and encapsulate a YCbCr to RGB conversion (Example 2).</p> <p>There are a few key points that these examples demonstrate:</p> <ul> <li>CLF is an XML document and therefore conforms to the requirements of any XML document.</li> <li>There is one <code>ProcessList</code>, which can contain any number of <code>ProcessNodes</code>. (A <code>ProcessNode</code> is an operator such as a <code>Matrix</code> or <code>LUT3D</code>.)</li> <li>A CLF may or may not contain \u201cLUTs\u201d (despite the name).</li> <li>Some parts are optional and others are required.</li> <li>CLF provides a richer metadata model than other LUT formats - it\u2019s not just numbers. Good metadata is highly encouraged and helps make the CLF file self-documenting.</li> <li>Every CLF must have a unique <code>id</code> attribute.</li> <li>The bit-depth attributes control formatting but not precision.</li> </ul> <p>Color coding for the following two examples:  red is required  blue is optional  green are comments</p>"},{"location":"guides/clf/#example-1","title":"Example 1:","text":"ASC CDL Implementation <pre>\n<code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!-- Required: XML Version and Encoding declaration --&gt;\n\n&lt;!-- Required: ProcessList element with \u2018id\u2019 and \u2018compCLFversion\u2019 --&gt;\n&lt;!-- name and xmlns are optional --&gt;\n&lt;ProcessList\n  compCLFversion=\"3.0\" \n  id=\"b4cca59a-9428-49c0-8e91-868718c4e526\" \n  name=\"FwdNoClamp style\" \n  xmlns=\"urn:AMPAS:CLF:v3.0\"&gt;\n\n  &lt;Description&gt;CDL with FwdNoClamp style&lt;/Description&gt;\n\n  &lt;ASC_CDL \n    id=\"clf/ctf no-clamp fwd\" \n    inBitDepth=\"10i\" \n    outBitDepth=\"8i\" \n    style=\"FwdNoClamp\"&gt;\n\n    &lt;SOPNode&gt;\n      &lt;Slope&gt;  1.000000  1.000000  0.800000&lt;/Slope&gt;\n      &lt;Offset&gt;-0.020000  0.000000  0.150000&lt;/Offset&gt;\n      &lt;Power&gt;  1.050000  1.150000  1.400000&lt;/Power&gt;\n    &lt;/SOPNode&gt;\n\n    &lt;SatNode&gt;\n      &lt;Saturation&gt;0.750000&lt;/Saturation&gt;\n    &lt;/SatNode&gt;\n  &lt;/ASC_CDL&gt;\n&lt;/ProcessList&gt;\n</code>\n</pre>"},{"location":"guides/clf/#example-2","title":"Example 2:","text":"BT.709 YCbCr (SMPTE/legal range) to RGB (full range) <pre>\n<code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!-- Line above is required: XML Version and Encoding declaration --&gt;\n\n&lt;!-- Required: ProcessList element with \u2018id\u2019 and \u2018compCLFversion\u2019 --&gt;\n&lt;ProcessList\n  id=\"0befc138-3757-45f0-a080-83bebb77baf2\"\n  compCLFversion=\"3.0\"&gt;\n\n  &lt;!-- Optional: ProcessList Description --&gt;\n  &lt;Description&gt;BT.709 YCbCr (legal) to RGB (full)&lt;/Description&gt;\n\n  &lt;!-- Optional: InputDescriptor / OutputDescriptor --&gt;\n  &lt;InputDescriptor&gt;YCbCr&lt;/InputDescriptor&gt;\n  &lt;OutputDescriptor&gt;RGB&lt;/OutputDescriptor&gt;\n\n  &lt;!-- Required: One or more ProcessNode elements --&gt;\n  &lt;!-- inBitDepth and OutBitDepth are required, id is optional --&gt;\n  &lt;!-- If in/outBitDepth values are different, the scale factor\n    between them must also be applied to the matrix coefficients! --&gt;\n  &lt;Matrix \n    id=\"815ebbac-550a-453b-a1e6-bf93779fc9c8\" \n    inBitDepth=\"32f\" \n    outBitDepth=\"32f\"&gt;\n\n    &lt;!-- Optional: ProcessNode description --&gt;\n    &lt;Description&gt;Input offsets for legal range luma and color\n      difference channels&lt;/Description&gt;\n\n    &lt;!-- White space formatting is recommended, but optional --&gt;\n    &lt;!-- Array element is required for a Matrix ProcessNode --&gt;\n    &lt;Array dim=\"3 4\"&gt;\n      &lt;!-- when dim=\u201d3 4\u201d, the 4th column is offset terms --&gt;\n      1.0 0.0 0.0 -0.0625\n      0.0 1.0 0.0 -0.5\n      0.0 0.0 1.0 -0.5\n    &lt;/Array&gt;\n  &lt;/Matrix&gt;\n\n  &lt;!-- Additional ProcessNodes are applied in order --&gt;\n  &lt;Matrix \n    id=\"deceda6e-8fee-471a-8599-fa513c17f3cf\"\n    inBitDepth=\"32f\" \n    outBitDepth=\"32f\"&gt;\n    &lt;Description&gt;BT.709 YCbCr to RGB matrix&lt;/Description&gt;\n    &lt;Array dim=\"3 3\"&gt;\n      1.16893193493151  0.000000000000000  1.799743966238840\n      1.16893193493151 -0.214081616673236 -0.534991005624129\n      1.16893193493151  2.120653355189730 -0.000000000000000\n    &lt;/Array&gt;\n  &lt;/Matrix&gt;\n&lt;/ProcessList&gt;\n\n</code>\n</pre>"},{"location":"guides/clf/#open-source-example-implemention","title":"Open Source Example Implemention","text":"<p>As you explore CLF and work to implement it into your product(s), it may be helpful to refer to some existing tools that already provide full CLF functionality. The tools described here are included in the open source project OpenColorIO (OCIO) v2. More details and the full installation process for OCIO can be found at https://www.opencolorio.org.</p>"},{"location":"guides/clf/#ociochecklut","title":"<code>ociochecklut</code>","text":"<p>The command-line utility <code>ociochecklut</code> can be used to load a CLF file and process an RGB triplet through the CLF file. It will report any errors that are encountered in parsing the file. If no RGB triplet is provided to process through the CLF file, then a list of the <code>ProcessNodes</code> contained in the LUT are returned. This tool is installed as part of OCIO v2.</p> <p>Here is sample output using the CLF in the example section (assuming it is saved as a file called <code>709_ycbcr-to-rgb.clf</code>):</p> <p>Summarizing the contents of the CLF: <pre><code>$ ociochecklut 709_ycbcr-to-rgb.clf\nTransform operators: \n    &lt;MatrixTransform direction=forward, fileindepth=32f, fileoutdepth=32f, matrix=1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1, offset=-0.0625 -0.5 -0.5 0&gt;\n    &lt;MatrixTransform direction=forward, fileindepth=32f, fileoutdepth=32f, matrix=1.16893193493151 0 1.79974396623884 0 1.16893193493151 -0.214081616673236 -0.534991005624129 0 1.16893193493151 2.12065335518973 -0 0 0 0 0 1, offset=0 0 0 0&gt;\n</code></pre></p> <p>Evaluating the RGB value [0.5, 0.4, 0.3]: <pre><code>$ ociochecklut 709_ycbcr-to-rgb.clf 0.5 0.4 0.3\n0.1514589 0.6398141 0.2993424\n</code></pre></p>"},{"location":"guides/clf/#ocioconvert","title":"<code>ocioconvert</code>","text":"<p>The command-line utility <code>ocioconvert</code> can be used to apply a CLF file to an image. To apply a CLF file, use the <code>--lut</code> option. A variety of image file formats are supported. This tool is installed as a part of OCIO v2, although it first requires installation of OpenImageIO.</p> <p>Processing the input image <code>syntheticChart.01.exr</code> to the output image <code>output_image.exr</code> through the CLF from the previous example: <pre><code>$ ocioconvert --lut 709_ycbcr-to-rgb.clf syntheticChart.01.exr output_image.exr\n</code></pre></p>"},{"location":"guides/clf/#ociomakeclf","title":"<code>ociomakeclf</code>","text":"<p>The command-line utility <code>ociomakeclf</code> will convert any LUT format supported by OpenColorIO into CLF format. The <code>--csc</code> option may be used to create an ACES Look Transform that is compatible with the ACES Metadata File (AMF). This tool is installed as a part of OCIO v2.</p> <p>Convert the LUT oldLUT.3dl to CLF format: <pre><code>$ ociomakeclf oldLUT.3dl oldLUT.clf\n</code></pre></p> <p>Convert the look LUT acescctLUT.3dl that expects and produces ACEScct values to an ACES Look Transform in CLF format that expects and produces ACES2065-1 values: <pre><code>$ ociomakeclf acescctLUT.3dl LMT.clf --csc ACEScct\n</code></pre></p>"},{"location":"guides/clf/#minimum-requirements","title":"Minimum Requirements","text":""},{"location":"guides/clf/#introduction_1","title":"Introduction","text":"<p>The products anticipated to implement CLF can be categorized into two broad categories: one for final production-level finished images and one for preview/proxy images. Due to the fundamental differences in the products, different requirements are provided for the two categories.</p>"},{"location":"guides/clf/#finishing-tier","title":"Finishing Tier","text":"<p>The primary category of products, dubbed the Finishing Tier, includes software implementations that have the logic and processing power available to parse and apply <code>ProcessNode</code> operations using floating-point calculations and in sequential order. Finishing Tier products provide the highest quality image processing and have the tightest tolerances, prioritizing accuracy in computation results. Finishing Tier products should be used to create images when the highest image fidelity is required in pipelines utilizing CLF files.</p>"},{"location":"guides/clf/#minimum-requirements_1","title":"Minimum Requirements","text":"<p>Products in the Finishing Tier category shall:</p> <ol> <li>Pass all the Read Tests</li> <li>Pass the Finishing Tier Apply Tests</li> </ol> <p>In addition to the minimum requirements, Finishing Tier products should review the \"Writing CLFs\" section and conform their CLF implementation to those recommendations wherever possible.</p>"},{"location":"guides/clf/#preview-tier","title":"Preview Tier","text":"<p>The second category of implementations are described as Preview Tier devices. These are products that, due to limited processing power or technical constraints, cannot implement a CLF <code>ProcessList</code> exactly and instead require that CLF files be \u201dbaked\u201d or collapsed into a simpler representation (for example, a single 3D-LUT). Hardware devices such as on-set LUT boxes would be an example of devices that might fall into this category. </p> <p>As the name implies, Preview Tier products are suitable for creating images such as for on-set viewing, where the requirements for accuracy and/or flexibility are lower than for the Finishing Tier.</p> <p>CLF is designed as a modern LUT format that can handle floating-point input and output pixels. However, the current ecosystem of devices still includes many products that work primarily on integer-encoded signals (e.g. HD-SDI and HDMI video) and do not support floating-point image data, including scene-linear encodings such as ACES2065-1. These types of devices would fall in the Preview Tier and CLF may be used to encapsulate any of the LUTs that are currently used in such devices. But there is no expectation that these devices will be able to accurately process other CLFs that contain transforms expecting scene-linear inputs or outputs.</p> <p>Note that although the processing requirements are lower for the Preview Tier, the read requirements are not. In other words, even Preview Tier devices must be able to read all of the files in the test suite. But as described in the section \"Applying CLFs\", if a Preview Tier device detects <code>ProcessNodes</code> that it does not support, there are two options:</p> <ol> <li>Inform the user of this situation and do not attempt to process the file.</li> <li>Attempt to bake the CLF down into a representation supported by the device. The user should be given some indication that they are seeing an approximation of the original CLF.</li> </ol>"},{"location":"guides/clf/#minimum-requirements_2","title":"Minimum Requirements","text":"<p>Products in the Preview Tier category shall:</p> <ol> <li>Pass all the Read Tests</li> <li>Pass the Preview Tier Apply test</li> </ol>"},{"location":"guides/clf/#reading-clfs","title":"Reading CLFs","text":"<p>This section describes the general requirements for parsing CLF files, the provided test suite, and the steps for validating an implementation using the test suite.</p>"},{"location":"guides/clf/#general-parsing-requirements","title":"General Parsing Requirements","text":""},{"location":"guides/clf/#requirements","title":"Requirements:","text":"<ol> <li> <p>Version Support</p> <p>Implementations are expected to support v3 of the CLF Specification. Backwards compatibility for versions prior to v3 is optional.</p> </li> <li> <p>Error Handling</p> <p>An implementation must check that a file is valid, and if not, issue an error or warning to the user hinting at what the problem is. The tool <code>ociochecklut</code> (described above) provides good examples of the types of errors that could be issued when illegal syntax is detected.</p> </li> <li> <p>Metadata reading</p> <p>An implementation must (at a minimum) be able to display to the user the contents of these key metadata fields:</p> <ol> <li>Top-level ProcessList <code>Description</code> elements</li> <li><code>InputDescriptor</code>, <code>OutputDescriptor</code></li> </ol> </li> <li> <p>Precision and formatting of numeric values</p> <p>Implementations must be able to read at least 32-bit float precision, though 64-bit precision is desirable for ProcessNodes other than <code>LUT1D</code> and <code>LUT3D</code>. Note that the CLF specification defines the numeric values as \u201cxs:float\u201d type, that is, XML Schema floats. Parsers should be able to handle all of the details of this format (e.g., integers without a decimal point, missing leading 0 on decimal values, the presence of a leading \u201c+\u201d, etc.). Note that an integer is a legal xs:float.</p> </li> </ol>"},{"location":"guides/clf/#recommendations","title":"Recommendations","text":"<p>XML is designed to be extensible, and XML files often contain data that was not defined in the specification. However, the desire for extensibility must be balanced against the need to detect erroneous content. On occasion, unrecognized XML elements may be detected. In those instances, the following logic is recommended:</p> <ul> <li>If the unrecognized element is at the <code>ProcessNode</code> level (in other words, the top level of the <code>ProcessList</code>), it should produce an error, or at least a warning.</li> <li>If the unrecognized element is within a <code>ProcessNode</code>, a warning should be issued.</li> <li>If the unrecognized element is within the <code>Info</code> block, it may be ignored.</li> </ul>"},{"location":"guides/clf/#clf-file-test-suite","title":"CLF File Test Suite","text":"<p>A number of test files are provided for implementers to test their system and demonstrate that their implementation can robustly handle all features of CLF v3. The tests provided in the OpenColorIO repository on Github include both legal and illegal test files. The file name and description in each file identifies what the file is testing.</p> <p>The test files confirm that each <code>ProcessNode</code> is functioning per the CLF specification. For <code>ProcessNodes</code> that allow for different styles or parameters, either separate test files or single test files with multiple <code>ProcessNode</code> variations are provided to check that all styles and parameters are functional. Standard files are expected to be processed without error. </p> <p>A number of \"illegal\" test files with various syntax errors are also provided to test the error handling capability of implementations. Illegal files should not be processed and the system should generate an appropriate error message.</p>"},{"location":"guides/clf/#test-procedure","title":"Test Procedure","text":"<p>Download the OpenColorIO repository from GitHub at the URL: https://github.com/AcademySoftwareFoundation/OpenColorIO</p> <p>If you are not familiar with Git, that\u2019s fine. Simply click on the green button that says Code and select \u201cDownload ZIP\u201d. Unzip this file on your computer and you will have all of the test files. </p> <p>The test files are in the directory: <code>OpenColorIO/tests/data/files/clf</code></p> <p>You may refer to a description of each test file in Annex B.</p> <p>For each legal test file:</p> <ol> <li>Read the file into your product and verify that it loads successfully.</li> </ol> <p>For each illegal test file (these are the files in the <code>clf/illegal</code> subdirectory):</p> <ol> <li>Read the file into your product and ensure that the user is notified that it is not a legal CLF file. Ideally, the nature of the problem should be communicated to the user.</li> <li>Verify that none of these files load successfully. They should not result in a processed image. If an implementation needs to pass through the unprocessed image, it should communicate clearly to the user in some other way that there was an error.</li> </ol>"},{"location":"guides/clf/#applying","title":"Applying CLFs","text":""},{"location":"guides/clf/#general","title":"General","text":"<p>For CLF to be useful, it is important that different products output the same results when applying the same CLF to the same input. This section describes the different expectations for Finishing Tier and Preview Tier products. Each category has their own metric and defined tolerances.</p>"},{"location":"guides/clf/#finishing-tier_1","title":"Finishing Tier","text":""},{"location":"guides/clf/#tolerances","title":"Tolerances","text":"<p>The Finishing Tier is intended to cover software-based implementations such as products for tasks including color correction and compositing. It is expected that these implementations will use floating-point math to apply the contents of the CLF, without converting it into a significantly different representation. Hence fairly tight tolerances may be expected.</p> <p>A CLF may be used to apply arbitrary color space conversions and so the input and output images may be in arbitrary color spaces, including video, logarithmic, and scene-linear color encodings. Both integer and wide-range floating-point pixel values are expected. Video and logarithmic encodings are typically sufficiently perceptually uniform that a simple absolute error metric such as <code>(actual \u2013 aim)</code> may be used. However, scene-linear encodings require a tolerance that is tighter for dark colors and looser for bright colors - in other words, a relative rather than absolute metric. This is due to the approximately logarithmic nature of human color perception (although the metric is actually computed per channel).</p> <p>When comparing an aim and actual value, a basic relative error metric has the form: </p> <p><code>(actual \u2013 aim) / aim</code></p> <p>However this can become overly sensitive when the values being compared become very small. In the limit, when the aim value is zero, the result is either infinity or NaN. Therefore it is useful to use a \u201csafe-guarded relative error metric\u201d that places a lower bound on the denominator: <code>actual \u2013 aim) / max(aim, lower_bound)</code>. </p> <p>This effectively transitions the error metric from being a relative error metric for bright and normal colors to an absolute error metric when approaching a certain noise floor determined by the <code>lower_bound</code> constant. A reasonable <code>lower_bound</code> constant for images in ACES2065-1 color space would be 0.1. It is also necessary to handle the case where the aim value may be negative, in which case the final error metric becomes: </p> <p><code>abs(actual \u2013 aim) / max(abs(aim), 0.1) &lt;= 0.002</code></p> <p>This is essentially a relative tolerance of +/\u2013 one part in 500 above 0.1 and an absolute tolerance of +/\u2013 0.0002 below 0.1.</p> <p>It is expected that implementations in the Finishing Tier will be using floating-point software calculations and the processing will be applied this way regardless of whether the color encodings involved are video, logarithmic, or scene-linear. Since relative tolerances are well-suited to verifying floating-point math, the safe-guarded relative error metric will be used for all Finishing Tier tolerances even though this may be perceptually either too tight or too loose when processing video or logarithmic pixel values.</p> <p>Above the lower bound transition, the tolerance for implementations in the Finishing Tier is a relative error of plus or minus one part in 500. For comparison, half-float numbers are spaced at a relative distance of one part in 1024. (In the literature on the subject of floating-point math, this distance is called a \u201cunit of least precision,\u201d or ULP.) So the tolerance for the metric is only slightly more loose than the precision imposed by the storage of images as half-float OpenEXR files.</p> <p>To validate the tolerance, testing was conducted using various processing modes in OpenColorIO. For example, various forms of CPU optimization were applied and the results were compared to the unoptimized reference version. These included optimizations such as concatenating adjacent matrices and approximating standard library logarithm and power functions with faster but less precise versions. Likewise, processing on the GPU was compared to the CPU reference version. The test image that was used is the one described in Annex A, so it sampled the full range of half-float numbers. The fact that all of these processing variations passed the specified error tolerance indicates that it should be an achievable performance level for a wide range of products.</p>"},{"location":"guides/clf/#test-procedure_1","title":"Test Procedure","text":"<p>The collection of test CLF files are included in the OpenColorIO repository, as described in Annex B. Process the source image (whose contents are described in Annex A) through your implementation for each of the CLFs described in Annex C.</p> <p>The processed reference frames to be used for comparison may be downloaded as described in Annex C, or you may generate them yourself using the steps described in Annex K</p> <p>The command-line application <code>oiiotool</code>, which is installed as a component of OpenImageIO, can be used to compare pixels between two images and evaluate the metric specified in this section. A Python script for doing this is provided with OpenColorIO. Here are the steps for how to install and run it:</p> <ol> <li>Install OpenImageIO.</li> <li> <p>Download the OpenColorIO source code from the URL:      https://github.com/AcademySoftwareFoundation/OpenColorIO</p> </li> <li> <p>Unzip the downloaded source code package. </p> </li> <li>In a shell, <code>cd</code> to the directory where the OpenColorIO source code was unzipped, and then <code>cd</code> into the sub-directory <code>share/clf</code>. This directory contains the Python script.</li> <li> <p>In the shell, type:</p> <p><code>$ python compare_clf_test_frames.py &lt;PATH-TO-REFERENCE-IMAGES&gt; &lt;PATH-TO-ACTUAL-IMAGES&gt;</code></p> <p>replacing the two items in brackets with the path to the downloaded reference images and the path to your implementation's actual images. </p> </li> </ol> <p>The script iterates over the images in the directory and prints the oiiotool command being run. It will then print either 'Test passed' or 'Test failed' after each command and then at the end will summarize with 'All tests passed' or 'These tests failed' with a list of the failed images.</p>"},{"location":"guides/clf/#alternate-test-procedure","title":"Alternate Test Procedure","text":"<p>Implementers may also choose to use other tools or implement their own tools to compare pixel values and calculate the metrics. In that case, proceed as follows to compare your actual images to the aim reference images described in Annex C:</p> <p>For each CLF identified in the Finishing Tier test list in Annex C:</p> <ol> <li>Use your implementation to process the test frame through each legal CLF file.</li> <li>Calculate the error metric:  <code>abs(actual \u2013 aim) / max(abs(aim), 0.1)</code></li> <li>Verify that:<ol> <li>max error &lt;= 0.002</li> <li>no Infinity or NaN values are created</li> </ol> </li> </ol>"},{"location":"guides/clf/#preview-tier_1","title":"Preview Tier","text":""},{"location":"guides/clf/#lut-baking","title":"LUT Baking","text":"<p>Preview Tier devices/implementations typically are not able to handle the full flexibility of the CLF processing model. If the contents of the CLF correspond to what the device may natively apply, then it is reasonable to expect fairly tight processing tolerances. However, if the CLF contains many ops that must be converted (or \"baked\") into some other representation in order to be applied in device hardware, then it may be unreasonable to demand the same level of performance. </p> <p>Different tolerances would need to be established for each CLF in the test suite depending on the contents and complexity of each CLF and on the details of how it is baked. Therefore, the current version of this guide only tests the ability to process a single <code>LUT3D</code> (e.g. a baked result, not the baking process itself).</p> <p>So all Preview Tier devices must at least handle the <code>LUT3D</code> apply test but implementers will need to decide how to handle CLFs that are more complicated. One approach is to simply inform the user that the CLF contains operators that cannot be processed by that implementation. Another approach is to attempt to bake the CLF into a form that the implementation is able to process. Annex E provides an introduction to the topic of baking.</p> <p>Implementations should notify users if their CLF is being baked into a significantly different structure in order to be processed by the device. (Simple adjustments such as concatenating adjacent matrices into a single matrix do not fall into this category, but combining multiple operators into a single <code>LUT3D</code> certainly does.)</p> <p>Implementers are strongly advised to document the CLF structures that they are able to apply exactly, without need of baking. This would allow users to plan their processing accordingly in order to make best use of the device. Advanced users will be able to generate CLFs in order to obtain the best accuracy possible, given the restrictions.</p>"},{"location":"guides/clf/#tolerances_1","title":"Tolerances","text":"<p>The Preview Tier is intended to cover devices such as on set LUT boxes that work on live SDI video signals. These devices are intended to process video or logarithmic encodings and typically do not handle floating-point pixel values. Therefore, a simple absolute error metric may be used. Integer values should be normalized to [0,1] by dividing by the maximum integer value (e.g., 1023, 4095, 65535), yielding an absolute error metric applied to normalized values:</p> \\[ {\\text{abs}(actual \u2013 aim) &lt;= 0.002} \\] <p>This is essentially a tolerance of +/\u2013 two 10-bit code values, if applied to a 10-bit signal, or equivalently +/\u2013 eight 12-bit code values, if applied to a 12-bit signal, etc.</p>"},{"location":"guides/clf/#test-procedure_2","title":"Test Procedure","text":"<p>The process of verification for the Preview Tier is to check for correct implementation of a <code>LUT3D</code> with tetrahedral interpolation using an integer test image.</p> <p>For the CLF identified in the Preview Tier test list in Annex D:</p> <ol> <li>Use your implementation to process the Preview Tier DPX source image (see Annex D) and produce an integer DPX result.</li> <li> <p>Compare your result to the DPX reference frame provided (see Annex D), and calculate the error metric described in the preceding paragraphs:</p> <ol> <li> <p>If using OpenImageIO, the command would be:</p> <pre><code>$ oiiotool &lt;aim-image&gt; &lt;actual-image&gt; --absdiff --rangecheck 0,0,0 .002,.002,.002 -o /tmp/tmp.dpx\n</code></pre> </li> </ol> </li> <li> <p>This command will print something like this:</p> <pre><code>0 &lt; 0,0,0\n0 &gt; .002,.002,.002\n1048576 within range\n</code></pre> <p>If the second line of the result indicates that there are 0 pixels greater than 0.002, then the test passed. </p> <p>(Note that <code>oiiotool</code> works in normalized units, so 0.002 is actually equivalent to 0.002 x 1023 = 2.046 10-bit code values.)</p> </li> </ol>"},{"location":"guides/clf/#writing","title":"Writing CLFs","text":"<p>Not all products must support writing CLFs, depending on the way they are used. If your product supports the writing of CLF files, it must adhere to the CLF specification. Some important highlights and recommendations for implementation default behavior, or for users authoring CLFs by hand, are described in the following sections.</p>"},{"location":"guides/clf/#file-extension","title":"File Extension","text":"<p>The extension used for CLF files should be a  <code>.clf</code>  at the end of the file name.</p>"},{"location":"guides/clf/#indentation","title":"Indentation","text":"<p>CLF files may be opened and read by users during troubleshooting, so readability is desirable. In particular, the following formatting is recommended for indentation: </p> <ol> <li>Use 2-4 spaces to indent relative to a parent item.</li> <li>A new indented line should be used after the starting element tag for complex XML types. For compactness, simple XML types may be placed on a single line.</li> <li>Arrays (contained in <code>Matrix</code>, <code>LUT1D</code>, <code>LUT3D</code>) should use line breaks to present data visually (e.g., by aligning columns of R, G, B). Large blocks of lines of numbers do not need to be indented since they are already visually distinct and there is no point adding spaces in front of thousands of lines (e.g., in the case of large LUTs).</li> <li>Long text strings (e.g. in a <code>Description</code> tag) should not contain embedded wrapping and indentation. It is better to let the application software determine where to wrap long lines of text in order to present them best in its own user interface.</li> </ol>"},{"location":"guides/clf/#indentation-example-using-made-up-element-names","title":"Indentation example (using made-up element names):","text":"<pre><code>&lt;aTag&gt;\n   &lt;simpleType&gt;This does not require a line break.&lt;/simpleType&gt;\n   &lt;complexType&gt;\n      &lt;simpleType&gt;This is a simpleType in a complexType.&lt;/simpleType&gt;\n      &lt;simpleType&gt;Here's a long text line that shouldn't be wrapped in the CLF since the app will decide where to wrap it.&lt;/simpleType&gt;\n   &lt;/complexType&gt;\n   &lt;simpleTypeWithParam param=0.5 /&gt;\n   &lt;Array dim=\"3 4\"&gt;\n1.0  0.0  0.0  -0.3\n0.0  1.0  0.0  -0.5\n0.0  0.0  1.0  -0.5\n   &lt;/Array&gt;\n&lt;/aTag&gt;\n</code></pre>"},{"location":"guides/clf/#use-of-xml-comments","title":"Use of XML Comments","text":"<p>CLF authors should avoid using XML comments to encapsulate metadata or important information. Most parsers ignore comments, so if a CLF gets read and rewritten, comments that were previously there may go missing. Any important information should be enclosed in provided metadata XML elements and attributes (e.g., the ProcessList's <code>Info</code> element). </p>"},{"location":"guides/clf/#discrete-operations","title":"Discrete Operations","text":"<p>Finishing Tier products should, whenever possible, encapsulate discrete math operations with one or more <code>ProcessNodes</code> in a <code>ProcessList</code> rather than simply exporting a 1D- and/or 3D-LUT. For example, a common color space conversion should use discrete <code>Log</code> and <code>Matrix</code> nodes, where appropriate, rather than a single <code>LUT3D</code>.</p>"},{"location":"guides/clf/#precision-and-range-of-numeric-values","title":"Precision and Range of Numeric Values","text":"<p>Ensure your implementation writes a sufficient number of digits. Even though image processing will typically be done at 32-bit floating-point precision, intermediate calculations (for example, combining matrices) may be done at higher precision.</p> <p>Also, take note that the bit-depth attributes do not impose range or quantization limits. Hence you should not impose these limits unnecessarily. For example, for a LUT1D with an <code>outBitDepth</code> of <code>10i</code>, the normal range would be expected to be 0 to 1023. However, it is legal to exceed this range and also to use fractional values. Thus, values such as <code>[-10.5, 0.01, 1055.2]</code> could be legal values. Please refer to the Implementation Notes on Bit Depth section of the CLF specification for more detail.</p>"},{"location":"guides/clf/#the-id-attribute","title":"The <code>id</code> Attribute","text":"<p>Every CLF is required to have an <code>id</code> attribute at the <code>ProcessList</code> level. The specification does not impose any restrictions on the formatting of this attribute. However, it should be noted that an ACES Metadata File that references a CLF file prefers that the <code>id</code> attribute contains a UUID object according to RFC 4122. Therefore, it is recommended that implementations use a UUID to fill the <code>id</code> attribute when writing new CLF files. Note that the <code>id</code> attribute is optional at the <code>ProcessNode</code> level.</p>"},{"location":"guides/clf/#storage-of-proprietary-metadata","title":"Storage of Proprietary Metadata","text":"<p>If an application wants to store \"dark metadata\" that is meaningful only for a special purpose within proprietary products or workflows, this is easily accomplished. Indeed this is one of the frequently cited benefits of the XML encoding. However, it is important that CLF writers are respectful of certain guidelines to ensure the CLF file remains usable by other readers. If you need to add proprietary metadata, please respect the following:</p> <ol> <li>Check the CLF spec to see if there is already an element whose purpose matches what you are trying to store.</li> <li>If not, you may create a custom XML element to store your metadata. As described in the spec, this should typically be placed within the <code>Info</code> block. You may add additional custom elements and attributes under your main element as needed in order to easily represent your information.</li> <li>Avoid using the standard existing elements such as <code>Description</code> in a way that is inconsistent with their purpose.</li> <li>Avoid placing custom elements at the <code>ProcessNode</code> level since that would make it an illegal file that most parsers will reject.</li> </ol>"},{"location":"guides/clf/#other-metadata-considerations","title":"Other Metadata Considerations","text":"<p>Inaccurate metadata is worse than no metadata. Implementers should make it as regular and easy as possible for the user to set required CLF metadata when writing a file. Accurate metadata is critical for other users to be able to understand the intended usage of the CLF file, especially the <code>Description</code>, <code>InputDescriptor</code>, and <code>OutputDescriptor</code> tags. If known by the application, the application should fill in the <code>InputDescriptor</code> and <code>OutputDescriptor</code> tags automatically. At this time, no standard list of values (i.e., text strings) for color spaces or other common settings is defined. </p> <p>When writing a CLF to represent an ACES Look Transform, the CLF should adhere to the structure and metadata described in Annex F.</p> <p>If translating an ACES CTL (Color Transformation Language) file into CLF, set the <code>ACESTransformID</code> and <code>ACESUserName</code> (under the <code>Info</code> block of metadata) using the corresponding strings from the CTL header. [Examples 13] and [14] in [section 6] of the specification show CLFs using this feature.</p>"},{"location":"guides/clf/#helpful-hints-for-a-successful-implementation","title":"Helpful Hints for a Successful Implementation","text":""},{"location":"guides/clf/#matrix-order","title":"<code>Matrix</code> Order","text":"<p>Take note that the order of coefficients in the <code>Matrix</code> ProcessNode follows the usual convention in color science but that this is the transposition of both the order sometimes used in computer graphics and the order used in CTL. The <code>Matrix</code> section of the CLF Specification clearly documents the ordering of matrix coefficients that must be used. Also, note that the 3x4 matrix includes an offset value after each of the three matrix values.</p>"},{"location":"guides/clf/#lut3d-serialization-order","title":"<code>LUT3D</code> Serialization Order","text":"<p>As described in the <code>LUT3D</code> section of the CLF Specification, take note that the <code>LUT3D</code> ProcessNode serializes the LUT entries in blue-fastest order. This is a commonly used ordering (for example it is used by the common .3dl format) but some other formats use red-fastest ordering (e.g., .cube format).</p>"},{"location":"guides/clf/#gamma-polarity","title":"Gamma Polarity","text":"<p>As described in the <code>Exponent</code> section of the CLF Specification, take note that the <code>Exponent</code> ProcessNode uses the specified parameter directly in the power function for the forward direction. This is the same usage as in an ASC CDL power. But take care since often \"gamma\" operators in color processing software apply the inverse of the power for the forward direction.</p>"},{"location":"guides/clf/#bit-depth-attributes-dont-affect-processing-precision","title":"Bit-Depth Attributes Don't Affect Processing Precision","text":"<p>As called out in the CLF specification, the <code>inBitDepth</code> and <code>outBitDepth</code> attributes of <code>ProcessNodes</code> are not intended to control the processing precision, which should normally be 32-bit floating-point. Rather, these attributes simply determine the scaling of various parameter values such as matrix and LUT entries. Please refer to the Bit Depth section of the CLF Specification for the details.</p>"},{"location":"guides/clf/#conversion-between-bit-depths","title":"Conversion Between Bit-Depths","text":"<p>When interpreting the <code>inBitDepth</code> and <code>outBitDepth</code> attributes, conversions happen using \"power of two minus 1\" scaling rather than \"power of 2\" scaling. Please refer to the section on conversion between bit depths in the CLF Specification for the details.</p>"},{"location":"guides/clf/#appendices","title":"Appendices","text":""},{"location":"guides/clf/#annexA","title":"Annex A: Test Image","text":"<p>A 16-bit OpenEXR test image was designed with many ramps and other values which should be useful for testing any CLF and/or CLF implementation. The image includes:</p> <ul> <li>33x33 cube spanning -1.0 to 1.0 </li> <li>33x33 cube spanning -65504 to 65504</li> <li>an ACES2065-1 ColorChecker chart</li> <li>0-1 grayscale ramps</li> <li>ColorChecker values and primaries/secondaries ramped in \u00bd stop increments</li> <li>a set of ramps designed to generate a spiderweb when viewed on a vectorscope</li> <li>extents lattice ramps designed to produce a bounding box around all possible normal positive and negative values when viewed in 3D</li> </ul> <p>Specific details for each of the image subsections can be found in the README at https://github.com/alexfry/CLFTestImage</p> <p>The test image (named <code>CLF_Finishing_SourceImage_v008.exr</code>) is included along with the processed reference images in the download referenced in Annex C and D.</p>"},{"location":"guides/clf/#annexB","title":"Annex B: CLF Test Suite Listing","text":"<p>These test files may be found in the OpenColorIO repository on GitHub: https://github.com/AcademySoftwareFoundation/OpenColorIO</p> <p>The files are in the sub-directory: <code>OpenColorIO/tests/data/files/clf</code></p> <p>Note: Some of the test files intentionally use unusual or difficult syntax to give a thorough test for parsers. They are not all intended as \"best practice\" examples.</p>"},{"location":"guides/clf/#legal-test-files","title":"Legal test files","text":"<p>An implementation should be able to load and process these files successfully.</p> Index Filename Ops tested Description Notes 0010 aces_to_video_with_look.clf Matrix, Log, CDL, Lut3D ACES2065-1 to ACEScct, CDL, then ACES Output Transform Interpolation=tetrahedral 0020 cdl_all_styles.clf CDL One ASC CDL of each style 0030 cdl_clamp_fwd.clf CDL Basic single CDL Has newlines in Slope element 0040 cdl_missing_sat.clf CDL Basic single CDL without SatNode 0050 cdl_missing_sop.clf CDL Basic single CDL without SOPNode 0060 cdl_missing_style.clf CDL Basic single CDL, relying on default style 0070 difficult_syntax.clf Matrix, Lut1D Legal file with lots of unusual formatting to stress parsers 0080 exponent_all_styles.clf Exponent, Range One Exponent of each style 0090 info_example.clf Matrix Info metadata element with app-specific elements 0100 inverseOf_id_test.clf Lut1D Example of inverseOf ProcessList attribute 0110 log_all_styles.clf Log, Range At least one Log op of each style and LogParams usage 0120 lut1d_32f_example.clf Lut1D Basic 65x1 32f/32f Lut1D Array is monotonic decreasing 0130 lut1d_comp.clf Lut1D Two Lut1D ops, 2x1 8i/8i and 32x3 8i/32f Channels of the 32x3 are unequal 0140 lut1d_example.clf Lut1D Basic 65x1 8i/12i Lut1D Array values exceed nominal 12i range and must not be clamped 0150 lut1d_half_domain_raw_half_set.clf Lut1D Lut1D 65536x1 16f/16f using halfDomain and rawHalfs 0160 lut1d_long.clf Lut1D Lut1D 131072x1 32f/16f Array contains occasional duplicate/quantized values 0170 lut3d_17x17x17_10i_12i.clf Lut3D Typical Lut3D 17x17x17 10i/12i No interpolation set (should use trilinear) 0180 lut3d_bizarre.clf Lut3D Unusual 3x3x3 10i/10i Lut3D, interpolation=tetrahedral Array values exceed nominal 10i range and must not be clamped 0190 lut3d_identity_12i_16f.clf Lut3D Basic identity 2x2x2 12i/16f Lut3D Interpolation=tetrahedral 0200 lut3d_preview_tier_test.clf Lut3D Typical Lut3D 33x33x33 32f/10i, tetrahedral This is the file used for the Preview-tier processing test. Values clamped to [4,1019]. 0210 matrix_3x4_example.clf Matrix Matrix 3x4 10i/12i, includes \"+\" and \"e-1\" in Array The 10i/12i depths requires normalizing the Array values by 1023/4095 0220 matrix_example_utf8.clf Matrix Matrix 3x3 32f/32f Matrix Description uses non-ASCII characters 0230 matrix_example.clf Matrix Matrix 3x3 32f/32f Uses the legal dim=\"3 3 3\" syntax allowed for CLF v2 compatibility 0240 matrix_no_newlines.clf Matrix Matrix 3x4 10i/10i, compact formatting with no newlines 0250 matrix_windows.clf Matrix Identity Matrix 3x3 16f/12i, Windows line-endings Per section 3.2.6 of the spec, Linux newlines should be used. But for maximum robustness, ideally other line-endings should be tolerated 0260 multiple_ops.clf All ops Tests at least one of all ops 0270 range_test1_clamp.clf Range Range 8i/32f with Clamp style The 8i minIn/maxIn values require normalizing by 1/255 0280 range_test1_noclamp.clf Range Range 8i/32f with noClamp style 0290 range_test2.clf Range Range 32f/16f, minValue only, with Clamp style 0300 range.clf Range Range 16i/16i, relying on default style (Clamp) The 16i values require normalizing by 1/65535 0310 tabulation_support.clf Lut3D Lut3D 3x3x3 10i/10i, interpolation=tetrahedral The Array RGB values are only separated by tabs 0320 xyz_to_rgb.clf Matrix, Range, Lut1D Matrix 3x3 32f/32f, Range, and Lut1D 128x3 32f/32f The Lut1D Array columns are unequal"},{"location":"guides/clf/#illegal-test-files","title":"Illegal test files","text":"<p>These files should not load successfully and the implementation should indicate that it is not a legal file. The files are in the directory: <code>OpenColorIO/tests/data/files/clf/illegal</code></p> Index Filename Ops tested Description Notes 0010 array_bad_dimension.clf Matrix Array dim attribute has 10 numbers 0020 array_bad_value.clf Matrix Array has \"P\" for the 4th value rather than a number 0030 array_missing_values.clf Matrix Array has 3 values instead of 9 0040 array_too_many_values.clf Matrix Array has 18 values instead of 9 0050 cdl_bad_power.clf CDL One of the power values is 0 0060 cdl_bad_sat.clf CDL Saturation has 2 values instead of 1 0070 cdl_bad_slope.clf CDL Slope has 2 values rather than 3 0080 cdl_bad_style.clf CDL Style is \"invalid_cdl_style\" 0090 cdl_missing_offset.clf CDL The SOPNode is missing the Offset element 0100 cdl_missing_power.clf CDL The SOPNode is missing the Power element 0110 cdl_missing_slope.clf CDL The SOPNode is missing the Slope element 0120 exponent_bad_param.clf Exponent The basicFwd style may not use the offset attribute 0130 exponent_bad_value.clf Exponent The monCurveFwd style requires exponent to be &gt;= 1 0140 image_png.clf None File is a binary PNG image, not actually a CLF file 0150 indexMap_test2.clf Lut3D The IndexMap element is no longer allowed in CLF v3 0160 log_bad_param.clf Log The linToLog style may not contain the linSideBreak parameter 0170 log_bad_style.clf Log Style is \"invalidStyle\" 0180 log_bad_version.clf Log The compCLFversion = 2.0, Log was introduced in v3 0190 log_missing_breakpnt.clf Log The cameraLogToLin style must have the linSideBreak parameter 0200 lut1d_half_domain_missing_values.clf Lut1D A half-domain LUT must have 65536 values 0210 lut1d_half_domain_set_false.clf Lut1D The halfDomain attribute may only have the value \"true\" 0220 lut1d_raw_half_set_false.clf Lut1D The rawHalfs attribute may only have the value \"true\" 0230 lut3d_unequal_size.clf Lut3D The Array dimension is 2x2x3x3 0240 matrix_end_missing.clf Matrix There is no  element 0250 process_list_bad_version.clf Matrix The compCLFversion = \"three\" 0260 process_list_higher_version.clf Matrix The compCLFversion = \"3.2\" 0270 process_list_missing.clf Matrix The ProcessList element is missing 0280 range_bad_noclamp.clf Range The noClamp style must have both min and max values 0290 range_bad_values.clf Range The minInValue must be less than the maxInValue 0300 range_empty.clf Range The Range element must have at least min or max values 0310 range_nonmatching_clamp.clf Range A one-sided Range must use the same normalized value for in and out Because the bit-depths differ, the values are not actually the same 0320 transform_bad_outdepth.clf Matrix The outBitDepth is \"16d\" 0330 transform_bitdepth_mismatch.clf Matrix, Lut1D The inBitDepth does not match the previous outBitDepth Also missing the XML header (though this is legal) 0340 transform_corrupted_tag.clf Matrix The closing ProcessList element is incorrect 0350 transform_element_end_missing.clf Matrix The closing ProcessList element is missing 0360 transform_empty.clf None The ProcessList must have at least one ProcessNode operator 0370 transform_id_empty.clf Log The ProcessList id attribute must not be empty 0380 transform_missing_id.clf Log The ProcessList id attribute must be present 0390 transform_missing_inbitdepth.clf Lut1D The inBitDepth attribute is missing 0400 transform_missing_outbitdepth.clf Lu1D The outBitDepth attribute is missing 0410 transform_missing.clf None There is nothing except the XML header 0420 unknown_elements.clf Matrix, Lut1D, Lut3D The ProcessList contains unknown elements outside the Info element"},{"location":"guides/clf/#annexC","title":"Annex C: Finishing Tier Apply Test CLF List","text":"<p>The list of CLF files for the Finishing Tier apply test is the complete list of legal files in Annex B.</p> <p>Processed reference images may be downloaded from here: CLF Apply Test Images</p>"},{"location":"guides/clf/#annexD","title":"Annex D: Preview Tier Apply Test CLF List","text":"<p>The CLF file for the Preview Tier apply test is simply:  <code>lut3d_preview_tier_test.clf</code></p> <p>Processed reference images may be downloaded from here: CLF Apply Test Images</p>"},{"location":"guides/clf/#annexE","title":"Annex E: Baking a CLF for Preview Tier Implementation","text":"<p>Because CLF allows a fairly powerful set of processing operators that may be assembled into pipelines of any ordering and any length, it will not be possible to exactly evaluate all CLFs on hardware or software that has a fixed processing pipeline. (In other words, the implementation must allow the CLF file itself to specify the pipeline of operators for a given color transform.)</p> <p>Converting color transforms into a simpler structure to make them simpler to evaluate is known as baking. This is often done to meet the needs of a particular hardware implementation. </p> <p>There are a number of factors that make the process of accurately baking color transforms a difficult and complicated subject:</p> <ul> <li>CLF is intended to support the needs of floating-point scene-linear color spaces and therefore the range of possible input and output values extends from very small to very large numbers (both positive and negative).</li> <li>A given CLF may expect virtually any color space (scene-linear, camera log, video, etc.) on input and another completely different space on output.</li> <li>The flexible nature of CLF and the way a chain of operators is built up makes it fairly easy for the function to have abrupt changes in slope. For example, at the edge of a gamut boundary, or due to a conversion into an internal working space for a look transform. These slope changes are usually not captured accurately when the transform is baked.</li> <li>The human visual system is often able to detect fairly small errors in color reproduction, particularly in dark colors.</li> </ul> <p>All of that said, there are many successful products that have used a baking process. For example, this technique is often used in products designed for on-set production monitoring involving look transforms. So it is highly likely that baking of CLFs could be a successful strategy for Preview Tier products. The key will be to clearly document the types of color transforms that may be successfully baked and those that the user should avoid.</p> <p>Integer-based implementations in the Preview Tier will typically be processing video or logarithmic color space encodings. So for example, if the documentation suggests avoiding use of CLFs expecting scene-linear color spaces on input, that is probably fine since (hopefully!) no one will be trying to send raw scene-linear values through an integer connection such as SDI video.</p> <p>The accuracy of the baking process may be judged by sending images through both the original CLF and the baked CLF and comparing them. OpenColorIO or any product that passes the Finishing Tier tests could be used for this type of comparison. But keep in mind that the result will depend on many factors, including the input and output color spaces and the internal structure of the original CLF. So even though one CLF may bake accurately through a given baking process, it certainly does not mean that all of them will. Testing with a range of user transforms is essential.</p> <p>OpenColorIO may be used to experiment with baking CLFs using the command-line tool <code>ociobakelut</code>. For example, here is a command that takes an original CLF named <code>complicated.clf</code> and bakes it into a single <code>LUT3D</code> with a cube dimension of 33x33x33 called <code>simple.clf</code>:</p> <pre><code>$ ociobakelut --lut complicated.clf --cubesize 33 --format \"Academy/ASC Common LUT Format\" --v simple.clf\n</code></pre> <p>You may edit the resulting CLF XML file in a text editor to add the <code>interpolation=\"tetrahedral\"</code> attribute to the <code>LUT3D</code> and any desired metadata.</p> <p>For CLFs that convert from one perceptually uniform color space to another (i.e., between most logarithmic and video color spaces), this will often be reasonably accurate for Preview Tier devices. Increasing the cube size will improve accuracy (the default <code>cubesize</code> is 64x64x64).</p> <p>However, this would not work well for baking a CLF that expects a scene-linear color space on input. In those situations, the usual technique is to add some kind of a \"shaper\" <code>LUT1D</code> or other non-linear transform in front of the <code>LUT3D</code> that will convert the linear values into something more perceptually uniform. A more modern and compact technique that takes advantage of the CLF capabilities would be to insert a <code>Log</code> ProcessNode rather than a <code>LUT1D</code>, but the most appropriate technique would be based on the needs of the given implementation.</p> <p>The <code>ociobakelut</code> tool is able to bake with shaper LUTs, but it requires an OCIO config file to define the input, output, and shaper spaces. But for the use-case here, the input is just a single CLF file, so there is no OCIO config file to use. OpenColorIO could still be used to do these more advanced types of baking, but it would require some scripting. One approach would be to create a config file that references the original CLF as a file transform and includes an appropriate shaper space. Another approach would be to just use the OCIO API directly to write your own baking tool, using the <code>ociobakelut</code> code for inspiration. (And if so, we encourage you to contribute it back to the OCIO project!)</p> <p>The <code>ociobakelut</code> command supports many arguments; use the <code>-h</code> argument for a summary. For example, note that you may supply many <code>--lut</code> arguments on the command line and they will all be baked together into the result. You may also consult the Baking LUT\u2019s section of the OCIO documentation for a tutorial on using <code>ociobakelut</code>.</p>"},{"location":"guides/clf/#annexF","title":"Annex F: Using CLF to represent ACES Look Transforms","text":"<p>Historically, look workflows have been based on applying a look in some kind of logarithmic color space, for example, a camera log space. This is partly because the existing infrastructure was built for integer pixel formats and did not support floating-point pixel formats. (The Preview Tier described above is an attempt to define requirements for these integer-based devices.) And until CLF, previous LUT formats did not support scene-linear color spaces well.</p> <p>However, the input values and output values of ACES Look Transforms (also known as \"LMTs\") must be ACES2065-1. This is to maintain universality of Look Transforms and not link them to project-specific working spaces. Look Transforms may then convert ACES2065-1 to some other working space internally (e.g., a camera log space) for look application. This is the vision for the future and the ACES Metadata File (AMF) format expects implementations to work this way.</p> <p>As an aside, it is recommended that the <code>InputDescriptor</code> and <code>OutputDescriptor</code> be set to ACES2065-1 when authoring Look Transforms as CLFs so it is always clear what the expected input and output color spaces are. The <code>Description</code> tag and other metadata may also be used to provide a more complete description of the look.</p> <p>When look operations are to be performed in a working space other than ACES2065-1, then appropriate conversions to and from the required working space can be prepended and appended within a CLF to communicate the transform. For accuracy, whenever possible, these conversions should be implemented using discrete operations such as <code>Log</code> and <code>Matrix</code> ProcessNodes rather than <code>LUT1D</code> or <code>LUT3D</code>. </p> <p>By using ProcessNodes such as <code>Log</code> and <code>Matrix</code>, a CLF author makes it easier for an implementation to detect and remove any unnecessary conversions when applying the CLF. For example, OpenColorIO will do this when optimizing transform chains.</p> <p>The OCIO tool <code>ociomakeclf</code> can create an ACES Look Transform by prepending and appending the appropriate color space conversions to an existing look LUT file. For example, if an existing look LUT expects ACEScct input and outputs ACEScct, the <code>--csc ACEScct</code> option will add appropriate conversions from ACES2065-1 to ACEScct at the beginning and from ACEScct back to ACES2065-1 at the end.</p> <p>Example:</p> <p>Convert the look <code>CDL cdl_test2.cc</code> that expects and produces ACEScct values to an ACES Look Transform in CLF format that expects and produces ACES2065-1 values: <pre><code>$ ociomakeclf OpenColorIO/tests/data/files/cdl_test2.cc LMT.clf --csc ACEScct\n</code></pre></p> <p>This generates the following CLF:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;ProcessList compCLFversion=\"3\" id=\"669980ac1ecd97ab18c1707250a13d20\"&gt;\n\n&lt;!-- Header metadata --&gt;\n    &lt;Description&gt;ACES LMT transform built from a look LUT expecting color space: ACEScct&lt;/Description&gt;\n    &lt;Description&gt;Original LUT name: OpenColorIO/tests/data/files/cdl_test2.cc&lt;/Description&gt;\n    &lt;InputDescriptor&gt;ACES2065-1&lt;/InputDescriptor&gt;\n    &lt;OutputDescriptor&gt;ACES2065-1&lt;/OutputDescriptor&gt;\n\n&lt;!-- Convert from ACES2065-1 to ACEScct --&gt;\n    &lt;Matrix inBitDepth=\"32f\" outBitDepth=\"32f\"&gt;\n            &lt;Array dim=\"3 3\"&gt;\n 1.45143931614567    -0.23651074689374   -0.214928569251925\n-0.0765537733960206 1.17622969983357   -0.0996759264375522\n 0.00831614842569772 -0.00603244979102103 0.997716301365323\n            &lt;/Array&gt;\n    &lt;/Matrix&gt;\n    &lt;Log inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"cameraLinToLog\"&gt;\n            &lt;LogParams base=\"2\" linSideSlope=\"1\" linSideOffset=\"0\" logSideSlope=\"0.0570776255707763\" logSideOffset=\"0.554794520547945\" linSideBreak=\"0.0078125\" /&gt;\n    &lt;/Log&gt;\n\n&lt;!-- Apply the user's CDL look in the ACEScct working space --&gt;\n    &lt;ASC_CDL id=\"cc0001\" inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"FwdNoClamp\"&gt;\n        &lt;SOPNode&gt;\n            &lt;Slope&gt;1.0 1.0 0.9&lt;/Slope&gt;\n            &lt;Offset&gt;-0.03 -0.02 0.0&lt;/Offset&gt;\n            &lt;Power&gt;1.25 1.0 1.0&lt;/Power&gt;\n        &lt;/SOPNode&gt;\n        &lt;SatNode&gt;\n            &lt;Saturation&gt;1.7&lt;/Saturation&gt;\n        &lt;/SatNode&gt;\n    &lt;/ASC_CDL&gt;\n\n&lt;!-- Convert from ACEScct back to ACES2065-1 --&gt;\n    &lt;Log inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"cameraLogToLin\"&gt;\n            &lt;LogParams base=\"2\" linSideSlope=\"1\" linSideOffset=\"0\" logSideSlope=\"0.0570776255707763\" logSideOffset=\"0.554794520547945\" linSideBreak=\"0.0078125\" /&gt;\n    &lt;/Log&gt;\n    &lt;Matrix inBitDepth=\"32f\" outBitDepth=\"32f\"&gt;\n            &lt;Array dim=\"3 3\"&gt;\n  0.695452241357452    0.140678696470294    0.163869062172254\n  0.0447945633720377   0.859671118456422    0.0955343181715404\n -0.00552588255811354  0.00402521030597866  1.00150067225213\n            &lt;/Array&gt;\n    &lt;/Matrix&gt;\n&lt;/ProcessList&gt;\n</code></pre> <p>Note that an ACES Look Transform is usually just one component in a larger pipeline of transforms. For example, Preview Tier implementations involving on-set LUT boxes will typically incorporate the Look Transform into a chain of transforms that expect a camera log color space on input and produce a video color space on output. As noted above, the use of explicit <code>Log</code> and <code>Matrix</code> ProcessNodes in a CLF transform allows an implementation to detect unnecessary operations. For example, if an ACES Input Transform were added in front of an ACES Look Transform that begins with the inverse of that Input Transform, the redundant operations could be optimized out for efficiency. Though in some implementations, this will all be baked into a single <code>LUT3D</code>, making such optimizations less necessary.</p>"},{"location":"guides/clf/#annexG","title":"Annex G: Using CLF with AMF","text":"<p>The ACES Metadata File (AMF) is a \"sidecar\" XML file designed to encapsulate the metadata required to recreate ACES viewing pipelines. An AMF can carry references to one or more CLF files as external ACES Look Transform files. </p> <p>When using a CLF file for ACES applications and especially when in conjunction with AMF, it is recommended that the CLF <code>id</code> attribute of the <code>ProcessList</code> be populated with a <code>uuid</code>. </p> <p>Multiple CLFs can be included by using multiple <code>lookTransform</code> elements in the AMF file. The CLFs are applied in the order in which they appear in the AMF.</p> <p>See the AMF Handbook for more details.</p>"},{"location":"guides/clf/#annexH","title":"Annex H: Identity (no-op) CLF example","text":"<p>To avoid doubt in cases where CLF files are exchanged as \u201csidecar\u201d files to batches of media, it may be desirable to use a standard CLF form even when no transform or color modification has been assigned to certain media clips / files. In these cases it may be useful that a \u201cNo-Op\u201d CLF file be included as the associated sidecar file to communicate to downstream users that no transform is assigned. This may be preferred to not providing a sidecar CLF file, as the absence of CLF files for a subset of a larger media batch often raises questions in certain workflows about whether an expected file is missing.</p> <p>For this situation, implementers may generate a minimal CLF file like that shown below, where the <code>ProcessList</code> has a single <code>ProcessNode</code> of the <code>Matrix</code> type with a 3x3 identity matrix. (Implementers do not need to copy this exactly, for example there are other operators that also give an identity and details like the <code>id</code> string will differ.)</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;ProcessList id=\"37efa85b-5bc8-40dc-8ffe-b488a3c013ea\" name=\"No-Op\" compCLFversion=\"3.0\"&gt;\n    &lt;Description&gt;No-Op CLF&lt;/Description&gt;\n    &lt;Matrix name=\"identity matrix\" inBitDepth=\"32f\" outBitDepth=\"32f\"&gt;\n        &lt;Array dim=\"3 3\"&gt;\n          1.0 0.0 0.0\n          0.0 1.0 0.0\n          0.0 0.0 1.0\n        &lt;/Array&gt;\n    &lt;/Matrix&gt;\n&lt;/ProcessList&gt;\n</code></pre>"},{"location":"guides/clf/#annexI","title":"Annex I: Infinity and NaN handling","text":"<p>The CLFs used for the apply tests were specially chosen to avoid floating-point overflows that could turn pixels in the test target image into infinity or NaN. However, with other CLFs, the comparison script described in the other sections may fail if the resulting images have infinity or NaN pixels.</p> <p>The handling of infinity and NaN is challenging in a number of regards. For example:</p> <ul> <li>There is not always consensus on what the ideal behavior is. For example, should overflows be clamped to a large finite value or allowed to produce an infinity.  </li> <li>There are sometimes performance penalties to be extra rigorous about the handling which are not always warranted.  </li> <li>The handling on GPUs tends to vary a lot and often does not match the CPU (where there are clearer specs for what is supposed to happen).  </li> <li>Once these values get into a chain of color processing, there can be somewhat unexpected results. For example, <code>(Inf \u2013 Inf)</code> is NaN, likewise, <code>(0 x Inf)</code> is NaN, so quite often an infinity going into a matrix or other operator that mixes the channels results in NaNs coming out.</li> </ul> <p>Due to issues such as these, neither the Academy reference CTL for ACES transforms nor OpenColorIO have ideal behavior with respect to the handling of floating-point infinity and NaN. We expect this is true of most other practical implementations and so this initial version of the Implementation Guide does not attempt to validate handling of these values.</p>"},{"location":"guides/clf/#annexJ","title":"Annex J: Python code for creating the Preview Tier test LUT","text":"<p>Here is some Python code that uses OCIO to create the test LUT for the Preview Tier.</p> <pre><code># This script builds the file for the Preview Tier test from the CLF test suite.\n\nimport PyOpenColorIO as ocio\nimport numpy as np\n\nv = np.linspace(0,1,33)\n# v =\n# array([ 0.    ,  0.03125,  0.0625 ,  0.09375,  0.125  ,  0.15625,\n#       0.1875 ,  0.21875,  0.25   ,  0.28125,  0.3125 ,  0.34375,\n#       0.375  ,  0.40625,  0.4375 ,  0.46875,  0.5 ,  0.53125,\n#       0.5625 ,  0.59375,  0.625  ,  0.65625,  0.6875 ,  0.71875,\n#       0.75   ,  0.78125,  0.8125 ,  0.84375,  0.875  ,  0.90625,\n#       0.9375 ,  0.96875,  1.  ])\n\n# Convert the vector into the values for a Lut3D.  Uses blue-fastest order.\na1, a2, a3 = np.meshgrid(v,v,v)\ngr = np.column_stack((a2.ravel(), a1.ravel(), a3.ravel()))\n\n# At this point, gr is a 35937 x 3 array with min=0 and max=1.\n\n# Build transforms to convert from ACEScct to Rec.709 using an ACES Output Transform.\nbt1 = ocio.BuiltinTransform(style='ACEScct_to_ACES2065-1')\nbt2 = ocio.BuiltinTransform(style='ACES-OUTPUT - ACES2065-1_to_CIE-XYZ-D65 - SDR-VIDEO_1.0')\nbt3 = ocio.BuiltinTransform(style='DISPLAY - CIE-XYZ-D65_to_REC.1886-REC.709')\ndvt = ocio.GroupTransform( [bt1, bt2, bt3] )\n\n# Set up an OCIO Processor and process the values.\nconfig = ocio.Config.CreateRaw()\nproc = config.getProcessor(dvt)\ncpu = proc.getDefaultCPUProcessor()\ntmp = gr.astype(np.float32)\ncpu.applyRGB(tmp)   # replaces tmp with output\n\n# For this example, the goal was to quantize to 10-bits and clamp to [4,1019]\n# since hardware involving SDI video may not be able to process these values.\nvals = np.round(tmp * 1023)\n# vals[0:10,:] =\n# array([[   0.,   0.,    0.],\n#        [   0.,   0.,    0.],\n#        [   0.,   0.,    0.],\n#        [   0.,   0.,    0.],\n#        [   0.,   0.,   35.],\n#        [   0.,   0.,   52.],\n#        [   0.,   0.,   68.],\n#        [   0.,   0.,   88.],\n#        [   0.,   0.,  114.],\n#        [   0.,   0.,  148.]], dtype=float32)\nvals2 = vals.clip(4,1019)\n\n# Now renormalize to [0,1], vectorize, and set the data into a Lut3D.\nvals2 = vals2 / 1023.\nlut = ocio.Lut3DTransform()\nlut.setData(vals2.ravel())\n\n# Set some LUT attributes.\nlut.setFileOutputBitDepth(ocio.BIT_DEPTH_UINT10)\nlut.setInterpolation(ocio.INTERP_TETRAHEDRAL)\nfmd = lut.getFormatMetadata()\nfmd.addChildElement('Description','This LUT is an ACEScct to Rec.709 Output Transform, clamped to [4,1019]')\n# print(lut)\n# &lt;Lut3DTransform direction=forward, fileoutdepth=10ui, interpolation=tetrahedral, \n#  gridSize=33, minrgb=[0.00391007 0.00391007 0.00391007], maxrgb=[0.99609 0.99609 \n#  0.99609]&gt;\n\n# Add the LUT to a GroupTransform, set the metadata and write.\ngrp = ocio.GroupTransform()\ngrp.appendTransform(lut)\nfmdg = grp.getFormatMetadata()\nfmdg.setID('00001')\nfmdg.setName('Preview-tier Lut3D test')\nfmdg.addChildElement('Description','Test LUT for Preview-tier CLF validation test suite')\nfmdg.addChildElement('InputDescriptor','ACEScct')\nfmdg.addChildElement('OutputDescriptor','Rec.1886 / Rec.709 video')\n# print(grp)\n# &lt;GroupTransform direction=forward, transforms=\n#       &lt;Lut3DTransform direction=forward, fileoutdepth=10ui, interpolation=tetrahedral, \n#       gridSize=33, minrgb=[0.00391007 0.00391007 0.00391007], \n#       maxrgb=[0.99609 0.99609 0.99609]&gt;&gt;\n\ngrp.write(formatName='Academy/ASC Common LUT Format', config=config, fileName='/tmp/lut3d_preview_tier_test.clf')\n</code></pre>"},{"location":"guides/clf/#annexK","title":"Annex K: Generation of the reference images","text":"<p>The processed reference images may be downloaded from the URL provided in Annex C and D above. This section documents how those images were generated.</p>"},{"location":"guides/clf/#finishing-tier-reference-images","title":"Finishing Tier Reference Images","text":"<p>Since OpenColorIO is an actively maintained open source implementation of CLF, it has been used to generate the processed reference images. There is a Python script in the OCIO repository that may be run to generate these images. The script is run as follows (these steps assume a Linux or Mac platform but could also be done on Windows):</p> <ol> <li>Install OpenImageIO</li> <li>Install OpenColorIO from source, following the instructions in the OCIO documentation available from opencolorio.org.  Given that OIIO was installed in step 1, this should build the ocioconvert command-line tool, which is needed by the Python script.</li> <li>In a shell, cd to the directory where OpenColorIO was installed and then cd into the sub-directory share/clf. This directory contains the Python script as well as a copy of the source target image described in Annex A.</li> <li>In the shell, type: <pre><code>$ python process_clf_test_frames.py &lt;OUTPUT-DIR&gt;\n</code></pre> replacing <code>&lt;OUTPUT-DIR&gt;</code> with a directory that you want to write the images to.</li> </ol> <p>(There are also some other optional arguments that are described if you run the script with just a \"-h\" option, but these are not necessary to generate the reference image set.)</p>"},{"location":"guides/clf/#preview-tier-source-image","title":"Preview Tier Source Image","text":"<p>The Preview Tier test uses an integer source image in DPX format. That image was generated from the main Finishing Tier floating-point source image by applying a conversion from ACES2065-1 to ACEScct. It may be generated using the command-line tool <code>oiiotool</code> that is included with OpenImageIO using the following steps (these steps assume a Linux or Mac platform but could also be done on Windows):</p> <ol> <li>Install a recent version of OpenImageIO and <code>oiiotool</code>, compiled with OpenColorIO support.</li> <li>Download the OpenColorIO source code, which contains an OCIO config file to use with <code>oiiotool</code>.</li> <li>In a shell, <code>cd</code> to the directory where OpenColorIO was installed and then <code>cd</code> into the sub-directory <code>share/clf</code>. This directory contains a copy of the source target image described in Annex A.</li> <li>Set the OCIO environment variable to point to the following config file in the OCIO repository: <code>docs/configurations/ocio-v2_demo.ocio</code></li> <li>Run the command:  <pre><code>$ oiiotool CLF_Finishing_SourceImage_v008.exr --colorconvert ACES2065-1 ACEScct -d uint10 -o CLF_Preview_SourceImage_v008.dpx\n</code></pre></li> </ol>"},{"location":"guides/clf/#preview-tier-reference-image","title":"Preview Tier Reference Image","text":"<p>The Preview Tier reference image may then be generated as follows:</p> <ol> <li>Follow steps 1-5 in the previous section. At this point the current directory will be the <code>share/clf</code> sub-directory of the OpenColorIO source code.</li> <li>Run the command:  <pre><code>$ oiiotool CLF_Preview_SourceImage_v008.dpx --ociofiletransform ../../tests/data/files/clf/lut3d_preview_tier_test.clf -d uint10 -o lut3d_preview_tier_test.dpx\n</code></pre></li> </ol>"},{"location":"guides/rgc-implementation/","title":"ACES Reference Gamut Compression Implementation Guide","text":""},{"location":"guides/rgc-implementation/#scope","title":"Scope","text":"<p>The purpose of this document is to detail and define standards for user interface and experience, workflow, tolerances, and tracking of the ACES Reference Gamut Compression (RGC) published in ACES 1.3. For detailed technical specifications, please refer to the ACES Gamut Mapping Architecture VWG - Technical Documentation.</p>"},{"location":"guides/rgc-implementation/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>ACES Gamut Mapping Architecture VWG - Technical Documentation Deliverable</li> </ul>"},{"location":"guides/rgc-implementation/#introduction","title":"Introduction","text":"<p>The ACES Reference Gamut Compression was introduced to help solve ACES user issues with out of gamut (negative) pixels introduced either in the conversion from camera raw RGB via an Input Transform (IDT) into ACES AP0 or in the conversion from ACES AP0 into ACES AP1 (ACEScg and ACEScct). These out of gamut pixel values are problematic when their negative components cause issues in compositing, and may also produce visual artifacts when viewed through an ACES Output Transform.</p> <p> </p> User Testing Footage Examples"},{"location":"guides/rgc-implementation/#target-audience","title":"Target Audience","text":"<p>This document is targeted at software developers and product managers looking to integrate the Reference Gamut Compression into their software package or library. It will focus on the \u201chow\u201d as opposed to the \u201cwhy\u201d, which is covered in the architecture documentation above.</p>"},{"location":"guides/rgc-implementation/#workflow-recommendations","title":"Workflow Recommendations","text":"Workflow <p>As visualized in the flowchart above, it is recommended (in the current absence of AMF for tracking) that most productions utilize the gamut compression in every area - from on set all the way to finishing.  This means that at this time, the RGC is \u201calways on\u201d by default in any viewing pipeline.  Following the general ACES workflow philosophy, the RGC is only baked into image data at the appropriate stage in the pipeline - which varies based on the needs of the production, as outlined in the flow chart above. For further workflow specifications, please refer to the User Guide.</p>"},{"location":"guides/rgc-implementation/#reference-implementation-specifications","title":"Reference Implementation Specifications","text":""},{"location":"guides/rgc-implementation/#versioning-and-naming","title":"Versioning and Naming","text":"<p>The Reference Gamut Compression published in ACES 1.3 uses the following ACES Transform ID and ACES User Name in the CTL:</p> <pre><code>&lt;ACEStransformID&gt;urn:ampas:aces:transformId:v1.5:LMT.Academy.ReferenceGamutCompress.a1.v1.0&lt;/ACEStransformID&gt;\n&lt;ACESuserName&gt;ACES 1.3 Look - Reference Gamut Compress&lt;/ACESuserName&gt;\n</code></pre> <p>Implementers should only make the RGC available in the UI when their application has the ACES version set to 1.3 or higher.</p>"},{"location":"guides/rgc-implementation/#project-and-clip-level-setting","title":"Project and Clip Level Setting","text":"<p>When an application has the ACES version set to 1.3 or higher, a simple check-box (defaulting to on) should be exposed in the project settings which applies the RGC to all clips in the project.</p> <p> </p> DaVinci Resolve project level RGC setting <p>An override should be provided at the clip level, so that the user can control the RGC setting for individual clips, if required.</p> <p> </p> DaVinci Resolve clip level RGC setting <p>Implementers may also choose to offer a parametric variation of the RGC (see Section 9).</p>"},{"location":"guides/rgc-implementation/#export-settings","title":"Export Settings","text":"<p>The user should be able to easily control whether rendered media will have the RGC \u201cbaked in\u201d. Options should be available so that exports can either follow the global project setting, individual clip settings, or be forced to either on or off for all clips. If an application offers export templates, then templates should be provided which force the RGC off when rendering EXRs (following the recommended workflow for VFX pulls) and follow the clip settings when rendering deliverables with an Output Transform baked in.</p>"},{"location":"guides/rgc-implementation/#test-images-and-tolerances","title":"Test Images and Tolerances","text":"Test image (without RGC) with sRGB Output Transform applied <p>To aid with validation of implementations, a test image has been created which covers a wide dynamic range (including negatives) and contains values which exceed the AP1 gamut in every direction. It also includes ColorCheckers at varying exposures, to confirm that the gamut compression does not alter colors within the \u201czone of trust\u201d.</p> <p>The Python code to create the test image can be found in this Google Colab. The resulting test file can be downloaded from here, and the test image processed through the CTL implementation of the RGC can be downloaded from here  (Note: <code>ctlrender</code> adds an alpha channel to the result, which can be ignored.)</p> <p>For comparison of an implementation with the reference, a relative error metric has been defined (see Appendix A).</p> <p>The command-line application <code>oiiotool</code>, which is installed as a component of OpenImageIO, can be used to compare pixels between two images and evaluate the metric specified above, using the following command line (with <code>test_target.exr</code>  replaced with the name of the file under test):</p> <pre><code>oiiotool gc_test_image_v007_gamut_compressed_ctlrender.exr --dup test_target.exr --absdiff --swap --abs --maxc 0.1 --div --rangecheck 0,0,0 .002,.002,.002 -o /tmp/tmp.exr\n</code></pre> <p>A match within tolerances will produce the following output:</p> <pre><code>       0  &lt; 0,0,0\n       0  &gt; .002,.002,.002\n 2073600  within range\n</code></pre> <p>Implementers are of course free to use their own code to perform validation, as long as it applies the same metric.</p>"},{"location":"guides/rgc-implementation/#tracking-via-aces-metadata-file-amf","title":"Tracking via ACES Metadata File (AMF)","text":"<p>The Reference Gamut Compression is trackable via a <code>lookTransform</code> element in an AMF file. If the RCG is used in the viewing pipeline, the <code>lookTransform</code> will be listed in the associated AMF. If the AMF is accompanying rendered media, use the <code>applied</code> flag to track whether or not the RGC has been \u201cbaked in\u201d. See below for an example AMF:</p> <pre><code>  &lt;aces:inputTransform applied=\"true\"&gt;\n    \u2026\n  &lt;/aces:inputTransform&gt;\n  &lt;aces:lookTransform applied=\"true\"&gt;\n    &lt;aces:description&gt;ACES 1.3 Look - Reference Gamut Compress&lt;/aces:description&gt;\n    &lt;aces:transformId&gt;urn:ampas:aces:transformId:v1.5:LMT.Academy. ReferenceGamutCompress.a1.v1.0&lt;/aces:transformId&gt;\n  &lt;/aces:lookTransform&gt;\n</code></pre> <p>If using the RGC in a viewing pipeline, this <code>lookTransform</code> should appear directly after the IDT, first in the list of any LMTs, to make sure other operations benefit from the gamut compression. The Transform ID outlined in the specification section should be included in any exported AMFs, with the <code>applied</code> flag set as appropriate, and the description set to the <code>ACESuserName</code> to enable proper tracking. Currently, only the Reference (i.e. static) Gamut Compression is trackable via AMF.</p>"},{"location":"guides/rgc-implementation/#parametric-version-implementation-specifications","title":"Parametric Version Implementation Specifications","text":"<p>An implementation of the gamut compression transform which exposes the parameters to the user should be treated differently than the ACES RGC and grouped with other, creative color correction operators instead.</p> <p>As a creative color correction tool, the parametric gamut compression transform is expected to be used as part of a color correction operator stack. This parametric transform can be useful to achieve a desired \u201clook\u201d or manage out-of-gamut artifacts created earlier in the process chain that were not, or could not, be addressed by the RGC.  The parametric version should not be used as a replacement for the RGC since it cannot be tracked by AMF.</p> <p>While such an operator is not explicitly endorsed, the following recommendations are made to facilitate a common user experience across implementations:</p> <p>Parameters</p> UI Label Components Slider Range Default Value(s) / Slider Detents Distance Limit Limit 3 (Cyan, Magenta, Yellow) 1.001 - 2.000 C:1.147, M:1.264, Y:1.312 Compression Threshold Threshold 3 (Cyan, Magenta, Yellow) 0.000 - 1.000 C:0.815, M:0.803, Y:0.880 Power Curve Exponent Roll-off 1 0.500 - 2.000 1.200 <p>These default values will exactly match a parametric implementation to the RGC.</p> <p> </p> DaVinci Resolve Parametric Gamut Compress Settings"},{"location":"guides/rgc-implementation/#appendices","title":"Appendices","text":""},{"location":"guides/rgc-implementation/#appendix-a-relative-metric-detail","title":"Appendix A: Relative metric detail","text":"<p>Where video and logarithmic encodings are typically sufficiently perceptually uniform that a simple absolute error metric such as <code>(actual - aim)</code> may be used, scene-linear encodings require a tolerance that is tighter for dark colors and looser for bright colors. This is due to the approximately logarithmic nature of human color perception (although the metric is actually computed per channel).</p> <p>When comparing an aim and actual value, a basic relative error metric has the form:</p> \\[\\frac{(actual - aim)}{aim}\\] <p>However this can become overly sensitive when the values being compared become very small. In the limit, when the aim value is zero, the result is either <code>infinity</code> or <code>NaN</code>. Therefore it is useful to use a \u201csafe-guarded relative error metric\u201d that places a lower bound on the denominator:</p> \\[\\frac{(actual - aim)}{max(aim, lower\\_bound)}\\] <p>This effectively transitions the error metric from being a relative error metric for bright and normal colors to an absolute error metric when approaching a certain noise floor determined by the <code>lower_bound</code> constant. A reasonable <code>lower_bound</code> constant for images in ACES2065-1 color space would be 0.1. It is also necessary to handle the case where the aim value may be negative, in which case the final error metric becomes:</p> \\[\\frac{abs(actual - aim)}{max(abs(aim), 0.1)} &lt;= 0.002\\] <p>This is essentially a relative tolerance of +/\u2013 one part in 500 above 0.1 and an absolute tolerance of +/\u2013 0.0002 below 0.1.</p>"},{"location":"guides/rgc-user/","title":"ACES Reference Gamut Compression User Guide","text":""},{"location":"guides/rgc-user/#scope","title":"Scope","text":"<p>The purpose of this document is to elaborate on suggested user workflows for on set, dailies, visual effects, and finishing using the ACES Reference Gamut Compression (RGC). For detailed technical specifications, please refer to the ACES Gamut Mapping Architecture VWG - Technical Documentation.</p>"},{"location":"guides/rgc-user/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>ACES Gamut Mapping Architecture VWG - Technical Documentation Deliverable</li> </ul>"},{"location":"guides/rgc-user/#introduction","title":"Introduction","text":"<p>A common complaint from users of ACES has been the artifacts resulting from out of gamut values in source images. These artifacts are most known for appearing in highly saturated, bright LED light sources such as police car lights, stoplights, etc - but also show up frequently in LED sources used to light scenes. In an ACES workflow, these artifacts appear at two stages - first in the conversion from camera raw RGB via an Input Transform (IDT) into ACES AP0 - and second in the conversion from ACES AP0 into ACES AP1 (ACEScg and ACEScct). These out of gamut pixel values are problematic when their negative components cause issues in compositing, and may also produce visual artifacts when viewed through an ACES Output Transform.</p> <p>A Look Modification Transform (LMT) referred to as the blue light artifact fix was created as a temporary solution, but this solution affected all pixels in the image, rather than just the problem areas. A new solution was needed which preserved colors within a \u201czone of trust\u201d, only altering the most saturated values. The Reference Gamut Compression algorithm published in ACES 1.3 is intended to replace and deprecate the Blue Light Artifact LMT.</p> <p>Various options were investigated, and the working group finally settled on a simple RGB ratio based algorithm which compresses values based on their distance from the neutral axis. This makes no attempt to ascertain the \u201ccorrect\u201d value for a pixel, since the nature of the problem means that these pixels may have no correct color. The algorithm is intended as a technical correction rather than an aesthetic look. It \u201cheals\u201d the problem pixels, to produce new RGB values which are less problematic when used in subsequent compositing or grading operations. Creative modifications are left for the user to apply as necessary downstream of the RGC.</p> <p>The ACES Reference Gamut Compression  uses fixed values <sup>1</sup> for the thresholds where compression begins, and for the amount of compression. These values have been calculated such that the colors of the ColorChecker 24 will remain unchanged, and that any colors that are within the encoding gamuts of all the commonly used digital cinema cameras (those with official ACES IDTs) will be brought within AP1, thus ensuring positive ACEScg values.  In most workflows, these constants will be invisible to the user, as demonstrated in the screenshots from Resolve 17.4 below - the user has the option to apply the RGC at a project or a clip level.</p> <p> </p> Reference Gamut Compression enabled via Project Settings in DaVinci Resolve 17.4 <p> </p> Reference Gamut Compression individual clip settings in DaVinci Resolve 17.4 <p>In the example below, artifacts such as the magenta solarization seen on the nose of the Okja toy are greatly reduced by application of the RGC.</p> <p> </p> Without the RGC <p> </p> With the RGC applied <p>Though the algorithm itself and application to an image is relatively simple, there are many considerations to discuss for overall workflows for an ACES project, from on set through to finishing.</p>"},{"location":"guides/rgc-user/#general-workflow","title":"General Workflow","text":"<p>As visualized in the flowchart above, it is recommended (in the current absence of AMF for tracking) that most productions utilize the gamut compression in every area - from on set all the way to finishing.  This means that at this time, for simplicity, the the RGC is \u201calways on\u201d by default in any viewing pipeline.  Following the general ACES workflow philosophy, the RGC is only baked in to image data at the appropriate stage in the pipeline - which varies based on the needs of your production, as outlined in the flow chart and explained below.</p> <p>Several DCCs, and OCIOv2.1, have already implemented the Reference Gamut Compression natively, but for those who need to use an application or version without native support, various implementations are provided as a stop-gap solution in this GitHub repo.</p>"},{"location":"guides/rgc-user/#on-set","title":"On Set","text":""},{"location":"guides/rgc-user/#live-grading","title":"Live Grading","text":"<p>If your production is utilizing an on set grading software, such as Pomfort Livegrade, use it to apply the Reference Gamut Compression. This will embed the RGC in the 3D LUT which is passed to the LUT box for viewing on a monitor.</p>"},{"location":"guides/rgc-user/#in-camera","title":"In-Camera","text":"<p>The production can create a 3D LUT of the appropriate size (normally 33x33x33 max) with the Reference Gamut Compression added to the existing viewing pipeline to load into the camera.</p>"},{"location":"guides/rgc-user/#dailies","title":"Dailies","text":"<p>Use a dailies generation software, such as Colorfront or Resolve, to import the original camera footage, and apply the Reference Gamut Compression as a part of your viewing pipeline for export to desired media.</p>"},{"location":"guides/rgc-user/#editorial","title":"Editorial","text":"<p>Use media supplied from dailies, and back from VFX, to verify media as work progresses. As editorial is largely offline and based on proxy media, the RGC, as viewed on set, should be baked into the files sent to editorial.</p>"},{"location":"guides/rgc-user/#vfx","title":"VFX","text":"<ul> <li>Frame pulls for VFX should NOT have the Reference Gamut Compression baked in. The files should be debayered to AP0.</li> <li>VFX will have the flexibility to apply the RGC wherever is best for their compositing chain. This will often be the first node in the tree, but sometimes operations such as a despill on a bluescreen will need to be performed pre-gamut compression. Sending pulls to VFX in AP0 gives compositors the flexibility to fine tune and control their work.</li> <li>Once applied, the Reference Gamut compression should NOT be inverted before delivery.</li> <li>It is important that the RGC get applied to all WIP QT renders for review and editorial, so as to match dailies.</li> </ul>"},{"location":"guides/rgc-user/#finishing","title":"Finishing","text":"<ul> <li>Finishing software should have the ability to apply the RGC at a project, timeline, or clip level. This should give the colorist flexibility to choose what works best for the project.</li> <li>The RGC should be applied directly after the IDT, ideally before any scaling, grading, or other finishing work.</li> <li>In a pre-conformed timeline, apply the RGC as early as possible. </li> <li>If frames are coming back from VFX, it will be important to track those vs. non-VFX shots, so that the gamut compression is not applied twice.</li> </ul>"},{"location":"guides/rgc-user/#production-realities","title":"Production Realities","text":""},{"location":"guides/rgc-user/#order-of-operations","title":"Order of Operations","text":"<p>The order in which the various operations are applied to an image has a significant impact on the end result. In particular, any scaling will produce a different result depending on whether it is done before or after the RGC, since its removal of negative values can reduce some scaling artifacts. Some applications may give the user detailed control over order of operations, but in others the underlying processes are hidden. This is an important consideration when planning workflows.</p> <p>In compositing in particular, there may be operations (edge despill in keying has been noted) where using the unmodified pixel values gives a preferable result. In these cases it may be necessary for the compositor to have access to both the original and gamut compressed image data in their node tree, choosing between them as necessary. For consistency, the RGC should still be applied at some other suitable point in the composite, such that the final renders delivered to DI still have the gamut compression applied as expected.</p> <p>Since normal practice in VFX is to return images with any pixel not touched by the compositing process unmodified from the original pulls, one might think that the RGC should be inverted for deliverables, as is done with CDL corrections, for example. However, it is better to think of the RGC more like a spill suppression, which is part of the composite, and would not be inverted out at the end. Inverting creates the possibility that elements added during compositing (CGI originally created in ACEScg, for example) which have not had the RGC applied may produce extreme values on inversion. An inverse mode is included in the algorithm, but is provided only for edge cases where it proves unavoidable. Some education of the various stakeholders may be required to establish why inverting is not preferable.</p>"},{"location":"guides/rgc-user/#tracking","title":"Tracking","text":"<p>In the long term, the expectation is that application of the RGC will be tracked using AMF (the ACES Metadata File). This will enable selective use of the algorithm, rather than the currently recommended default of \u201calways on\u201d. Since AMF is currently in development by the various software vendors, this will not be practical until AMF is widely implemented. Unless AMF can be relied upon to be correctly read and updated at every stage of the process, it will be of little use \u2013 incorrect metadata is worse that no metadata.</p> <p>The RGC is classed as an LMT (Look Modification Transform). But unlike most LMTs, it is applied first, immediately after the IDT, rather than last, just before the Output Transform. AMF can list multiple LMTs in its specification of the viewing pipeline for a shot, so will include one for the RGC as well as optionally one for a scene/show look. Compositing and grading work will be done between these two LMTs.</p> <p>LMT elements in an AMF include an <code>applied</code> attribute, so a shot which was previously viewed (e.g. on set) with the RGC enabled will include an LMT for the RGC, with the <code>applied</code> attribute set to <code>false</code>. If the shot is then passed through VFX, and the RGC is then baked in, the <code>applied</code> attribute should be set to <code>true</code> in the AMF returned with the shot to finishing. This will enable the finishing system to automatically apply the RGC to original footage, but disable it for shots from VFX.</p> <p>In the short term, manual tracking will be needed. This is the reason for the recommendation to have the RGC always enabled in the viewing pipeline (and therefore baked into any media which includes the Output Transform). A slight complexity is introduced by the requirement to apply the RGC before compositing work, and therefore bake it into any VFX renders. This means it is necessary for anybody working with a shot which has passed through VFX (including both colorists and VFX artists adding a secondary compositing pass) to take account of the fact that the RGC is already baked in, and not apply it a second time. Until AMF automates this process, careful communication, and agreement upon standard practices will be required. Please note that the workflows outlined in this guide are recommendations, but the needs may vary by facility and production.</p>"},{"location":"guides/rgc-user/#3d-lut-implementation","title":"3D LUT Implementation","text":"<p>While it is generally recommended to use full precision CPU/GPU implementations of the ACES RGC transform some use cases may still require a 3D LUT based implementation instead. Examples for this include (but are not limited to):</p> <ul> <li>On-set monitoring of live camera feeds</li> <li>Implementations using legacy versions of OpenColorIO (v2.0 or earlier)</li> <li>Other DCC applications that do not yet support ACES v1.3</li> </ul> <p>The two main considerations for a 3D LUT implementation of the ACES RGC are the LUT input color space and the transform precision.</p>"},{"location":"guides/rgc-user/#lut-input-color-space","title":"LUT Input Color Space:","text":"<p>3D LUT input domains are usually in a 0.0 to 1.0 range (or equivalent integer ranges). Since out-of-gamut color samples have component values below zero an appropriate LUT input color space must be used in which all expected color samples map into the 0.0 to 1.0 range.</p> <p>For on-set monitoring, where the input gamut is known and fixed, such an input color space could be the camera\u2019s specific log-encoding (e.g. LogC3/ARRIWideGamut, Log3G10/REDWideGamutRGB). These encodings are optimized for the particular camera model and are expected to map all color samples into the 0.0\u21921.0 domain. Please note that ACEScct does not fulfill this requirement, even if it is available as a camera RAW development target, and is therefore not recommended to be used as an input encoding for a 3D LUT implementation of the RGC. So for visual effects, review or mastering applications that have to account for multiple input gamuts and are not able to rely on a specific camera vendor encoding an analytic implementation is required.</p>"},{"location":"guides/rgc-user/#lut-precision","title":"LUT Precision:","text":"<p>To achieve a reasonable approximation of the ACES RGC transform a 3D LUT implementation should use the highest practical resolution (e.g. 65x65x65) as well as tetrahedral interpolation. However, even with high resolution LUTs, the residual interpolation errors are significant enough to prevent accurate inversion of the transform, especially at the gamut boundaries. Therefore 3D LUT implementations of the RGC should be considered non-invertible.</p>"},{"location":"guides/rgc-user/#implementation-guide","title":"Implementation Guide","text":"<p>If you are a software developer or engineer looking for technical implementation guidelines for integrating the ACES Reference Gamut Compression in software, please see our Implementation Guide.</p>"},{"location":"guides/rgc-user/#appendix","title":"Appendix","text":"<p>Before and after images, viewed through the Rec. 709 Output Transform</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <ol> <li> <p>Some implementations may also include a parametric version of the ACES gamut compression. If you choose to use this, it falls outside the scope of published ACES workflows, and therefore will need to be tracked manually. At that point it is simply another creative tool in the colorist\u2019s arsenal.\u00a0\u21a9</p> </li> </ol>"},{"location":"specifications/acescc/","title":"ACEScc \u2013 A Quasi-Logarithmic Encoding of ACES Data for use within Color Grading Systems","text":""},{"location":"specifications/acescc/#introduction","title":"Introduction","text":"<p>The Academy Color Encoding Specification (ACES) defines a common color encoding method using half- precision floating point values corresponding to linear exposure values encoded relative to a fixed set of extended-gamut RGB primaries. Many digital-intermediate color grading systems have been engineered assuming image data with primaries similar to the grading display and a logarithmic relationship between relative scene exposures and image code values.</p> <p>This document describes a 32-bit single precision floating-point logarithm encoding of ACES known as ACEScc.</p> <p>The logarithmic encoding of ACES for use in 10-bit and 12-bit integer systems is known as ACESproxy and is specified in a separate document, \u201cAcademy S-2013-001.\u201d ACEScc provides compatibility for color grading systems with on-set look metadata generated using the ACESproxy specification. Both encodings use the same color primaries. ACESproxy has a restricted range of values; the minimum and maximum ACES values that can be represented in ACESproxy correspond to a range between 0.0 and 1.0 of ACEScc encoding. ACEScc, however, uses values above 1.0 and below 0.0 to encode the entire range of ACES values. ACEScc values should not be clamped except as part of color correction needed to produce a desired artistic intent.</p> <p>There is no image file container format specified for use with ACEScc as the encoding is intended to be transient and internal to software or hardware systems, and is specifically not intended for interchange or archiving.</p>"},{"location":"specifications/acescc/#scope","title":"Scope","text":"<p>This document describes a 32-bit floating point encoding of ACES for use within color grading systems. It is intended to be compatible with on-set look metadata generated from systems using the ACESproxy encodings specified in \u201cAcademy S-2013-001.\u201d Equivalent functions may be used for implementation purposes as long as correspondence of grading parameters to this form of log implementation is properly maintained. This document is intended as a guideline to aid developers who are integrating an ACES workflow into a color correction system.</p>"},{"location":"specifications/acescc/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>Academy S-2013-001 - ACESproxy - An Integer Log Encoding of ACES Data</li> <li>ST 2065-1:2021 - SMPTE Standard - Academy Color Encoding Specification (ACES)</li> <li>RP 177:1993 - SMPTE Recommended Practice - Derivation of Basic Television Color Equations</li> <li>754-2019 - IEEE Standard for Floating-Point Arithmetic</li> </ul>"},{"location":"specifications/acescc/#specification","title":"Specification","text":""},{"location":"specifications/acescc/#naming-conventions","title":"Naming conventions","text":"<p>The logarithmic encoding of ACES specified in this document shall be known as ACEScc.</p>"},{"location":"specifications/acescc/#color-component-value-encoding","title":"Color component value encoding","text":"<p>ACEScc values are encoded as 32-bit floating-point numbers. This floating-point encoding uses 32 bits per component as described in IEEE 754.</p>"},{"location":"specifications/acescc/#color-space","title":"Color space chromaticities","text":"<p>ACEScc uses a different set of primaries than the ACES RGB primaries defined in SMPTE ST 2065-1. The CIE 1931 colorimetry of the ACEScc RGB primaries and white are specified below.</p>"},{"location":"specifications/acescc/#color-primaries","title":"Color primaries","text":"<p>The chromaticity values of the RGB primaries (known as AP1) shall be those found below:</p> R G B CIE x CIE y Red 1.0 0.0 0.0 0.713 0.293 Green 0.0 1.0 0.0 0.165 0.830 Blue 0.0 0.0 1.0 0.128 0.044      ACEScc RGB primaries chromaticity values"},{"location":"specifications/acescc/#white-point","title":"White point","text":"<p>The white point shall be:</p> R G B CIE x CIE y White 1.0 1.0 1.0 0.32168 0.33767      ACEScc RGB white point chromaticity values"},{"location":"specifications/acescc/#acescc","title":"ACEScc","text":"<p>The following functions shall be used to convert between ACES values, encoded according to SMPTE ST 2065-1, and ACEScc.</p>"},{"location":"specifications/acescc/#encoding-function","title":"Encoding Function","text":"<p>ACES \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_1\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACEScc values according to Equation 1.</p> <p></p> \\[\\begin{equation}      ACEScc = \\left\\{      \\begin{array}{l l }         \\dfrac{\\log_{2}(2^{-16}) + 9.72}{17.52};    &amp; \\quad lin_{AP1} \\leq 0 \\\\[10pt]         \\dfrac{\\log_{2}(2^{-16} + lin_{AP1} \\times 0.5) + 9.72}{17.52};     &amp; \\quad lin_{AP1} &lt; 2^{-15} \\\\[10pt]         \\dfrac{\\log_{2}(lin_{AP1}) + 9.72}{17.52};  &amp; \\quad lin_{AP1} \\geq 2^{-15} \\\\         \\end{array} \\right. \\end{equation}\\] Equation 1: Linear AP1 to ACEScc  <p>Note</p> <p>Equation 2 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{1}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP1}\\) inverse and \\(NPM_{AP0}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACEScc specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}  \\begin{bmatrix}     R_{lin_{AP1}}\\\\     G_{lin_{AP1}}\\\\     B_{lin_{AP1}} \\end{bmatrix} = TRA_{1} \\cdot \\begin{bmatrix}     R_{ACES}\\\\     G_{ACES}\\\\     B_{ACES} \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = \\begin{bmatrix}     \\phantom{-}1.4514393161 &amp; -0.2365107469 &amp; -0.2149285693 \\\\    -0.0765537734 &amp;  \\phantom{-}1.1762296998 &amp; -0.0996759264 \\\\     \\phantom{-}0.0083161484 &amp; -0.0060324498 &amp;  \\phantom{-}0.9977163014 \\\\ \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = NPM^{-1}_{AP1} \\cdot NPM_{AP0} \\end{equation}\\] <p> Equation 2: ACES to linear AP1 </p> <p>Note</p> <p>Clipping ACES values below 0 in the above function is not required. Implementers are encouraged to encode negative values or take care when clipping color outside the ACEScc gamut. See Appendix A for details.</p>"},{"location":"specifications/acescc/#decoding-function","title":"Decoding Function","text":"<p>ACEScc \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) values using Equation 3.</p> <p></p> \\[\\begin{equation}     lin_{AP1} = \\left\\{      \\begin{aligned}         &amp;\\left( 2^{(ACEScc \\times 17.52-9.72)} - 2.0^{-16}\\right)\\times 2.0 ;   &amp; ACEScc&amp; \\leq \\dfrac{9.72-15}{17.52} \\\\[10pt]         &amp;2^{(ACEScc \\times 17.52-9.72)}; &amp;  \\dfrac{9.72-15}{17.52} \\leq ACEScc &amp; &lt; \\dfrac{\\log_{2}(65504)+9.72}{17.52} \\\\[10pt]         &amp;65504; &amp; ACEScc&amp; \\geq \\dfrac{\\log_{2}(65504)+9.72}{17.52} \\\\         \\end{aligned} \\right. \\end{equation}\\] Equation 3: ACEScc to linear AP1  <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACES \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_{2}\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 4 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and ACEScc \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{2}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP0}\\) inverse and \\(NPM_{AP1}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACEScc specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}      \\begin{bmatrix}         R_{ACES}\\\\         G_{ACES}\\\\         B_{ACES}     \\end{bmatrix}     =     TRA_{2}     \\cdot     \\begin{bmatrix}         R_{lin_{AP1}}\\\\         G_{lin_{AP1}}\\\\         B_{lin_{AP1}}     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} =     \\begin{bmatrix}         \\phantom{-}0.6954522414 &amp; 0.1406786965 &amp; 0.1638690622 \\\\         \\phantom{-}0.0447945634 &amp; 0.8596711185 &amp; 0.0955343182 \\\\         -0.0055258826 &amp; 0.0040252103 &amp; 1.0015006723 \\\\     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} = NPM^{-1}_{AP0} \\cdot NPM_{AP1} \\end{equation}\\] <p> Equation 4: Linear AP1 to ACES </p>"},{"location":"specifications/acescc/#appendices","title":"Appendices","text":""},{"location":"specifications/acescc/#appendix-a-encoding-of-negative-values","title":"Appendix A: Encoding of negative values","text":"<p>Very small ACES scene referred values below 7 1/4 stops below 18% middle gray are encoded as negative ACEScc values. These values should be preserved per the encoding in ACEScc so that all positive ACES values are maintained.</p> <p>When ACES values are matrixed into the smaller ACEScc color space, colors outside the ACEScc gamut can generate negative values even before the log encoding. If these values are clipped, a conversion back to ACES will not restore the original colors. A specific method of preserving negative values produced by the transformation matrix has not been defined in part to help ease adoption across various color grading systems that have different capabilities and methods for handling negative values. Clipping these values has been found to have minimal visual impact when viewed through the Reference Rendering Transform (RRT) and an appropriate Output Device Transform (ODT) on currently available display technology. However to preserve creative choice in downstream processing and to provide the highest quality archival master, developers implementing ACEScc encoding are encouraged to adopt a method of preserving negative values so that a conversion from ACES to ACEScc and back can be made lossless. Alternatively, a gamut mapping algorithm may be applied to minimize hue shifts resulting from clipping negative ACEScc values. Specific methods for handling negative values may be added to the ACEScc specification in the future.</p>"},{"location":"specifications/acescc/#appendix-b-application-of-asc-cdl-parameters-to-acescc-image-data","title":"Appendix B: Application of ASC CDL parameters to ACEScc image data","text":"<p>American Society of Cinematographers Color Decision List (ASC CDL) slope, offset, power, and saturation modifiers can be applied directly to ACEScc image data. ASC CDL color grades created on-set with ACESproxy images per the ACESproxy specification will reproduce the same look when applied to ACEScc images. ACEScc images however arent limited to the ACESproxy range. To preserve the extended range of ACEScc values, no limiting function should be applied with ASC CDL parameters. The power function, however, should not be applied to any negative ACEScc values after slope and offset are applied. Slope, offset, and power are applied with the following function.</p> \\[\\begin{equation}     ACEScc_{out} = \\left\\{      \\begin{array}{l r }         ACEScc_{in} \\times slope + offset; &amp; \\quad ACEScc_{slopeoffset} \\leq 0 \\\\         (ACEScc_{in} \\times slope + offset)^{power}; &amp; \\quad ACEScc_{slopeoffset} &gt; 0 \\\\     \\end{array} \\right. \\\\  \\end{equation}\\] \\[\\begin{equation}     \\begin{array}{l}     \\text{Where:}\\\\     ACEScc_{slopeoffset} = ACEScc_{in} \\times slope + offset     \\end{array} \\end{equation}\\] <p>ASC CDL Saturation is also applied with no limiting function:</p> \\[\\begin{gather*}     luma = 0.2126 \\times ACEScc_{red} + 0.7152 \\times ACEScc_{green} + 0.0722 \\times ACEScc_{blue} \\\\     \\begin{aligned}         ACEScc_{red} &amp;= luma + saturation \\times (ACEScc_{red} - luma) \\\\         ACEScc_{green} &amp;= luma + saturation \\times (ACEScc_{green} - luma) \\\\                 ACEScc_{blue} &amp;= luma + saturation \\times (ACEScc_{blue} - luma) \\\\      \\end{aligned} \\end{gather*}\\]"},{"location":"specifications/acescct/","title":"ACEScct \u2013 A Quasi-Logarithmic Encoding of ACES Data for use within Color Grading Systems","text":""},{"location":"specifications/acescct/#introduction","title":"Introduction","text":"<p>The Academy Color Encoding Specification (ACES) defines a common color encoding method using half- precision floating point values corresponding to linear exposure values encoded relative to a fixed set of extended-gamut RGB primaries. Many digital-intermediate color grading systems have been engineered assuming image data with primaries similar to the grading display and a logarithmic relationship between relative scene exposures and image code values.</p> <p>This document describes a 32-bit single precision floating-point logarithm encoding of ACES known as ACEScct.</p> <p>ACEScct uses values above 1.0 and below 0.0 to encode the entire range of ACES values. ACEScct values should not be clamped except as part of color correction needed to produce a desired artistic intent.</p> <p>There is no image file container format specified for use with ACEScct as the encoding is intended to be transient and internal to software or hardware systems, and is specifically not intended for interchange or archiving.</p> <p>For ACES values greater than 0.0078125, the ACEScct encoding function is identical to the pure-log encoding function of ACEScc. Below this point, the addition of a \u201dtoe\u201d results in a more distinct \u201dmilking\u201d or \u201dfogging\u201d of shadows when a lift operation is applied when compared to the same operation applied in ACEScc. This difference in grading behavior is provided in response to colorist requests for behavior more similar to that of traditional legacy log film scan encodings.</p>"},{"location":"specifications/acescct/#scope","title":"Scope","text":"<p>This document describes a 32-bit floating point encoding of ACES for use within color grading systems.</p> <p>Equivalent functions may be used for implementation purposes as long as correspondence of grading param- eters to this form of log implementation is properly maintained. This document is intended as a guideline to aid developers who are integrating an ACES workflow into a color correction system.</p>"},{"location":"specifications/acescct/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>ST 2065-1:2021 - SMPTE Standard - Academy Color Encoding Specification (ACES)</li> <li>RP 177:1993 - SMPTE Recommended Practice - Derivation of Basic Television Color Equations</li> <li>754-2019 - IEEE Standard for Floating-Point Arithmetic</li> </ul>"},{"location":"specifications/acescct/#specification","title":"Specification","text":""},{"location":"specifications/acescct/#naming-conventions","title":"Naming conventions","text":"<p>The quasi-logarithmic encoding of ACES specified in this document shall be known as ACEScct.</p>"},{"location":"specifications/acescct/#color-component-value-encoding","title":"Color component value encoding","text":"<p>ACEScct values are encoded as 32-bit floating-point numbers. This floating-point encoding uses 32 bits per component as described in IEEE 754.</p>"},{"location":"specifications/acescct/#color-space","title":"Color space chromaticities","text":"<p>ACEScct uses a different set of primaries than ACES RGB primaries defined in SMPTE ST 2065-1. The CIE 1931 colorimetry of the ACEScct RGB primaries and white are specified below.</p>"},{"location":"specifications/acescct/#color-primaries","title":"Color primaries","text":"<p>The chromaticity values of the RGB primaries (known as AP1) shall be those found below:</p> R G B CIE x CIE y Red 1.0 0.0 0.0 0.713 0.293 Green 0.0 1.0 0.0 0.165 0.830 Blue 0.0 0.0 1.0 0.128 0.044      ACEScct RGB primaries chromaticity values"},{"location":"specifications/acescct/#white-point","title":"White point","text":"<p>The white point shall be:</p> R G B CIE x CIE y White 1.0 1.0 1.0 0.32168 0.33767      ACEScct RGB white point chromaticity values"},{"location":"specifications/acescct/#acescct","title":"ACEScct","text":"<p>The following functions shall be used to convert between ACES values, encoded according to SMPTE ST 2065-1, and ACEScct.</p>"},{"location":"specifications/acescct/#encoding-function","title":"Encoding Function","text":"<p>ACES \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_1\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACEScct values according to Equation 1.</p> <p></p> \\[\\begin{equation}      ACEScct = \\left\\{      \\begin{array}{l l }         10.5402377416545 \\times lin_{AP1} + 0.0729055341958355;    &amp; \\quad lin_{AP1} \\leq 0.0078125 \\\\[10pt]         \\dfrac{\\log_{2}(lin_{AP1}) + 9.72}{17.52}; &amp; \\quad lin_{AP1} &gt; 0.0078125 \\\\         \\end{array} \\right. \\end{equation}\\] Equation 1: Linear AP1 to ACEScct  <p>Note</p> <p>Equation 2 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{1}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP1}\\) inverse and \\(NPM_{AP0}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACEScct specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}  \\begin{bmatrix}     R_{lin_{AP1}}\\\\     G_{lin_{AP1}}\\\\     B_{lin_{AP1}} \\end{bmatrix} = TRA_{1} \\cdot \\begin{bmatrix}     R_{ACES}\\\\     G_{ACES}\\\\     B_{ACES} \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = \\begin{bmatrix}     \\phantom{-}1.4514393161 &amp; -0.2365107469 &amp; -0.2149285693 \\\\    -0.0765537734 &amp;  \\phantom{-}1.1762296998 &amp; -0.0996759264 \\\\     \\phantom{-}0.0083161484 &amp; -0.0060324498 &amp;  \\phantom{-}0.9977163014 \\\\ \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = NPM^{-1}_{AP1} \\cdot NPM_{AP0} \\end{equation}\\] <p> Equation 2: ACES to linear AP1 </p>"},{"location":"specifications/acescct/#decoding-function","title":"Decoding Function","text":"<p>ACEScct \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) values using Equation 3.</p> <p></p> \\[\\begin{equation}     lin_{AP1} = \\left\\{      \\begin{aligned}         &amp;\\dfrac{\\left(ACEScct-0.0729055341958355\\right)}{10.5402377416545};&amp; ACEScct&amp; \\leq 0.155251141552511 \\\\[10pt]         &amp;2^{(ACEScct \\times 17.52-9.72)}; &amp;0.155251141552511 \\leq ACEScct&amp; &lt; \\dfrac{\\log_{2}(65504)+9.72}{17.52} \\\\[10pt]         &amp;65504; &amp; ACEScct&amp; \\geq \\dfrac{\\log_{2}(65504)+9.72}{17.52} \\\\         \\end{aligned} \\right. \\end{equation}\\] Equation 3: ACEScct to linear AP1  <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACES \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_{2}\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 4 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and ACEScct \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{2}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP0}\\) inverse and \\(NPM_{AP1}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACEScct specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}      \\begin{bmatrix}         R_{ACES}\\\\         G_{ACES}\\\\         B_{ACES}     \\end{bmatrix}     =     TRA_{2}     \\cdot     \\begin{bmatrix}         R_{lin_{AP1}}\\\\         G_{lin_{AP1}}\\\\         B_{lin_{AP1}}     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} =     \\begin{bmatrix}         \\phantom{-}0.6954522414 &amp; 0.1406786965 &amp; 0.1638690622 \\\\         \\phantom{-}0.0447945634 &amp; 0.8596711185 &amp; 0.0955343182 \\\\         -0.0055258826 &amp; 0.0040252103 &amp; 1.0015006723 \\\\     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} = NPM^{-1}_{AP0} \\cdot NPM_{AP1} \\end{equation}\\] <p> Equation 4: Linear AP1 to ACES </p>"},{"location":"specifications/acescct/#appendices","title":"Appendices","text":""},{"location":"specifications/acescct/#appendix-a-application-of-asc-cdl-parameters-to-acescct-image-data","title":"Appendix A: Application of ASC CDL parameters to ACEScct image data","text":"<p>American Society of Cinematographers Color Decision List (ASC CDL) slope, offset, power, and saturation modifiers can be applied directly to ACEScct image data. To preserve the extended range of ACEScct values, no limiting function should be applied with ASC CDL parameters. The power function, however, should not be applied to any negative ACEScct values after slope and offset are applied. Slope, offset, and power are applied with the following function.</p> <p>Note</p> <p>ACEScct is not compatible with ASC CDL values generated on-set using the ACESproxy encoding. If there is a need to reproduce a look generated on-set where ACESproxy was used, ACEScc must be used in the dailies and/or DI environment to achieve a match.</p> \\[\\begin{equation}     ACEScct_{out} = \\left\\{      \\begin{array}{l r }         ACEScct_{in} \\times slope + offset; &amp; \\quad ACEScct_{slopeoffset} \\leq 0 \\\\         (ACEScct_{in} \\times slope + offset)^{power}; &amp; \\quad ACEScct_{slopeoffset} &gt; 0 \\\\     \\end{array} \\right. \\\\  \\end{equation}\\] \\[\\begin{equation}     \\begin{array}{l}     \\text{Where:}\\\\     ACEScct_{slopeoffset} = ACEScct_{in} \\times slope + offset     \\end{array} \\end{equation}\\] <p>ASC CDL Saturation is also applied with no limiting function:</p> \\[\\begin{gather*}     luma = 0.2126 \\times ACEScct_{red} + 0.7152 \\times ACEScct_{green} + 0.0722 \\times ACEScct_{blue} \\\\     \\begin{aligned}         ACEScct_{red} &amp;= luma + saturation \\times (ACEScct_{red} - luma) \\\\         ACEScct_{green} &amp;= luma + saturation \\times (ACEScct_{green} - luma) \\\\                 ACEScct_{blue} &amp;= luma + saturation \\times (ACEScct_{blue} - luma) \\\\      \\end{aligned} \\end{gather*}\\]"},{"location":"specifications/acescg/","title":"ACEScg \u2013 A Working Space for CGI Render and Compositing","text":""},{"location":"specifications/acescg/#introduction","title":"Introduction","text":"<p>SMPTE ST 2065-1:2012 defines a common color encoding method using half-precision floating point corresponding to linear exposure values encoded relative to a fixed set of RGB primaries. These primaries were designed to encompass the entire visual gamut. However, the algorithms commonly used to synthetically render imagery (CGI) have long used certain optimizations that are different than in typical color management scenarios and sometimes do not work well with very wide-gamut primaries. For example, in CGI rendering calculations, the RGB values of materials typically represent the percentage of light reflected. This is a different interpretation than in color science based on CIE colorimetry, where the RGBs are interpreted as tristimulus values with respect to a certain primary set. In other words, in CGI the RGB values often are used to describe a property of a material rather than the resulting color stimulus received by an observer.</p> <p>This document describes the AP1 color primaries and their use as the basis of a 16-bit or 32-bit floating point working space for CGI rendering and compositing. These primaries are also the basis of the ACEScc working space, which was developed to better support color grading in the ACES workflow. They do lie slightly outside of the visual gamut - in order to be able to encompass both the Rec-2020 gamut and the DCI-P3 gamut for a range of white points.</p>"},{"location":"specifications/acescg/#scope","title":"Scope","text":"<p>This document describes the translation of ACES2065-1 to/from ACEScg. ACEScg is intended as an appropriate working space for CGI rendering and compositing. ACEScg utilizes the AP1 color primaries for the purpose of rendering and compositing computer generated imagery (CGI).</p>"},{"location":"specifications/acescg/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>ST 2065-1:2021 - SMPTE Standard - Academy Color Encoding Specification (ACES)</li> <li>RP 177:1993 - SMPTE Recommended Practice - Derivation of Basic Television Color Equations</li> <li>754-2019 - IEEE Standard for Floating-Point Arithmetic</li> </ul>"},{"location":"specifications/acescg/#specification","title":"Specification","text":""},{"location":"specifications/acescg/#naming-convention","title":"Naming convention","text":"<p>The encoding of ACES specified in this document shall be known as ACEScg.</p>"},{"location":"specifications/acescg/#color-component-value-encoding","title":"Color component value encoding","text":"<p>ACEScg shall be stored as either 16-bit (IEEE binary16) or 32-bit (IEEE binary32) floating point values.</p>"},{"location":"specifications/acescg/#color-component-value-range","title":"Color component value range","text":"<p>The value range for ACEScg color component values is [-65504.0, +65504.0].</p> <p>The chromaticity coordinates of the defined ACEScg RGB primaries (AP1) form a triangle on the CIE chromaticity diagram. ACEScg RGB values which express visible colors are represented by points within this triangle that also lie within the visual gamut.</p> <p>The set of valid ACEScg RGB values also includes members whose projection onto the CIE chromaticity diagram falls outside the region of the AP1 primaries. These ACEScg RGB values include those with one or more negative ACEScg color component values; Ideally these values would be preserved through any compositing operations done in ACEScg space but it is recognized that keeping negative values is not always practical, in which case it will be acceptable to replace negative values with zero.</p> <p>Values well above 1.0 are expected and should not be clamped except as part of the color correction needed to produce a desired artistic intent.</p>"},{"location":"specifications/acescg/#color-component-transfer-function","title":"Color component transfer function","text":"<p>The color component transfer function directly encodes relative exposure values and is defined as</p> \\[\\begin{equation} R=E_r, G=E_g, B=E_b \\end{equation}\\] <p>where \\(E_r\\), \\(E_g\\) and \\(E_b\\) represent relative exposure values that would be captured from the scene by the ACES Reference Image Capture Device (RICD) and \\(R\\), \\(G\\) and \\(B\\) are the resulting ACES color component values transformed to ACEScg using the methods specified in Converting ACES2065-1 RGB values to ACEScg RGB values.</p>"},{"location":"specifications/acescg/#color-space","title":"Color space chromaticities","text":"<p>ACEScg uses a different set of primaries than ACES RGB primaries defined in SMPTE ST 2065-1. The CIE 1931 colorimetry of the ACEScg RGB primaries and white are specified below.</p>"},{"location":"specifications/acescg/#color-space-primaries","title":"Color space primaries","text":"<p>The chromaticity values of the RGB primaries (known as AP1) shall be those found below:</p> R G B CIE x CIE y Red 1.0 0.0 0.0 0.713 0.293 Green 0.0 1.0 0.0 0.165 0.830 Blue 0.0 0.0 1.0 0.128 0.044      AP1 primaries chromaticity values"},{"location":"specifications/acescg/#white-point","title":"White point","text":"<p>The white point shall be:</p> R G B CIE x CIE y White 1.0 1.0 1.0 0.32168 0.33767      ACES RGB white point chromaticity values  <p>Note</p> <p>The ACEScg white point is the same as the white point of ACES2065-1.</p>"},{"location":"specifications/acescg/#acescg","title":"ACEScg","text":"<p>The following functions shall be used to convert between ACES values, encoded according to SMPTE ST 2065-1, and ACEScg.</p>"},{"location":"specifications/acescg/#converting-aces2065-1-rgb-values-to-acescg-rgb-values","title":"Converting ACES2065-1 RGB values to ACEScg RGB values","text":"<p>ACES2065-1 \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACEScg \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_1\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 1 shows the relationship between ACES2065-1 \\(R\\), \\(G\\), and \\(B\\) values and ACEScg \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{1}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP1}\\) inverse and \\(NPM_{AP0}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACEScg specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}  \\begin{bmatrix}     R_{lin_{AP1}}\\\\     G_{lin_{AP1}}\\\\     B_{lin_{AP1}} \\end{bmatrix} = TRA_{1} \\cdot \\begin{bmatrix}     R_{AP0}\\\\     G_{AP0}\\\\     B_{AP0} \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = \\begin{bmatrix}     \\phantom{-}1.4514393161 &amp; -0.2365107469 &amp; -0.2149285693 \\\\    -0.0765537734 &amp;  \\phantom{-}1.1762296998 &amp; -0.0996759264 \\\\     \\phantom{-}0.0083161484 &amp; -0.0060324498 &amp;  \\phantom{-}0.9977163014 \\\\ \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = NPM^{-1}_{AP1} \\cdot NPM_{AP0} \\end{equation}\\] <p> Equation 1: AP0 to AP1 </p>"},{"location":"specifications/acescg/#converting-acescg-rgb-values-to-aces2065-1-rgb-values","title":"Converting ACEScg RGB values to ACES2065-1 RGB values","text":"<p>ACEScg \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACES2065-1 \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_2\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 2 shows the relationship between ACES2065-1 \\(R\\), \\(G\\), and \\(B\\) values and ACEScg \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{2}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP0}\\) inverse and \\(NPM_{AP1}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACEScg specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}      \\begin{bmatrix}         R_{AP0}\\\\         G_{AP0}\\\\         B_{AP0}     \\end{bmatrix}     =     TRA_{2}     \\cdot     \\begin{bmatrix}         R_{AP1}\\\\         G_{AP1}\\\\         B_{AP1}     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} =     \\begin{bmatrix}         \\phantom{-}0.6954522414 &amp; 0.1406786965 &amp; 0.1638690622 \\\\         \\phantom{-}0.0447945634 &amp; 0.8596711185 &amp; 0.0955343182 \\\\         -0.0055258826 &amp; 0.0040252103 &amp; 1.0015006723 \\\\     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} = NPM^{-1}_{AP0} \\cdot NPM_{AP1} \\end{equation}\\] <p> Equation 2: AP1 to AP0 </p>"},{"location":"specifications/acesproxy/","title":"ACESproxy \u2013 An Integer Log Encoding of ACES Image Data","text":""},{"location":"specifications/acesproxy/#introduction","title":"Introduction","text":"<p>This document specifies a logarithmic-type integer encoding for the Academy Color Encoding System (ACES) for use in 10-bit and 12-bit hardware systems that need to transmit a representation of an ACES floating-point image.</p> <p>The Academy Color Encoding Specification prescribes a digital encoding method using the IEEE half-precision floating-point encoding defined in IEEE 754-2008. Many production systems do not support the use of 16-bit-per-color component transmission especially where hardware video systems are utilized. Some systems used for preview, look creation, and color grading are limited to common 10-bit and 12-bit video signals. In some cases, it is still necessary for a user to see a representation of the ACES image without it having been rendered for the output device using an Output Transform, and where no 16-bit floating-point capability exists in the hardware and software.</p> <p>This document specifies an encoding of ACES using 10-bit or 12-bit integer data types for compatibility with those systems. This encoding is defined and named herein as ACESproxy.</p> <p>10-bit and 12-bit integer data types cannot store the full range of ACES data with the same level of precision provided by the ACES half-precision floating-point format. To make appropriate use of the limited range of the 10-bit and 12-bit integer data types, the ACESproxy encoding uses a middle portion of the possible range of ACES values and is encoded using a logarithmic transfer function. To better facilitate on-set look creation, ACESproxy also uses a smaller color gamut. ACES values outside of this encoded range cannot be transmitted using ACESproxy and are assumed to be clipped to the maximum and minimum ACESproxy values.</p> <p>ACESproxy is appropriate as a working-space encoding for on-set preview and look creation systems since those systems are typically designed to work with other image data encoded in a similar fashion. The ACESproxy encoding is specifically designed to work well when graded using the American Society of Cinematographers Color Decision List (ASC CDL).</p> <p>The ACESproxy encoding was designed for the transmission of images across transports such as High Definition Serial Digital Interface (HD-SDI), for use within hardware systems limited to 10- or 12-bit operation, and for the creation of look metadata such as ASC CDL values.</p> <p>ACESproxy images are designed to be viewed through an ACES viewing pipeline as detailed in Appendix A. When viewed without an ACES output transform, ACESproxy images are dim and low in contrast and saturation, but allow all of the sensors image data to be viewed.</p> <p>ACESproxy-encoded images are intermediate encodings and are not replacements for ACES image data in postproduction color grading or finishing environments. There is no image file container format specified for use with ACESproxy. ACESproxy encoding is specifically not intended for interchange, mastering finals, or archiving, all of which are better completed using the original ACES files.</p>"},{"location":"specifications/acesproxy/#scope","title":"Scope","text":"<p>This document specifies 10-bit and 12-bit integer encodings of ACES for use with imaging systems that produce look metadata such as ASC CDL, and with transport systems such as HD-SDI. The color encoding provided in this format represents ACES relative exposure values as RGB triplets in a logarithmic encoding, and does not define the interfaces or signals that may carry this encoding.</p>"},{"location":"specifications/acesproxy/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>ST 2065-1:2021 - SMPTE Standard - Academy Color Encoding Specification (ACES)</li> <li>RP 177:1993 - SMPTE Recommended Practice - Derivation of Basic Television Color Equations</li> </ul>"},{"location":"specifications/acesproxy/#specification","title":"Specification","text":""},{"location":"specifications/acesproxy/#naming-conventions","title":"Naming conventions","text":"<p>Both the 10-bit and 12-bit logarithmic integer encoding of ACES specified in this document shall be known as ACESproxy.</p> <p>Systems that are limited to the display of 8 characters for control labels shall use the abbreviation ACESPRXY.</p> <p>Systems that are limited to the display of 5 to 7 characters for control labels shall use the abbreviation ACSPX.</p>"},{"location":"specifications/acesproxy/#color-space","title":"Color space chromaticities","text":"<p>ACESporxy uses a different set of primaries than the ACES RGB primaries defined in SMPTE ST 2065-1. The CIE 1931 colorimetry of the ACESproxy RGB primaries and white are specified below.</p>"},{"location":"specifications/acesproxy/#color-primaries","title":"Color primaries","text":"<p>The chromaticity values of the RGB primaries (known as AP1) shall be those found below:</p> R G B CIE x CIE y Red 1.0 0.0 0.0 0.713 0.293 Green 0.0 1.0 0.0 0.165 0.830 Blue 0.0 0.0 1.0 0.128 0.044      ACESproxy RGB primaries chromaticity values"},{"location":"specifications/acesproxy/#white-point","title":"White point","text":"<p>The white point shall be:</p> R G B CIE x CIE y White 1.0 1.0 1.0 0.32168 0.33767      ACESproxy RGB white point chromaticity values"},{"location":"specifications/acesproxy/#acesproxy10","title":"ACESproxy 10-bit definition","text":"<p>The following functions shall be used to convert between ACES values, encoded according to SMPTE ST 2065-1, and 10-bit integer ACESproxy values.</p>"},{"location":"specifications/acesproxy/#encoding-function","title":"Encoding Function","text":"<p>ACES \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_1\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACESproxy10 values according to Equation 1.</p> <p></p> \\[\\begin{equation}      ACESproxy = \\left\\{      \\begin{array}{l l }         64;     &amp; \\quad lin_{AP1} \\leq 2^{-9.72} \\\\[10pt]         \\mathrm{FLOAT2CV10}[(log_{2}(lin_{AP1}) + 2.5) \\times 50 + 425];    &amp; \\quad lin_{AP1} &gt; 2^{-9.72} \\\\         \\end{array} \\right. \\end{equation}\\] <p>Where:</p> <p>\\(\\mathrm{FLOAT2CV10}(\\mathbf{a})\\) returns MAX\\((64,\\) MIN\\((940,\\) ROUND\\((\\mathbf{a})))\\)</p> <p>ROUND(\\(\\mathbf{a}\\)) returns the integer value closest to the floating point value \\(\\mathbf{a}\\)</p> <p>MAX\\((\\mathbf{a}, \\mathbf{b})\\) returns the greater of \\(\\mathbf{a}\\) or \\(\\mathbf{b}\\)</p> <p>MIN\\((\\mathbf{a}, \\mathbf{b})\\) returns the lesser of \\(\\mathbf{a}\\) or \\(\\mathbf{b}\\)</p> Equation 1: Linear AP1 to ACESproxy10  <p>Note</p> <p>Equation 2 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{1}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP1}\\) inverse and \\(NPM_{AP0}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACESproxy specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}  \\begin{bmatrix}     R_{lin_{AP1}}\\\\     G_{lin_{AP1}}\\\\     B_{lin_{AP1}} \\end{bmatrix} = TRA_{1} \\cdot \\begin{bmatrix}     R_{ACES}\\\\     G_{ACES}\\\\     B_{ACES} \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = \\begin{bmatrix}     \\phantom{-}1.4514393161 &amp; -0.2365107469 &amp; -0.2149285693 \\\\    -0.0765537734 &amp;  \\phantom{-}1.1762296998 &amp; -0.0996759264 \\\\     \\phantom{-}0.0083161484 &amp; -0.0060324498 &amp;  \\phantom{-}0.9977163014 \\\\ \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = NPM^{-1}_{AP1} \\cdot NPM_{AP0} \\end{equation}\\] <p> Equation 2: ACES to linear AP1 </p> <p>Note</p> <p>ACESproxy values encoded using the equation above are not appropriate for storage or for archiving. They are intended for use only with digital transport interfaces unable to carry half-precision floating-point values, and with integer-based grading systems designed to generate look metadata that will guide the color grading applied to ACES image data later in the post-production process.</p> <p>Note</p> <p>ACESproxy encodes values into the SMPTE \u201clegal-range\u201d for video systems; grading systems should use this as their nominal range.</p>"},{"location":"specifications/acesproxy/#decoding-function","title":"Decoding Function","text":"<p>ACESproxy \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) values using Equation 3.</p> <p></p> \\[\\begin{equation}     lin_{AP1} = 2^{\\left(\\dfrac{ACESproxy10-425}{50}-2.5\\right)} \\end{equation}\\] Equation 3: ACESproxy10 to linear AP1  <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACES \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_{2}\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 4 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and ACESproxy \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{2}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP0}\\) inverse and \\(NPM_{AP1}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACESproxy specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}      \\begin{bmatrix}         R_{ACES}\\\\         G_{ACES}\\\\         B_{ACES}     \\end{bmatrix}     =     TRA_{2}     \\cdot     \\begin{bmatrix}         R_{lin_{AP1}}\\\\         G_{lin_{AP1}}\\\\         B_{lin_{AP1}}     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} =     \\begin{bmatrix}         \\phantom{-}0.6954522414 &amp; 0.1406786965 &amp; 0.1638690622 \\\\         \\phantom{-}0.0447945634 &amp; 0.8596711185 &amp; 0.0955343182 \\\\         -0.0055258826 &amp; 0.0040252103 &amp; 1.0015006723 \\\\     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} = NPM^{-1}_{AP0} \\cdot NPM_{AP1} \\end{equation}\\] <p> Equation 4: Linear AP1 to ACES </p>"},{"location":"specifications/acesproxy/#acesproxy12","title":"ACESproxy 12-bit definition","text":"<p>The following functions shall be used to convert between ACES values, encoded according to SMPTE ST 2065-1, and 10-bit integer ACESproxy values.</p>"},{"location":"specifications/acesproxy/#encoding-function_1","title":"Encoding Function","text":"<p>ACES \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_1\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACESproxy12 values according to Equation 5.</p> <p></p> \\[\\begin{equation}      ACESproxy = \\left\\{      \\begin{array}{l l }         256;    &amp; \\quad lin_{AP1} \\leq 2^{-9.72} \\\\[10pt]         \\mathrm{FLOAT2CV12}[(log_{2}(lin_{AP1}) + 2.5) \\times 200 + 1700];  &amp; \\quad lin_{AP1} &gt; 2^{-9.72} \\\\         \\end{array} \\right. \\end{equation}\\] <p>Where:</p> <p>\\(\\mathrm{FLOAT2CV12}(\\mathbf{a})\\) returns MAX\\((256,\\) MIN\\((3760,\\) ROUND\\((\\mathbf{a})))\\)</p> <p>ROUND(\\(\\mathbf{a}\\)) returns the integer value closest to the floating point value \\(\\mathbf{a}\\)</p> <p>MAX\\((\\mathbf{a}, \\mathbf{b})\\) returns the greater of \\(\\mathbf{a}\\) or \\(\\mathbf{b}\\)</p> <p>MIN\\((\\mathbf{a}, \\mathbf{b})\\) returns the lesser of \\(\\mathbf{a}\\) or \\(\\mathbf{b}\\)</p> Equation 5: Linear AP1 to ACESproxy12  <p>Note</p> <p>Equation 6 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and \\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{1}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP1}\\) inverse and \\(NPM_{AP0}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACESproxy specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}  \\begin{bmatrix}     R_{lin_{AP1}}\\\\     G_{lin_{AP1}}\\\\     B_{lin_{AP1}} \\end{bmatrix} = TRA_{1} \\cdot \\begin{bmatrix}     R_{ACES}\\\\     G_{ACES}\\\\     B_{ACES} \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = \\begin{bmatrix}     \\phantom{-}1.4514393161 &amp; -0.2365107469 &amp; -0.2149285693 \\\\    -0.0765537734 &amp;  \\phantom{-}1.1762296998 &amp; -0.0996759264 \\\\     \\phantom{-}0.0083161484 &amp; -0.0060324498 &amp;  \\phantom{-}0.9977163014 \\\\ \\end{bmatrix} \\\\ \\end{equation}\\] \\[\\begin{equation} TRA_{1} = NPM^{-1}_{AP1} \\cdot NPM_{AP0} \\end{equation}\\] <p> Equation 6: ACES to linear AP1 </p> <p>Note</p> <p>ACESproxy values encoded using the equation above are not appropriate for storage or for archiving. They are intended for use only with digital transport interfaces unable to carry half-precision floating-point values, and with integer-based grading systems designed to generate look metadata that will guide the color grading applied to ACES image data later in the post-production process.</p> <p>Note</p> <p>ACESproxy encodes values into the SMPTE \u201clegal-range\u201d for video systems; grading systems should use this as their nominal range.</p>"},{"location":"specifications/acesproxy/#decoding-function_1","title":"Decoding Function","text":"<p>ACESproxy \\(R\\), \\(G\\), and \\(B\\) values shall be converted to \\(lin_{AP1}\\) values using Equation 7.</p> <p></p> \\[\\begin{equation}     lin_{AP1} = 2^{\\left(\\dfrac{ACESproxy12-1700}{200}-2.5\\right)} \\end{equation}\\] Equation 7: ACESproxy12 to linear AP1  <p>\\(lin_{AP1}\\) \\(R\\), \\(G\\), and \\(B\\) values shall be converted to ACES \\(R\\), \\(G\\), and \\(B\\) values using the transformation matrix (\\(TRA_{2}\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 8 shows the relationship between ACES \\(R\\), \\(G\\), and \\(B\\) values and ACESproxy \\(R\\), \\(G\\), and \\(B\\) values. \\(TRA_{2}\\), rounded to 10 significant digits, is derived from the product of \\(NPM_{AP0}\\) inverse and \\(NPM_{AP1}\\) calculated using methods provided in Section 3.3 of SMPTE RP 177:1993. AP0 are the primaries of ACES specified in SMPTE ST 2065-1. AP1 are the primaries of ACESproxy specified in Color space chromaticities.</p> <p></p> \\[\\begin{equation}      \\begin{bmatrix}         R_{ACES}\\\\         G_{ACES}\\\\         B_{ACES}     \\end{bmatrix}     =     TRA_{2}     \\cdot     \\begin{bmatrix}         R_{lin_{AP1}}\\\\         G_{lin_{AP1}}\\\\         B_{lin_{AP1}}     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} =     \\begin{bmatrix}         \\phantom{-}0.6954522414 &amp; 0.1406786965 &amp; 0.1638690622 \\\\         \\phantom{-}0.0447945634 &amp; 0.8596711185 &amp; 0.0955343182 \\\\         -0.0055258826 &amp; 0.0040252103 &amp; 1.0015006723 \\\\     \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation}     TRA_{2} = NPM^{-1}_{AP0} \\cdot NPM_{AP1} \\end{equation}\\] <p> Equation 8: Linear AP1 to ACES </p>"},{"location":"specifications/acesproxy/#appendices","title":"Appendices","text":""},{"location":"specifications/acesproxy/#appendix-a-encoding-of-negative-values","title":"Appendix A: Encoding of negative values","text":"<p>As a part of the ACES system, images encoded in ACESproxy form are intended to be decoded into ACES values and viewed using an ACES Output Transform appropriate for an intended viewing device.</p> <p>Without such a transform in place, viewed ACESproxy images will appear dim, severely low in contrast and desaturated. However, directly viewing the unrendered log-encoded images is sometimes useful, for example while looking at the wide range of captured image data in the highlights and shadows that are preserved in the ACES system.</p> <p>ACESproxy has been designed to place scene details into the SMPTE \u201clegal-range\u201d of video systems. Scene detail from about 7 stops under mid-gray to 10 stops over mid-gray should be visible within normal legal- range monitor setups. No rescaling of the device output signal should be needed for direct viewing, but is required before applying color grading transforms as described in Appendix C.</p> <p>The ACESproxy encoding allows an amount of \u2018headroom\u2019 beyond the current dynamic range capabilities of digital motion picture cameras, and it is expected that the range of exposed highlight values seen on a waveform monitor will be lower on a monitors scale than the corresponding range that would be shown if other forms of log encoding were used.</p> <p>Specific knowledge of the dynamic range of a camera system and its output encoding can be used to determine the maximum value that will appear on a waveform monitor indicating an exposure has reached full saturation of the sensor.</p> <p>On a waveform monitor displaying ACESproxy values in IRE units, a gray card representing an 18% reflectance would appear at a level of 41% IRE under a normal exposure assumption. A perfect white reflector under the same conditions would appear at 55% IRE. A camera which reaches sensor saturation at 7 stops above 18% reflectance would not show any values above 81% IRE.</p>"},{"location":"specifications/acesproxy/#appendixB","title":"Appendix B: Range of ACES values","text":"<p>This appendix is intended for developers who wish to validate the accuracy of their implementation.</p> <p>The table below contains the results of conversions using exact 16-bit ACES codes. 16-bit ACES has higher precision than either form of ACESproxy so rounding of ACES values will occur. These numbers are accurate for neutral values where R=G=B.</p> ACES 16-bit half-float Hex Code ACES ACESproxy 10-bit CV ACESproxy 12-bit CV ACES decoded from 10-bit ACESproxy ACES decoded from 12-bit ACESproxy 14DA 0.001184464 64 256 0.001185417 0.001185417 31C3 0.180053711 426 1705 0.179199219 0.179809570 5AF7 222.875 940 3760 222.875 222.875"},{"location":"specifications/acesproxy/#appendix-c-convention-for-use-of-acesproxy-smpte-range","title":"Appendix C: Convention for use of ACESproxy SMPTE range","text":"<p>This appendix defines the equivalent of a [0.0 ... 1.0] range in both 10-bit and 12-bit ACESproxy for use in applications.</p> <p>For consistency in using ACESproxy values in video systems designed around the [0.0 . . . 1.0] video range (encoded in 10-bit video as the range 64-940), the following ACES values are considered the equivalents to 0.0 and 1.0 for both 10-bit and 12-bit video.</p> Video normalized scale IRE 10-bit CV 12-bit CV ACES 0.0 0% 64 256 0.001185417 1.0 100% 940 3760 222.875 <p>ASC CDL values are applied to values in the range [0.0 ... 1.0] which by this convention are set to ACES values 0.001185417 to 222.875. In integer based color grading systems this is typically accomplished by scaling from \u201clegal\u201d to full range before applying ASC CDL transforms. The ASC CDL application is further defined in a separate document.</p>"},{"location":"specifications/acesproxy/#appendix-d-acesproxy-function-derivation","title":"Appendix D: ACESproxy function derivation","text":"<p>The ACESproxy 10-bit and 12-bit logarithmic encoding and decoding functions have been derived from the single mathematical function described below. A series of parameters are defined and the values for the parameters specified based on the bit depth of the encoding.</p>"},{"location":"specifications/acesproxy/#math-functions","title":"Math functions","text":"<p>The following general-use math functions are defined for use within the equations.</p> \\(\\mathrm{ROUND}(a)\\) <p>Math function taking a floating-point value \\(a\\), and returning the integer value closest to \\(a\\) </p> \\(\\mathrm{MAX}(a, b)\\) <p>Math function returning the greater of \\(a\\) or \\(b\\) </p> \\(\\mathrm{MIN}(a, b)\\) Math function returning the lesser of \\(a\\) or \\(b\\) \\(\\mathrm{FLOAT2CV}(a)\\) <p>Math function returning \\(\\mathrm{MAX}(\\mathrm{CV_{min}}, \\mathrm{MIN}[\\mathrm{CV_{max}},\\mathrm{ROUND}(a)])\\)</p>"},{"location":"specifications/acesproxy/#parameters","title":"Parameters","text":"<p>The following parameters are defined for each bit-depth.</p> <ul> <li>\\(\\mathrm{CV_{min}}\\) is the minimum code value available for representation of ACES image data</li> <li>\\(\\mathrm{CV_{max}}\\) is the maximum code value available for representation of ACES image data</li> <li>\\(StepsPerStop\\) is the number of code values representing a change of 1 stop in exposure</li> <li>\\(\\mathrm{MidCVOffset}\\) is the integer code value representing the assigned midpoint of the exposure scale for a particular bit-depth encoding. (e.g. the point to which a mid-grey exposure value would be mapped)</li> <li>\\(\\mathrm{MidLogOffset}\\) is the base 2 logarithmic value representing the assigned midpoint of the exposure scale in log space, [e.g. \\(\\mathrm{MidLogOffset} = log_{2}(2^{\u22122.5}) = \u22122.5\\) ]</li> </ul> ACESproxy 10-bit CV ACESproxy 12-bit CV \\(\\mathrm{CV_{min}}\\) 64 256 \\(\\mathrm{CV_{max}}\\) 940 3760 \\(\\mathrm{StepsPerStop}\\) 50 200 \\(\\mathrm{MidCVOffset}\\) 425 1700 \\(\\mathrm{MidLogOffset}\\) -2.5 -2.5 <p>Note</p> <p>\\(\\mathrm{MidCVOffset}\\) is not equal to the ACESproxy value that most closely represents an ACES mid-gray value of 0.18. ACES 0.18 is most closely represented by a 10-bit ACESproxy code value of 426 or a 12-bit ACESproxy code value of 1705.</p>"},{"location":"specifications/acesproxy/#encoding-function_2","title":"Encoding function","text":"<p>The following floating-point equation is used to convert linear values to integer code values.</p> \\[\\begin{equation}      \\mathrm{ACESproxy} = \\left\\{      \\begin{array}{l l}         \\mathrm{CV_{min}};    &amp; \\mathrm{lin} \\leq 2^{\\left({\\frac{(\\mathrm{CV_{min}}-\\mathrm{MidCVOffset})}{\\mathrm{StepsPerStop}}-\\mathrm{MidLogOffset}}\\right)} \\\\         \\mathrm{FLOAT2CV}\\left[\\left(\\log_2(lin)-\\mathrm{MidLogOffset}\\right)\\times \\mathrm{StepsPerStop}+\\mathrm{MidCVOffset}\\right];        &amp; \\mathrm{lin} &gt; 2^{\\left({\\frac{(\\mathrm{CV_{min}}-\\mathrm{MidCVOffset})}{\\mathrm{StepsPerStop}}-\\mathrm{MidLogOffset}}\\right)} \\\\     \\end{array} \\right. \\end{equation}\\] <p>where \\(\\mathrm{ACESproxy}\\) is the resulting integer code value in the range of code values from \\(\\mathrm{CV_{min}}\\) to \\(\\mathrm{CV_{max}}\\). </p> <p>An implementation may use mathematically equivalent forms of this encoding equation.</p>"},{"location":"specifications/acesproxy/#decoding-function_2","title":"Decoding function","text":"<p>The following floating-point equation is used to convert ACESproxy integer code values to linear values.</p> \\[\\begin{equation} \\mathrm{lin} = 2^{\\left( \\dfrac{(\\mathrm{ACESproxy}-\\mathrm{MidCVOffset})}{\\mathrm{StepsPerStop}} + \\mathrm{MidLogOffset}\\right)} \\end{equation}\\] <p>The conversion to linear creates the closest value in 16-bit half precision floating-point to the floating-point result of the equation. Linear values resulting from this equation are limited to the range of values that can be encoded in ACESproxy as illustrated in Appendix B. This decoding function does not produce negative values.</p> <p>An implementation may use mathematically equivalent forms of this decoding equation.</p>"},{"location":"specifications/amf/","title":"ACES Metadata File (AMF) Specification","text":""},{"location":"specifications/amf/#scope","title":"Scope","text":"<p>This document specifies the ACES Metadata File (\u201cAMF\u201d), a \u2018sidecar\u2019 XML file intended to exchange themetadata required to recreate ACES viewing pipelines.This specification supersedes TB-2014-009 \u2013 Academy Color Encoding System (ACES) Clip-level MetadataFile Format Definition and Usage (\u201cACESclip\u201d). TB-2014-009 is now considered obsolete.</p>"},{"location":"specifications/amf/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>Academy S-2014-002, Academy Color Encoding System - Versioning System</li> <li>SMPTE ST 2065-1:2021, Academy Color Encoding Specification (ACES)</li> <li>SMPTE ST 2065-4:2013, ACES Image Container File Layout</li> <li>Academy TB-2014-010, Design, Integration and Use of ACES Look Modification Transforms (LMTs)</li> <li>ISO 8601:2004, Data elements and interchange formats \u2013 Information interchange \u2013 Representation of datesand times</li> <li>ISO/IEC 11578:1996, Information technology \u2013 Open Systems Interconnection \u2013 Remote Procedure Call(RPC)</li> <li>SMPTE ST 2067-50, Interoperable Master Format \u2014 Application #5 ACES</li> <li>SMPTE RDD 47, Interoperable Master Format \u2014 Isochronous Stream of XML Documents (ISXD) Plugin</li> <li>SMPTE ST.2067-9, Interoperable Master Format \u2014 Sidecar Composition Map</li> </ul>"},{"location":"specifications/amf/#introduction","title":"Introduction","text":""},{"location":"specifications/amf/#why-is-metadata-needed-for-aces","title":"Why is metadata needed for ACES?","text":"<p>ACES defines a standard color encoding (SMPTE ST 2065-1) for exchange of images, along with InputTransforms to convert from different image sources to ACES, and Output Transforms in order to view ACESimages on different types of displays.</p> <p>However, when exchanging ACES images during production, there is often missing information required tofully describe the viewing pipeline or \u201ccreative intent\u201d of that particular image.</p> <p>Examples of such information:</p> <ul> <li>ACES Version \u2013 which version of ACES was used?</li> <li>Look Transform \u2013 is there a creative look?</li> <li>Output Transform \u2013 how was this viewed on a display?</li> </ul> <p>To maintain consistent color appearance, transporting this information is crucial. Additionally, this information serves as an unambiguous archive of the creative intent.</p>"},{"location":"specifications/amf/#what-is-amf","title":"What is AMF","text":"<p>The ACES Metadata File (\u201cAMF\u201d) is a sidecar XML file intended to exchange the metadata required to recreate ACES viewing pipelines. It describes the transforms necessary to configure an ACES viewing pipelinefor a collection of related image files.</p> <p>An AMF may have a specified association with a single frame or clip. Alternatively, it may exist without anyassociation to an image, and one may apply it to an image. An application of an AMF to an image wouldtranslate its viewing pipeline to the target image.</p> <p>Images are formed at several stages of production and post-production, including:</p> <ul> <li>Digital cameras</li> <li>Film scanners</li> <li>Animation and VFX production</li> <li>Virtual production</li> <li>Editorial and color correction systems</li> </ul> <p>AMF can be compatible with any digital image, and is not restricted to those encoded in the ACES (SMPTE ST 2065-1). They may be camera native file formats or other encodings if they have associated Input DeviceTransforms (IDTs) (using the <code>&lt;inputTransform&gt;</code> element) so they may be displayed using an ACESviewing pipeline.</p> <p>AMFs may also embed creative look adjustments as one or more LMTs (using the <code>&lt;lookTransform&gt;</code> elements). These looks may be in the form of ASC CDL values, or a reference to an external look file, suchas a CLF (Common LUT Format). Multiple <code>&lt;lookTransform&gt;</code> elements are allowed, and the order ofoperations in which they are applied shall be the order in which they appear in the AMF.</p> <p>AMFs can also serve as effective archival elements. When paired with finished ACES image files, theyform a complete archival record of how image content is intended to be viewed (for example, using the   <code>&lt;outputTransform&gt;</code> and <code>&lt;systemVersion&gt;</code> elements).</p> <p>AMFs do not contain \u201ctimeline\u201d metadata such as edit points. Timeline management files such as EditDecision Lists (EDLs) or Avid Log Exchange files (ALEs) may reference AMFs, attaching them to editingevents and thus enable standardized color management throughout all stages of production.</p> <p></p> Figure1 1.Overall structure of an AMF in simplified form."},{"location":"specifications/amf/#use-case","title":"Use Case","text":"<p>ACES Metadata Files (AMFs) are intended to contain the minimum required metadata for transferring information about ACES viewing pipelines during production, post-production, and archival.</p> <p>Typical use cases for AMF files are the application of \u201cshow LUT\u201d LMTs in cameras and on-set systems,the capture of shot-to-shot looks generated on-set using ASC-CDL, and communication of both to dailies,editorial, VFX, and post-production mastering facilities.</p> <p>AMF supports the transfer of looks by embedding ASC-CDL values within the AMF file or by referencingsidecar look files containing LMTs, such as CLF (Common LUT Format) files.</p>"},{"location":"specifications/amf/#look-development","title":"Look Development","text":"<p>The development of a creative look before the commencement of production is common. Production uses this look to produce a pre-adjusted reference for on-set monitoring. The creative look may be a package of files containing a viewing transform (also known as a \u201cShow LUT\u201d), CDL grades, or more. There are no consistent standards specifying how to produce them, and exchanging them is complex due to a lack of metadata.</p> <p>AMF contains the ability to completely specify the application of a creative look. This automates the exchange of these files and the recreation of the look when applying the AMF. In an ACES workflow, one specifies the creative look as one or more Look Modification Transforms (LMT). AMF can include references to any number of these transforms, and maintains their order of operations.</p> <p>The input and output of an LMT is always a triplet of ACES RGB relative exposure values, as defined in SMPTE ST 2065-1. This will likely need a robust transform, such as CLF, that can handle linear input data and output data.</p> <p>AMF offers an unambiguous description of the full ACES viewing pipeline for on-set look management software to load and display images as intended.</p>"},{"location":"specifications/amf/#on-set","title":"On Set","text":"<p>Before production begins, an AMF may be created and shared with production as a \"look template\" for use during on-set monitoring or look management.</p> <p>Cameras with AMF support can load or generate AMFs to configure or communicate the viewing pipeline of images viewed out of the camera's live video signal.</p> <p>On-set color grading software can load or generate AMFs, allowing the communication of the color adjustments created on set.</p>"},{"location":"specifications/amf/#dailies","title":"Dailies","text":"<p>Dailies can apply AMFs from production to the camera files to reproduce the same images seen on set. There is no single method of exchange between production and dailies. AMFs should be agnostic to the given exchange method.</p> <p>It is possible, or even likely, that one will update AMFs in the dailies stage. For example, a dailies colorist may choose to balance shots at this stage and update the look. Another example could be that dailies uses a different ODT than the one used in on-set monitoring.</p> <p>This specification does not define how one should transport AMFs between stages. Existing exchange formats may reference them, or image files themselves may embed them. One may also transport AMFs independently of any other files.</p>"},{"location":"specifications/amf/#vfx","title":"VFX","text":"<p>The exchange of shots for VFX work requires perfect translation of each shot\u2019s viewing pipeline, or \u2018color recipe\u2019. If the images cannot be accurately reproduced from VFX plates, effects will be created with an incorrect reference.</p> <p>AMF provides a complete and unambiguous translation of ACES viewing pipelines. If they travel with VFX plates, they can describe how to view each plate along with any associated looks.</p> <p>VFX software should have the ability to read AMF to configure its internal viewing pipeline. Or, AMF will inform the configuration of third party color management software, such as OpenColorIO.</p>"},{"location":"specifications/amf/#finishing","title":"Finishing","text":"<p>In finishing, the on-set or dailies viewing reference can be automatically recreated upon reading an AMF. This stage typically uses a higher quality display, which may warrant the use of a different ODT than one specified in an ingested AMF.</p> <p>AMF can seamlessly provide the colorist a starting point that is consistent with the creative intent of the filmmakers on-set. This removes any necessity to recreate a starting look from scratch.</p>"},{"location":"specifications/amf/#archival","title":"Archival","text":"<p>AMF enables the ability to establish a complete ACES archive, and effectively serves as a snapshot of creative intent for preservation and remastering purposes. All components required to recreate the look of an ACES archive are meaningfully described and preserved within the AMF.</p> <p>One possible method for this could be the utilization of SMPTE standards such as ST.2067-50 (IMF App #5) -- commonly referred to as \"ACES IMF\" -- and SMPTE RDD 47 (ISXD) -- a virtual track file containing XML data -- in order to form a complete and flexible ACES archival package.</p> <p>Another method could be to use SMPTE ST.2067-9 (Sidecar Composition Map) which would allow linking of a single AMF to a CPL (Composition Playlist) in the case where there is a single AMF for an entire playlist.</p>"},{"location":"specifications/amf/#data-model","title":"Data Model","text":"<p>This section describes the data intended for use within the ACES Metadata file.</p> <p>All top level structures shall be tagged as being within the \\texttt{aces} namespace with urn <code>urn:acesMetadata:acesMetadataFile:v1.0</code></p>"},{"location":"specifications/amf/#uml-diagram","title":"UML Diagram","text":"<p>The following UML diagrams are segments of the complete UML diagram which is not included in this document due to space constraints.  To view the entire UML diagram in SVG format visit \\url{https://aces.mp/amf_uml}.</p>"},{"location":"specifications/amf/#acesmetadatafile","title":"acesMetadataFile","text":""},{"location":"specifications/amf/#amfinfo","title":"amfInfo","text":""},{"location":"specifications/amf/#clipid","title":"clipId","text":""},{"location":"specifications/amf/#pipeline","title":"pipeline","text":""},{"location":"specifications/amf/#pipelineinfo","title":"pipelineInfo","text":""},{"location":"specifications/amf/#inputtransform","title":"inputTransform","text":""},{"location":"specifications/amf/#looktransform","title":"lookTransform","text":""},{"location":"specifications/amf/#outputtransform","title":"outputTransform","text":""},{"location":"specifications/amf/#types","title":"Types","text":"<p>The following types are defined for use within the AMF XML file and are validated with the XSD schema included in Appendix A.  The types are used as the basis to form the elements listed in section X in the schema.</p> <p>\\input{sec-types.tex}</p>"},{"location":"specifications/amf/#elements-by-type","title":"Elements (by type)","text":"<p>The following elements are defined for use with the AMF XML file and are validated with the XSD schema included in Appendix A. </p>"},{"location":"specifications/clf/","title":"Common LUT Format (CLF) - A Common File Format for Look-Up Tables","text":""},{"location":"specifications/clf/#introduction","title":"Introduction","text":"<p>Look-Up Tables (LUTs) are a common implementation for transformations from one set of color values to another. With a large number of product developers providing software and hardware solutions for LUTs, there is an explosion of unique vendor-specific LUT file formats, which are often only trivially different from each other. This can create workflow problems when a LUT being used on a production is not supported by one or more of the applications being used. Furthermore, many LUT formats are designed for a particular use case only and lack the quality, flexibility, and metadata needed to meet modern requirements.</p> <p>The Common LUT Format (CLF) can communicate an arbitrary chain of color operators (also called processing nodes) which are sequentially processed to achieve an end result. The set of available operator types includes matrices, 1D LUTs, 3D LUTs, ASC-CDL, log and exponential shaper functions, and more. Even when 1D or 3D LUTs are not present, CLF can be used to encapsulate any supported color transforms as a text file conforming to the XML schema.</p>"},{"location":"specifications/clf/#scope","title":"Scope","text":"<p>This document introduces a human-readable text file format for the interchange of color transformations using an XML schema. The XML format supports Look-Up Tables of several types: 1D LUTs, 3D LUTs, and 3\u00d71D LUTs, as well as additional transformation needs such as matrices, range rescaling, and \u201cshaper LUTs.\u201d The document defines what is a valid CLF file. Though it is not intended as a tutorial for users to create their own files, LUT creators will find it useful to understand the elements and attributes available for use in a CLF file. The document is also not intended to provide guidance to implementors on how to optimize their implementations, but does provide a few notes on the subject. This document assumes the reader has knowledge of basic color transformation operators and XML.</p>"},{"location":"specifications/clf/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>IETF RFC 3066: IETF (Internet Engineering Task Force). RFC 3066: Tags for the Identification of Lan- guages, ed. H. Alvestrand. 2001 IEEE DRAFT Standard P123</li> <li>Academy S-2014-002, Academy Color Encoding System \u2013 Versioning System</li> <li>Academy TB-2014-002, Academy Color Encoding System Version 1.0 User Experience Guidelines</li> <li>ASC Color Decision List (ASC CDL) Transfer Functions and Interchange Syntax. ASC-CDL Release1.2. Joshua Pines and David Reisner. 2009-05-04.</li> </ul>"},{"location":"specifications/clf/#specification","title":"Specification","text":""},{"location":"specifications/clf/#general","title":"General","text":"<p>A Common LUT Format (CLF) file shall be written using Extensible Markup Language (XML) and adhere to a defined XML structure. A CLF file shall have the file extension '<code>.clf</code>'.</p> <p>The top level element in a CLF file defines a <code>ProcessList</code> which represents a sequential set of color transformations. The result of each individual color transformation feeds into the next transform in the list to create a daisy chain of transforms.</p> <p>An application reads a CLF file and initializes a transform engine to perform the operations in the list. The transform engine reads as input a stream of code values of pixels, performs the calculations and/or interpolations, and writes an output stream representing a new set of code values for the pixels.</p> <p>In the sequence of transformations described by a <code>ProcessList</code>, each <code>ProcessNode</code> performs a transform on a stream of pixel data, and only one input line (input pixel values) may enter a node and only one output line (output pixel values) may exit a node. A <code>ProcessList</code> may be defined to work on either 1- component or 3-component pixel data, however all transforms in the list must be appropriate, especially in the 1-component case (black-and-white) where only 1D LUT operations are allowed. Implementation may process 1-component transforms by applying the same processing to R, G, and B.</p> <p> </p> Figure 1. Example of a ProcessList containing a sequence of multiple ProcessNodes <p>The file format does not provide a mechanism to assign color transforms to either image sequences or image regions. However, the XML structure defining the LUT transform, a ProcessList, may be encapsulated in a larger XML structure potentially providing that mechanism. This mechanism is beyond the scope of this document.</p> <p>Each CLF file shall be completely self-contained requiring no external information or metadata. The full content of a color transform must be included in each file and a color transform may not be incorporated by reference to another CLF file. This restriction ensures that each CLF file can be an independent archival element.</p> <p>Each ProcessList shall be given a unique ID for reference.</p> <p>The data for LUTs shall be an ordered array that is either all floats or all integers. When three RGB color components are present, it is assumed that these are red, green, and blue in that order. There is only one order for how the data array elements are specified in a LUT, which is in general from black to white (from the minimum input value position to the maximum input value position). Arbitrary ordering of list elements is not supported in the format (see XML Elements for details).</p> <p>Note</p> <p>For 3D LUTs, the indexes to the cube are assumed to have regular spacing across the range of input values. To accommodate irregular spacing, a \"<code>halfDomain</code>\" 1D LUT or Log node should be used as a shaper function prior to the 3D LUT.</p>"},{"location":"specifications/clf/#xml-structure","title":"XML Structure","text":""},{"location":"specifications/clf/#general_1","title":"General","text":"<p>A CLF file shall contain a single occurrence of the XML root element known as the ProcessList. The ProcessList element shall contain one or more elements known as ProcessNodes. The order and number of process nodes is determined by the designer of the CLF file.</p> <p>An example of the overall structure of a simple CLF file is thus:</p> <pre><code>&lt;ProcessList id=\"123\"&gt;\n    &lt;Matrix id=\"1\"&gt;\n        data &amp; metadata\n    &lt;/Matrix&gt;\n    &lt;LUT1D id=\"2\"&gt;\n        data &amp; metadata\n    &lt;/LUT1D&gt; \n    &lt;Matrix id=\"3\"&gt;\n        data &amp; metadata\n    &lt;/Matrix&gt;\n&lt;/ProcessList&gt;\n</code></pre>"},{"location":"specifications/clf/#xml-version-and-encoding","title":"XML Version and Encoding","text":"<p>A CLF file shall include a starting line that declares XML version number and character encoding. This line is mandatory once in a file and looks like this:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n</code></pre>"},{"location":"specifications/clf/#comments","title":"Comments","text":"<p>The file may also contain XML comments that may be used to describe the structure of the file or save information that would not normally be exposed to a database or to a user. XML comments are enclosed in brackets like so:</p> <pre><code>&lt;!--  This is a comment  --&gt;\n</code></pre>"},{"location":"specifications/clf/#language","title":"Language","text":"<p>It is often useful to identify the natural or formal language in which text strings of XML documents are written. The special attribute named xml:lang may be inserted in XML documents to specify the language used in the contents and attribute values of any element in an XML document. The values of the attribute are language identifiers as defined by IETF RFC 3066. In addition, the empty string may be specified. The language specified by xml:lang applies to the element where it is specified (including the values of its attributes), and to all elements in its content unless overridden with another instance of xml:lang. In particular, the empty value of xml:lang can be used to override a specification of xml:lang on an enclosing element, without specifying another language.</p>"},{"location":"specifications/clf/#white-space","title":"White Space","text":"<p>Particularly when creating CLF files containing certain elements (such as <code>Array</code>, <code>LUT1D</code>, or <code>LUT3D</code>) it is desirable that single lines per entry are maintained so that file contents can be scanned more easily by a human reader. There exist some difficulties with maintaining this behavior as XML has some non-specific methods for handling white-space. Especially if files are re-written from an XML parser, white space will not necessarily be maintained. To maintain line layout, XML style sheets may be used for reviewing and checking the CLF file\u2019s entries.</p>"},{"location":"specifications/clf/#newline-control-characters","title":"Newline Control Characters","text":"<p>Different end of line conventions, including <code>&lt;CR&gt;</code>, <code>&lt;LF&gt;</code>, and <code>&lt;CRLF&gt;</code>, are utilized between Mac, Unix, and Windows systems. Different newline characters may result in the collapse of values into one long line of text. To maintain intended linebreaks, CLF specifies that the \u2018newline\u2019 string (i.e. the byte(s) to be interpreted as ending each line of text) shall be the single code value \\(10_{10} = 0\\textrm{A}_{16}\\) (ASCII \u2018Line Feed\u2019 character), also indicated <code>&lt;LF&gt;</code>.</p> <p>Note</p> <p>Parsers of CLF files may choose to interpret Microsoft\u2019s <code>&lt;CR&gt;&lt;LF&gt;</code> or older-MacOS\u2019 <code>&lt;CR&gt;</code> newline conventions, but CLF files should only be generated with the <code>&lt;LF&gt;</code> encoding.</p> <p>Note</p> <p><code>&lt;LF&gt;</code> is the newline convention native to all *nix operating systems (including Linux and modern macOS).</p>"},{"location":"specifications/clf/#xml-elements","title":"XML Elements","text":"<p> Figure 2. Object Model of XML Elements</p>"},{"location":"specifications/clf/#processlist","title":"ProcessList","text":"<p>Description:  The <code>ProcessList</code> is the root element for any CLF file and is composed of one or more ProcessNodes. A <code>ProcessList</code> is required even if only one <code>ProcessNode</code> will be present.</p> <p>Note</p> <p>The last node of the <code>ProcessList</code> is expected to be the final output of the LUT. A LUT designer can allow floating-point values to be interpreted by applications and thus delay control of the final encoding through user selections.</p> <p>Note</p> <p>If needed, a <code>Range</code> node can be placed at the end of a <code>ProcessList</code> to control minimum and maximum output values and clamping.</p> Attributes: <code>id</code> (required) a string to serve as a unique identifier of the <code>ProcessList</code> <code>compCLFversion</code> (required) a string indicating the minimum compatible CLF specification version required to read this file   The <code>compCLFversion</code> corresponding to this version of the specification is be <code>\"3.0\"</code>. <code>name</code> (optional) a concise string used as a text name of the <code>ProcessList</code> for display or selection from an application\u2019s user interface <code>inverseOf</code> (optional) a string for linking to another ProcessList <code>id</code> (unique) which is the inverse of this one Elements: <code>Description</code> (optional) a string for comments describing the function, usage, or any notes about the <code>ProcessList</code>. A <code>ProcessList</code> can contain zero or more <code>Description</code> elements. <code>InputDescriptor</code> (optional) an arbitrary string used to describe the intended source code values of the <code>ProcessList</code> <code>OutputDescriptor</code> (optional) an arbitrary string used to describe the intended output target of the <code>ProcessList</code> (e.g. target display) <code>ProcessNode</code> (required) a generic XML element that in practice is substituted with a particular color operator. The <code>ProcessList</code> must contain at least one <code>ProcessNode</code>. The <code>ProcessNode</code> is described in ProcessNode. <code>Info</code> (optional) optional element for including additional custom metadata not needed to interpret the transforms. The <code>Info</code> element includes: <code>AppRelease</code> (optional) a string used for indicating application software release level <code>Copyright</code> (optional) a string containing a copyright notice for authorship of the CLF file <code>Revision</code> (optional) a string used to track the version of the LUT itself (e.g. an increased resolution from a previous version of the LUT) <code>ACEStransformID</code> (optional) a string containing an ACES transform identifier as described in Academy S-2014-002. If the transform described by the <code>ProcessList</code> is the concatenation of several ACES transforms, this element may contain several ACES Transform IDs, separated by white space or line separators. This element is mandatory for ACES transforms and may be referenced from ACES Metadata Files. <code>ACESuserName</code> (optional) a string containing the user-friendly name recommended for use in product user interfaces as described in Academy TB-2014-002 <code>CalibrationInfo</code> (optional) container element for calibration metadata used when making a LUT for a specific device.  <code>CalibrationInfo</code> can contain the following child elements: <ul> <code>DisplayDeviceSerialNum</code> <code>DisplayDeviceHostName</code> <code>OperatorName</code> <code>CalibrationDateTime</code> <code>MeasurementProbe</code> <code>CalibrationSoftwareName</code> <code>CalibrationSoftwareVersion</code> </ul>"},{"location":"specifications/clf/#processNode","title":"ProcessNode","text":"<p>Description:  A <code>ProcessNode</code> element represents an operation to be applied to the image data. At least one <code>ProcessNode</code> element must be included in a <code>ProcessList</code>. The generic <code>ProcessNode</code> element contains attributes and elements that are common to and inherited by the specific sub-types of the <code>ProcessNode</code> element that can substitute for <code>ProcessNode</code>. All <code>ProcessNode</code> substitutes shall inherit the following attributes.</p> Attributes: <code>id</code> (optional) a unique identifier for the <code>ProcessNode</code> <code>name</code> (optional) a concise string defining a name for the <code>ProcessNode</code> that can be used by an application for display in a user interface <code>inBitDepth</code> (required) a string that is used by some ProcessNodes to indicate how array or parameter values have been scaled <code>outBitDepth</code> (required) a string that is used by some ProcessNodes to indicate how array or parameter values have been scaled The supported values for both <code>inBitDepth</code> and <code>outBitDepth</code> are the same: <ul> <li><code>\"8i\"</code>: 8-bit unsigned integer</li> <li><code>\"10i\"</code>: 10-bit unsigned integer</li> <li><code>\"12i\"</code>: 12-bit unsigned integer</li> <li><code>\"16i\"</code>: 16-bit unsigned integer</li> <li><code>\"16f\"</code>: 16-bit floating point (half-float)</li> <li><code>\"32f\"</code>: 32-bit floating point (single precision)</li> </ul> Elements: <code>Description</code> (optional) an arbitrary string for describing the function, usage, or notes about the ProcessNode. A ProcessNode can contain one or more Descriptions."},{"location":"specifications/clf/#array","title":"Array","text":"<p>Description:  The <code>Array</code> element contains a table of entries with a single line for each grouping of values. This element is used in the <code>LUT1D</code>, <code>LUT3D</code>, and <code>Matrix</code> ProcessNodes. The <code>dim</code> attribute specifies the dimensions of the array and, depending on context, defines the size of a matrix or the length of a LUT table. The specific formatting of the <code>dim</code> attribute must match with the type of node in which it is being used. The usages are summarized below but specific requirements for each application of <code>Array</code> are described when it appears as a child element for a particular <code>ProcessNode</code>.</p> Attributes: <code>dim</code> (required) <p>Specifies the dimension of the LUT or the matrix and the number of color components. The <code>dim</code> attribute provides the dimensionality of the indexes, where:</p> <ul> <li>4 entries represent the dimensions of a 3D cube and the number of components per entry.    e.g. <code>dim = 17 17 17 3</code> indicates a 17-cubed 3D LUT with 3 color components </li> </ul> <ul> <li>2 entries represent the dimensions of a matrix. e.g. <code>dim = 3 3</code> indicates a 3\u00d73 matrix e.g. <code>dim = 3 4</code> indicates a 3\u00d74 matrix</li> </ul> <ul> <li>2 entries represent the length of the LUT and the component value (1 or 3). e.g. <code>dim = 256 3</code> indicates a 256 element 1D LUT with 3 components (a 3\u00d71D LUT) e.g. <code>dim = 256 1</code> indicates a 256 element 1D LUT with 1 component (1D LUT)</li> </ul>"},{"location":"specifications/clf/#substitutes-for-processnode","title":"Substitutes for <code>ProcessNode</code>","text":""},{"location":"specifications/clf/#general_2","title":"General","text":"<p>The attributes and elements defined for <code>ProcessNode</code> are inherited by the substitutes for <code>ProcessNode</code>. This section defines the available substitutes for the generalized <code>ProcessNode</code> element.</p> <p>The <code>inBitDepth</code> of a <code>ProcessNode</code> must match the <code>outBitDepth</code> of the preceding <code>ProcessNode</code> (if any).</p>"},{"location":"specifications/clf/#lut1d","title":"<code>LUT1D</code>","text":"<p>Description: A 1D LUT transform uses an input pixel value, finds the two nearest index positions in the LUT, and then interpolates the output value using the entries associated with those positions.</p> <p>This node shall contain either a 1D LUT or a 3x1D LUT in the form of an <code>Array</code>. If the input to a <code>LUT1D</code> is an RGB value, the same LUT shall be applied to all three color components.</p> <p>A 3x1D LUT transform looks up each color component in a separate <code>LUT1D</code> of the same length. In a 3x1D LUT, by convention, the <code>LUT1D</code> for the first component goes in the first column of <code>Array</code>.</p> <p>The scaling of the array values is based on the <code>outBitDepth</code> (the <code>inBitDepth</code> is not considered).</p> <p>The length of a 1D LUT should be limited to at most 65536 entries, and implementations are not required to support <code>LUT1D</code>s longer than 65536 entries.</p> <p>Linear interpolation shall be used for <code>LUT1D</code>. More information about linear interpolation can be found in Appendix A.</p> Elements: <code>Array</code> (required) an array of numeric values that are the output values of the 1D LUT. <code>Array</code> shall contain the table entries of a LUT in order from minimum value to maximum value. For a 1D LUT, one value per entry is used for all color channels. For a 3x1D LUT, each line should contain 3 values, creating a table where each column defines a 1D LUT for each color component.  For RGB, the first column shall correspond to R\u2019s 1D LUT, the second column shall correspond to G\u2019s 1D LUT, and the third column shall correspond to B\u2019s 1D LUT. Attributes: <code>dim</code> (required) <p>two integers that represent the dimensions of the array. The first value defines the length of the array and shall equal the number of entries (lines) in the LUT. The second value indicates the number of components per entry and shall equal 1 for a 1D LUT or 3 for a 3x1D LUT.</p> <p>Example</p> <p><code>dim = \"1024 3\"</code> indicates a 1024 element 1D LUT with 3 component color (a 3x1D LUT)</p> <p>Example</p> <p><code>dim = \"256 1\"</code> indicates a 256 element 1D LUT with 1 component color (a 1D LUT)</p> <p>Note</p> <p><code>Array</code> is formatted differently when it is contained in a <code>LUT3D</code> or <code>Matrix</code> element (see Array).</p> Attributes: <code>interpolation</code> (optional) <p>a string indicating the preferred algorithm used to interpolate values in the <code>1DLUT</code>. This attribute is optional but, if present, shall be set to <code>\"linear\"</code>.</p> <p>Note</p> <p>Previous versions of this specification allowed for implementations to utilize different types of interpolation but did not define what those interpolation types were or how they should be labeled. For simplicity and to ensure similarity across implementations, 1D LUT interpolation has been limited to <code>\"linear\"</code> in this version of the specification. Support for additional interpolation types could be added in future version.</p> <code>halfDomain</code> (optional) <p>If this attribute is present, its value must equal <code>\u201ctrue\u201d</code>. When true, the input domain to the node is considered to be all possible 16-bit floating-point values, and there must be exactly 65536 entries in the <code>Array</code> element.</p> <p>Note</p> <p>For example, the unsigned integer 15360 has the same bit-pattern (0011110000000000) as the half-float value 1.0, so the 15360th entry (zero-indexed) in the <code>Array</code> element is the output value corresponding to an input value of 1.0.</p> <code>rawHalfs</code> (optional) <p>If this attribute is present, its value must equal <code>\u201ctrue\u201d</code>. When true, the <code>rawHalfs</code> attribute indicates that the output array values in the form of unsigned 16-bit integers are interpreted as the equivalent bit pattern, half floating-point values.</p> <p>Note</p> <p>For example, to represent the value 1.0, one would use the integer 15360 in the <code>Array</code> element because it has the same bit-pattern. This allows the specification of exact half-float values without relying on conversion from decimal text strings.</p> <p>Examples: <pre><code>&lt;LUT1D id=\"lut-23\" name=\"4 Value Lut\" inBitDepth=\"12i\" outBitDepth=\"12i\"&gt; \n    &lt;Description&gt;1D LUT - Turn 4 grey levels into 4 inverted codes&lt;/Description&gt;\n    &lt;Array dim=\"4 1\"&gt;\n        3\n        2\n        1\n        0\n    &lt;/Array&gt;\n&lt;/LUT1D&gt;\n</code></pre></p> <p>Example 1. Example of a very simple <code>LUT1D</code></p>"},{"location":"specifications/clf/#lut3d","title":"<code>LUT3D</code>","text":"<p>Description: This node shall contain a 3D LUT in the form of an Array. In a LUT3D, the 3 color components of the input value are used to find the nearest indexed values along each axis of the 3D cube. The 3-component output value is calculated by interpolating within the volume defined by the nearest corresponding positions in the LUT. LUT3Ds have the same dimension on all axes (i.e. Array dimensions are of the form \u201cn n n 3\u201d). A LUT3D with axial dimensions greater than 128x128x128 should be avoided. The scaling of the array values is based on the outBitDepth (the inBitDepth is not considered). </p> Attributes: <code>interpolation</code> (optional) <p>a string indicating the preferred algorithm used to interpolate values in the 3DLUT. This attribute is optional with a default of <code>\"trilinear\"</code> if the attribute is not present.  Supported values are:</p> <ul> <li><code>\"trilinear\"</code>: perform trilinear interpolation</li> <li><code>\"tetrahedral\"</code>: perform tetrahedral interpolation </li> </ul> <p>Note</p> <p>Interpolation methods are specified in Appendix A.</p> Elements: <code>Array</code> (required) an array of numeric values that are the output values of the 3D LUT. The <code>Array</code> shall contain the table entries for the <code>LUT3D</code> from the minimum to the maximum input values, with the third component index changing fastest. Attributes: <code>dim</code> (required) <p>four integers that reperesent the dimensions of the 3D LUT and the number of color components. The first three values define the dimensions of the LUT and if multiplied shall equal the number of entries actually present in the array. The fourth value indicates the number of components per entry.  4 entries have the dimensions of a 3D cube plus the number of components per entry.</p> <p>Example</p> <p><code>dim = \"17 17 17 3\"</code> indicates a 17-cubed 3D lookup table with 3 component color</p> <p>Note</p> <p><code>Array</code> is formatted differently when it is contained in a <code>LUT1D</code> or <code>Matrix</code> element (see Array).</p> <p>Examples: <pre><code>&lt;LUT3D id=\"lut-24\" name=\"green look\" interpolation=\"trilinear\" inBitDepth=\"12i\" outBitDepth=\"16f\"&gt;\n    &lt;Description&gt;3D LUT&lt;/Description&gt;\n    &lt;Array dim=\"2 2 2 3\"&gt;\n        0.0 0.0 0.0\n        0.0 0.0 1.0\n        0.0 1.0 0.0\n        0.0 1.0 1.0\n        1.0 0.0 0.0\n        1.0 0.0 1.0\n        1.0 1.0 0.0\n        1.0 1.0 1.0\n    &lt;/Array&gt;\n&lt;/LUT3D&gt;\n</code></pre></p> <p>Example 2. Example of a simple <code>LUT3D</code></p>"},{"location":"specifications/clf/#matrix","title":"<code>Matrix</code>","text":"<p>Description: This node specifies a matrix transformation to be applied to the input values. The input and output of a <code>Matrix</code> are always 3-component values.</p> <p>All matrix calculations should be performed in floating point, and input bit depths of integer type should be treated as scaled floats. If the input bit depth and output bit depth do not match, the coefficients in the matrix must incorporate the results of the \u2018scale\u2019 factor that will convert the input bit depth to the output bit depth (e.g. input of <code>10i</code> with an output of <code>12i</code> requires the matrix coefficients already have a factor of \\(4095/1023\\) applied). Changing the input or output bit depth requires creation of a new set of coefficients for the matrix.</p> <p>The output values are calculated using row-order convention: </p> \\[     \\begin{bmatrix}         a_{11} &amp; a_{12} &amp; a_{13} \\\\         a_{21} &amp; a_{22} &amp; a_{23} \\\\         a_{31} &amp; a_{32} &amp; a_{33} \\\\     \\end{bmatrix}     \\begin{bmatrix}         r_1\\\\         g_1\\\\         b_1     \\end{bmatrix}     =     \\begin{bmatrix}         r_2\\\\         g_2\\\\         b_2     \\end{bmatrix} \\] <p>which is equivalent in functionality to the following:</p> \\[     \\begin{aligned}         r_2 = (r_1 \\cdot a_{11}) + (g_1 \\cdot a_{12}) + (b_1 \\cdot a_{13}) \\\\         g_2 = (r_1 \\cdot a_{21}) + (g_1 \\cdot a_{22}) + (b_1 \\cdot a_{23}) \\\\         b_2 = (r_1 \\cdot a_{31}) + (g_1 \\cdot a_{32}) + (b_1 \\cdot a_{33})     \\end{aligned}     \\] <p>Matrices using an offset calculation will have one more column than rows. An offset matrix may be defined using a 3x4 <code>Array</code>, wherein the fourth column is used to specify offset terms, \\(k_1\\), \\(k_2\\), \\(k_3\\): </p> \\[     \\begin{bmatrix}         a_{11} &amp; a_{12} &amp; a_{13} &amp; k_1\\\\         a_{21} &amp; a_{22} &amp; a_{23} &amp; k_2\\\\         a_{31} &amp; a_{32} &amp; a_{33} &amp; k_3\\\\     \\end{bmatrix}     \\begin{bmatrix}         r_1\\\\         g_1\\\\         b_1\\\\         1.0     \\end{bmatrix}     =     \\begin{bmatrix}         r_2\\\\         g_2\\\\         b_2     \\end{bmatrix} \\] <p>Expanded out, this means that the offset terms \\(k_1\\), \\(k_2\\), and \\(k_3\\) are added to each of the normal matrix calculations:</p> \\[     \\begin{aligned}     r_2 = (r_1 \\cdot a_{11}) + (g_1 \\cdot a_{12}) + (b_1 \\cdot a_{13}) + k_1\\\\     g_2 = (r_1 \\cdot a_{21}) + (g_1 \\cdot a_{22}) + (b_1 \\cdot a_{23}) + k_2\\\\     b_2 = (r_1 \\cdot a_{31}) + (g_1 \\cdot a_{32}) + (b_1 \\cdot a_{33}) + k_3     \\end{aligned}     \\] Elements: <code>Array</code> (required) a table that provides the coefficients of the transformation matrix. The matrix dimensions are either 3x3 or 3x4. The matrix is serialized row by row from top to bottom and from left to right, i.e., \"\\(a_{11}\\ a_{12}\\ a_{13}\\ a_{21}\\ a_{22}\\ a_{23}\\ \\ldots\\)\" for a 3x3 matrix. \\[     \\begin{bmatrix}         a_{11} &amp; a_{12} &amp; a_{13} \\\\         a_{21} &amp; a_{22} &amp; a_{23} \\\\         a_{31} &amp; a_{32} &amp; a_{33} \\\\     \\end{bmatrix} \\] Attributes: <code>dim</code> (required) <p>two integers that describe the dimensions of the matrix array. The first value define the number of rows and the second is the number of columns.  2 entries have the dimensions of a matrix</p> <p>Example</p> <p><code>dim = \"3 3\"</code> indicates a 3x3 matrix</p> <p>Example</p> <p><code>dim = \"3 4\"</code> indicates a 3x4 matrix</p> <p>Note</p> <p>Previous versions of this specification used three integers for the <code>dim</code> attribute, rather than the current two. In order to facilitate backwards compatibility, implementations should allow a third value for the <code>dim</code> attribute and may simply ignore it.</p> <p>Note</p> <p><code>Array</code> is formatted differently when it is contained in a LUT1D or LUT3D element (see Array)</p> <p>Examples: <pre><code>&lt;Matrix id=\"lut-28\" name=\"AP0 to AP1\" inBitDepth=\"16f\" outBitDepth=\"16f\" &gt;\n    &lt;Description&gt;3x3 color space conversion from AP0 to AP1&lt;/Description&gt;\n    &lt;Array dim=\"3 3\"&gt;\n         1.45143931614567     -0.236510746893740    -0.214928569251925\n        -0.0765537733960204    1.17622969983357     -0.0996759264375522\n         0.00831614842569772  -0.00603244979102103   0.997716301365324\n    &lt;/Array&gt;\n&lt;/Matrix&gt;\n</code></pre></p> <p>Example 3. Example of a <code>Matrix</code> node with <code>dim=\"3 3 3\"</code></p> <pre><code>&lt;Matrix id=\"lut-25\" name=\"colorspace conversion\" inBitDepth=\"10i\" outBitDepth=\"10i\" &gt;\n    &lt;Description&gt; 3x4 Matrix , 4th column is offset &lt;/Description&gt;\n    &lt;Array dim=\"3 4\"&gt;\n        1.2     0.0     0.0     0.002\n        0.0     1.03    0.001   -0.005\n        0.004   -0.007  1.004   0.0\n    &lt;/Array&gt;\n&lt;/Matrix&gt;\n</code></pre> <p>Example 4. Example of a <code>Matrix</code> node</p>"},{"location":"specifications/clf/#range","title":"<code>Range</code>","text":"<p>Description: This node maps the input domain to the output range by scaling and offsetting values. The <code>Range</code> element can also be used to clamp values.</p> <p>Unless otherwise specified, the node\u2019s default behavior is to scale and offset with clamping. If clamping is not desired, the <code>style</code> attribute can be set to <code>\"noClamp\"</code>.</p> <p>To achieve scale and/or offset of values, all of <code>minInValue</code>, <code>minOutValue</code>, <code>maxInValue</code>, and <code>maxOutValue</code> must be present. In this explicit case, the formula for <code>Range</code> shall be: </p> <p></p> \\[     out = in \\times scale + \\texttt{minOutValue} - \\texttt{minInValue} \\times scale \\] <p>where:  \\(scale = \\dfrac{(\\texttt{maxOutValue} - \\texttt{minOutValue})}{(\\texttt{maxInValue} - \\texttt{minInValue})}\\)</p> <p>The scaling of <code>minInValue</code> and <code>maxInValue</code> depends on the input bit-depth, and the scaling of <code>minOutValue</code> and <code>maxOutValue</code> depends on the output bit-depth.</p> <p>If <code>style=\"Clamp\"</code>, the output value of \\(out\\) from the above equation is furthur modified as follows:</p> <p></p> \\[     out_{clamped} = \\mathrm{MIN}(\\texttt{maxOutValue}, \\mathrm{MAX}( \\texttt{minOutValue}, out)) \\] <p>where:  \\(\\mathrm{MAX}(a,b)\\) returns \\(a\\) if \\(a &gt; b\\) and \\(b\\) if \\(b \\geq a\\) \\(\\mathrm{MIN}(a,b)\\) returns \\(a\\) if \\(a &lt; b\\) and \\(b\\) if \\(b \\leq a\\)</p> <p>The <code>Range</code> element can also be used to clamp values on only the top or bottom end. In such instances, no offset is applied, and the formula simplifies because only one pair of min or max values are required. (The <code>style</code> shall not be <code>\"noClamp\"</code> for this use-case.)</p> <p>If only the minimum value pair is provided, then the result shall be clamping at the low end, according to:</p> \\[     out = \\mathrm{MAX}( \\texttt{minOutValue}, in \\times bitDepthScale) \\] <p>Values must be set such that \\(\\texttt{minOutValue} = \\texttt{minInValue} \\times bitDepthScale\\).</p> <p>Likewise, if only the maximum values pairs are provided, the result shall be clamping at the high end, according to:</p> \\[     out = \\mathrm{MIN}( \\texttt{maxOutValue}, in \\times bitDepthScale) \\] <p>And values must be set such that \\(\\texttt{maxOutValue} = \\texttt{maxInValue} \\times bitDepthScale\\).</p> <p>The following formulas are used in the above equations:</p> \\[ bitDepthScale = \\dfrac{\\mathrm{scaleFactor}(\\texttt{outBitDepth})}{\\mathrm{scaleFactor}(\\texttt{inBitDepth})} \\] \\[ \\mathrm{scaleFactor}(a) = \\begin{cases}     2^{bitDepth}-1 &amp; \\text{when }a \\in \\{\\texttt{\"8i\"},\\texttt{\"10i\"},\\texttt{\"12i\"},\\texttt{\"16i\"}\\} \\\\     1.0 &amp; \\text{when }a \\in \\{\\texttt{\"16f\"},\\texttt{\"32f\"}\\} \\end{cases} \\] <p>Note</p> <p>The bit depth scale factor intentionally uses \\(2^{bitDepth}\u22121\\) and not \\(2^{bitDepth}\\). This means that the scale factor created for scaling between different bit depths is \"non-integer\" and is slightly different depending on the bit depths being scaled between. While instinct might be that this scale should be a clean bit-shift factor (i.e. \\(2\\times\\) or \\(4\\times\\) scale), testing with a few example values plugged into the formula will show that the resulting non-integer scale is the correct and intended behavior.</p> <p>At least one pair of either minimum or maximum values, or all four values, must be provided.</p> Elements: <code>minInValue</code> (optional) The minimum input value. Required if <code>minOutValue</code> is present. <code>maxInValue</code> (optional) The maximum input value. Required if <code>maxOutValue</code> is present. The <code>maxInValue</code> shall be greater than the <code>minInValue</code>. <code>minOutValue</code> (optional) The minimum output value. Required if <code>minInValue</code> is present. <code>maxOutValue</code> (optional) The maximum output value. Required if <code>maxInValue</code> is present.  The <code>maxOutValue</code> shall be greater than or equal to the <code>minOutValue</code>. Attributes: <code>style</code> (optional) Describes the preferred handling of the scaling calculation of the <code>Range</code> node. If the style attribute is not present, clamping is performed. The options for <code>style</code> are: <code>\"noClamp\"</code> If present, scale and offset is applied without clamping (i.e. values below <code>minOutValue</code> or above <code>maxOutValue</code> are preserved) <code>\"Clamp\"</code> If present, clamping is applied upon the result of the scale and offset expressed by the result of the non-clamping <code>Range</code> equation <p>Examples: <pre><code>&lt;Range inBitDepth=\"10i\" outBitDepth=\"10i\"&gt;\n    &lt;Description&gt;10-bit full range to SMPTE range&lt;/Description&gt;\n    &lt;minInValue&gt;0&lt;/minInValue&gt;\n    &lt;maxInValue&gt;1023&lt;/maxInValue&gt;\n    &lt;minOutValue&gt;64&lt;/minOutValue&gt;\n    &lt;maxOutValue&gt;940&lt;/maxOutValue&gt;\n&lt;/Range&gt;\n</code></pre></p> <p>Example 5. Using <code>\"Range\"</code> for scaling 10-bit full range to 10-bit SMPTE (legal) range.</p>"},{"location":"specifications/clf/#log","title":"<code>Log</code>","text":"<p>Description: This node contains parameters for processing pixels through a logarithmic or anti-logarithmic function. A couple of main formulations are supported. The most basic formula follows a pure logarithm or anti-logarithm of either base 2 or base 10. Another supported formula allows for a logarithmic function with a gain factor and offset. This formulation can be used to convert from linear to Cineon. Another style of log formula follows a piece-wise function consisting of a logarithmic function with a gain factor, an offset, and a linear segment. This style can be used to implement many common \u201ccamera-log\u201d encodings.</p> <p>Note</p> <p>The equations for the <code>Log</code> node assume integer data is normalized to floating-point scaling. <code>LogParams</code> do not change based on the input and output bit-depths.</p> <p>Note</p> <p>On occasion it may be necessary to transform a logarithmic function specified in terms of traditional Cineon-style parameters to the parameters used by CLF. Guidance on how to do this is provided in Appendix B.</p> Attributes: <code>style</code> (required) <p>specifies the form of the of log function to be applied Supported values for \u201dstyle\u201d are:</p> <ul> <li><code>\"log10\"</code></li> <li><code>\"antiLog10\"</code></li> <li><code>\"log2\"</code></li> <li><code>\"antiLog2\"</code></li> <li><code>\"linToLog\"</code></li> <li><code>\"logToLin\"</code></li> <li><code>\"cameraLinToLog\"</code></li> <li><code>\"cameraLogToLin\"</code></li> </ul> <p>The formula to be applied for each style is described by the equations below, for all of which:</p> \\[ \\texttt{FLT_MIN} = 1.175494 \\times 10^{-38} \\] <p>\\(\\textrm{MAX}(a, b)\\) returns \\(a\\) if \\(a \\gt b\\) and \\(b\\) if \\(b \\geq a\\)</p> <ul> <li><code>\"log10\"</code>: applies a base 10 logarithm according to  </li> </ul> \\[ y = log_{10}(\\textrm{MAX}(x,\\texttt{FLT_MIN})) \\] <ul> <li><code>\"antiLog10\"</code>: applies a base 10 anti-logarithm according to</li> </ul> \\[  x = 10^{y} \\] <ul> <li><code>\"log2\"</code>: applies a base 2 logarithm according to</li> </ul> \\[  y = log_{2}(\\textrm{MAX}(x,\\texttt{FLT_MIN})) \\] <ul> <li><code>\"antiLog2\"</code>: applies a base 2 anti-logarithm according to</li> </ul> \\[  x = 2^{y} \\] <ul> <li><code>\"linToLog\"</code>: applies a logarithm according to</li> </ul> \\[  y = \\text{logSideSlope} \\times \\text{log}_\\text{base}(\\textrm{MAX}(\\text{linSideSlope} \\times x + \\text{linSideOffset}, \\texttt{FLT_MIN}))+\\text{logSideOffset} \\] <ul> <li><code>\"logToLin\"</code>: applies an anti-logarithm according to</li> </ul> \\[ x = \\frac{\\left(\\text{base}^{\\frac{y-\\text{logSideOffset}}{\\text{logSideSlope}}} - \\text{linSideOffset}\\right)}{\\text{linSideSlope}} \\] <ul> <li><code>\"cameraLinToLog\"</code>: applies a piecewise function with logarithmic and linear segments on linear values, converting them to non-linear values</li> </ul> <p></p> \\[ y = \\begin{cases}  \\text{linearSlope} \\times x + \\text{linearOffset} &amp; \\text{if } x \\leq \\text{linSideBreak}\\\\ \\text{logSideSlope} \\times \\text{log}_{\\text{base}}(\\mathrm{MAX}(\\text{linSideSlope} \\times x + \\text{linSideOffset},\\texttt{FLT_MIN})) + \\text{logSideOffset} &amp; \\text{otherwise} \\\\ \\end{cases} \\\\ \\] <p>Note</p> <p>The calculation of \\(\\text{linearSlope}\\), and \\(\\text{linearOffset}\\) is described in Solving for <code>LogParams</code></p> <ul> <li><code>\"cameraLogToLin\"</code>: applies a piecewise function with logarithmic and linear segments on non-linear values, converting them to linear values</li> </ul> \\[ x = \\begin{cases} \\frac{(y - \\text{linearOffset})}{\\text{linearSlope}} &amp; \\text{if } y \\leq \\text{logSideBreak} \\\\                    \\frac{\\left(\\text{base}^{\\frac{y-\\text{logSideOffset}}{\\text{logSideSlope}}} - \\text{linSideOffset}\\right)}{\\text{linSideSlope}} &amp; \\text{otherwise} \\end{cases} \\] <p>Note</p> <p>The calculation of \\(\\text{logSideBreak}\\), \\(\\text{linearSlope}\\), and \\(\\text{linearOffset}\\) is described in Solving for <code>LogParams</code></p> Elements: <code>LogParams</code> (required - if <code>\"style\"</code> is not a basic logarithm) <p>contains the attributes that control the <code>\"linToLog\"</code>, <code>\"logToLin\"</code>, <code>\"cameraLinToLog\"</code>, or <code>\"cameraLogToLin\"</code> functions  This element is required if <code>style</code> is of type <code>\"linToLog\"</code>, <code>\"logToLin\"</code>, <code>\"cameraLinToLog\"</code>, or <code>\"cameraLogToLin\"</code>.  Attributes:</p> <code>\"base\"</code> (optional) the base of the logarithmic function  Default is 2. <code>\"logSideSlope\"</code> (optional) \"slope\" (or gain) applied to the log side of the logarithmic segment. Default is 1. <code>\"logSideOffset\"</code> (optional) offset applied to the log side of the logarithmic segment. Default is 0. <code>\"linSideSlope\"</code> (optional) slope of the linear side of the logarithmic segment.  Default is 1. <code>\"linSideOffset\"</code> (optional) offset applied to the linear side of the logarithmic segment.  Default is 0. <code>\"linSideBreak\"</code> (optional) the break-point, defined in linear space, at which the piece-wise function transitions between the logarithmic and linear segments. This is required if <code>style=\"cameraLinToLog\"</code> or <code>\"cameraLogToLin\"</code> <code>\"linearSlope\"</code> (optional) the slope of the linear segment of the piecewise function. This attribute does not need to be provided unless the formula being implemented requires it. The default is to calculate using <code>linSideBreak</code> such that the linear portion is continuous in value with the logarithmic portion of the curve, by using the value of the logarithmic portion of the curve at the break-point. This is described in the following note below. <code>\"channel\"</code> (optional) the color channel to which the exponential function is applied. Possible values are <code>\"R\"</code>, <code>\"G\"</code>, <code>\"B\"</code>. If this attribute is utilized to target different adjustments per channel, then up to three <code>LogParams</code> elements may be used, provided that <code>\"channel\"</code> is set differently in each. However, the same value of base must be used for all channels. If this attribute is not otherwise specified, the logarithmic function is applied identically to all three color channels. <p></p> <p>Solving for <code>LogParams</code></p> <p>\\(\\text{linearOffset}\\) is the offset of the linear segment of the piecewise function. This value is calculated using the position of the break-point and the linear slope in order to ensure continuity of the two segments. The following steps describe how  to calculate \\(\\text{linearOffset}\\).</p> <p>First, the value of the break-point on the log-axis is calculated using the value of \\(\\text{linSideBreak}\\) as input to the logarithmic segment of the piecewise function, as below:</p> <p></p> \\[ \\text{logSideBreak} = \\text{logSideSlope} \u00d7 \\text{log}_{base}(\\text{linSideSlope} \u00d7 \\textbf{linSideBreak} + \\text{linSideOffset}) + \\text{logSideOffset} \\] <p>Then, if \\(\\text{linearSlope}\\) was not provided, the value of \\(\\text{linSideBreak}\\) is used again to solve for the derivative of the logarithmic function. The value of \\(\\text{linearSlope}\\) is set to equal the instantaneous slope at the break-point, or derivative, as shown below:</p> <p></p> \\[  \\text{linearSlope} = \\text{logSideSlope} \\times \\left(\\frac{\\text{linSideSlope}}{(\\text{linSideSlope} \\times \\textbf{linSideBreak} + \\text{linSideOffset}) \\times \\text{ln}(\\text{base})}\\right) \\] <p>Finally, the value of \\(\\text{linearOffset}\\) can be solved for by rearranging the linear segment of of the piecewise function and using the values of \\(\\text{logSideBreak}\\) and \\(\\text{linearSlope}\\), as below:</p> <p></p> \\[ \\text{linearOffset} = \\textbf{logSideBreak} - \\textbf{ linearSlope} \\times \\text{linSideBreak} \\] <p>Examples: <pre><code>&lt;Log inBitDepth=\"16f\" outBitDepth=\"16f\" style=\"log10\"&gt; \n    &lt;Description&gt;Base 10 Logarithm&lt;/Description&gt;\n&lt;/Log&gt;\n</code></pre></p> <p>Example 6. <code>Log</code> node applying a base 10 logarithm.</p> <pre><code>&lt;Log inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"cameraLinToLog\"&gt;\n    &lt;Description&gt;Linear to DJI D-Log&lt;/Description&gt;\n    &lt;LogParams base=\"10\" logSideSlope=\"0.256663\" logSideOffset=\"0.584555\"\n        linSideSlope=\"0.9892\" linSideOffset=\"0.0108\" linSideBreak=\"0.0078\"\n        linearSlope=\"6.025\"/&gt;\n&lt;/Log&gt;\n</code></pre> <p>Example 7. <code>Log</code> node applying the DJI D-Log formula.</p>"},{"location":"specifications/clf/#exponent","title":"<code>Exponent</code>","text":"<p>Description: This node contains parameters for processing pixels through a power law function. Two main formulations are supported. The first follows a pure power law. The second is a piecewise function that follows a power function for larger values and has a linear segment that is followed for small and negative values. The latter formulation can be used to represent the Rec. 709, sRGB, and CIE L* equations.</p> Attributes: <code>style</code> (required) <p>specifies the form of the exponential function to be applied. Supported values are:</p> <ul> <li><code>\"basicFwd\"</code></li> <li><code>\"basicRev\"</code></li> <li><code>\"basicMirrorFwd\"</code></li> <li><code>\"basicMirrorRev\"</code></li> <li><code>\"basicPassThruFwd\"</code></li> <li><code>\"basicPassThruRev\"</code></li> <li><code>\"monCurveFwd\"</code></li> <li><code>\"monCurveRev\"</code></li> <li><code>\"monCurveMirrorFwd\"</code></li> <li><code>\"monCurveMirrorRev\"</code></li> </ul> <p>Each of these supported styles are described in detail below, and for all of which the following definitions apply:  \\(g =\\) <code>exponent</code> \\(k =\\) <code>offset</code> \\(\\textrm{MAX}(a, b)\\) returns \\(a\\) if \\(a \\gt b\\) and \\(b\\) if \\(b \\geq a\\) </p> <code>\"basicFwd\"</code> applies a power law using the exponent value specified in the <code>ExponentParams</code> element.  Values less than zero are clamped. \\[     \\text{basicFwd}(x) = [\\textrm{MAX}(0,x)]^g \\] <code>\"basicRev\"</code> applies power law using the exponent value specified in the <code>ExponentParams</code> element.  Values less than zero are clamped. \\[     \\text{basicRev}(y) = [\\textrm{MAX}(0,y)]^{1/g} \\] <code>\"basicMirrorFwd\"</code> applies a basic power law using the exponent value specified in the <code>ExponentParams</code> element for values greater than or equal to zero and mirrors the function for values less than zero (i.e. rotationally symmetric around the origin): \\[ \\text{basicMirrorFwd}(x) = \\begin{cases} x^{g} &amp; \\text{if } x \\geq 0 \\\\ [6pt] -\\Big[(-x)^{g}\\Big] &amp; \\text{otherwise} \\end{cases} \\] <code>\"basicMirrorRev\"</code> applies a basic power law using the exponent value specified in the <code>ExponentParams</code> element for values greater than or equal to zero and mirrors the function for values less than zero (i.e. rotationally symmetric around the origin): \\[ \\text{basicMirrorRev}(y) = \\begin{cases} y^{1/g} &amp; \\text{if } y \\geq 0 \\\\[6pt] -\\Big[(-y)^{1/g}\\Big] &amp; \\text{otherwise} \\end{cases} \\] <code>\"basicPassThruFwd\"</code> applies a basic power law using the exponent value specified in the <code>ExponentParams</code> element for values greater than or equal to zero and passes values less than zero unchanged: \\[ \\text{basicPassThruFwd}(x) = \\begin{cases} x^{g} &amp; \\text{if } x \\geq 0 \\\\[6pt] x &amp; \\text{otherwise} \\end{cases} \\] <code>\"basicPassThruRev\"</code> applies a basic power law using the exponent value specified in the <code>ExponentParams</code> element for values greater than or equal to zero and and passes values less than zero un- changed: \\[ \\text{basicPassThruRev}(y) = \\begin{cases} y^{1/g} &amp; \\text{if } y \\geq 0 \\\\[6pt] y &amp; \\text{otherwise} \\end{cases} \\] <code>\"monCurveFwd\"</code> applies a power law function with a linear segment near the origin <p></p> \\[ \\text{monCurveFwd}(x) = \\begin{cases} \\left( \\frac{x\\:+\\:k}{1\\:+\\:k} \\right)^{g} &amp; \\text{if } x \\geq xBreak \\\\[8pt] x\\:s &amp; \\text{otherwise} \\end{cases} \\] where:  \\(xBreak = \\dfrac{k}{g-1}\\) and, for the \\(\\text{monCurveFwd}\\) (above) and \\(\\text{monCurveRev}\\) (below) equations:  \\(s = \\left(\\dfrac{g-1}{k}\\right)  \\left(\\dfrac{k g}{(g-1)(1+k)}\\right)^{g}\\) <code>\"monCurveRev\"</code> applies a power law function with a linear segment near the origin <p></p> \\[ \\text{monCurveRev}(y) = \\begin{cases} (1 + k)\\:y^{(1/g)} - k &amp; \\text{if } y \\geq yBreak \\\\[8pt] \\dfrac{y}{s} &amp; \\text{otherwise} \\end{cases} \\] where:  \\(yBreak = \\left(\\dfrac{k g}{(g-1)(1+k)}\\right)^g\\) <code>\"monCurveMirrorFwd\"</code> applies a power law function with a linear segment near the origin and mirrors the function for values less than zero (i.e. rotationally symmetric around the origin): \\[ \\text{monCurveMirrorFwd}(x) = \\begin{cases} \\text{monCurveFwd}(x) &amp; \\text{if } x \\geq 0 \\\\[8pt] -[\\text{monCurveFwd}(-x)] &amp; \\text{otherwise} \\end{cases}     \\] <code>\"monCurveMirrorRev\"</code> applies a power law function with a linear segment near the origin and mirrors the function for values less than zero (i.e. rotationally symmetric around the origin): \\[ \\text{monCurveMirrorRev}(y) = \\begin{cases} \\text{monCurveRev}(y) &amp; \\text{if } y \\geq 0 \\\\[8pt] -[\\text{monCurveRev}(-y)] &amp; \\text{otherwise} \\end{cases} \\] <p>Note</p> <p>The above equations assume that the input and output bit-depths are floating-point. Integer values are normalized to the range \\([0.0, 1.0]\\).</p> Elements: <code>ExponentParams</code> (required) <p>contains one or more attributes that provide the values to be used by the enclosing <code>Exponent</code> element.  If <code>style</code> is any of the \u201cbasic\u201d types, then only <code>exponent</code> is required.  If <code>style</code> is any of the \u201cmonCurve\u201d types, then <code>exponent</code> and <code>offset</code> are required.</p> Attributes: <code>\"exponent\"</code> (required) the power to which the value is to be raised  If style is any of the \u201cmonCurve\u201d types, the valid range is \\([1.0, 10.0]\\). The nominal value is 1.0. <p>Note</p> <p>When using a \u201cmonCurve\u201d style, a value of 1.0 assigned to <code>exponent</code> could result in a divide-by-zero error. Implementors should protect against this case.</p> <code>\"offset\"</code> (optional) the offset value to use  If offset is used, the enclosing <code>Exponent</code> element\u2019s style attribute must be set to one of the \u201cmonCurve\u201d types. Offset is not allowed when <code>style</code> is any of the \u201cbasic\u201d types.  The valid range is \\([0.0, 0.9]\\). The nominal value is 0.0. <p>Note</p> <p>If zero is provided as a value for <code>offset</code>, the calculation of \\(xBreak\\) or \\(yBreak\\) could result in a divide-by-zero error. Implementors should protect against this case.</p> <code>\"channel\"</code> (optional) the color channel to which the exponential function is applied.  Possible values are <code>\"R\"</code>, <code>\"G\"</code>, <code>\"B\"</code>.  If this attribute is utilized to target different adjustments per channel, up to three <code>ExponentParams</code> elements may be used, provided that <code>\"channel\"</code> is set differently in each. If this attribute is not otherwise specified, the exponential function is applied identically to all three color channels. <p>Examples: <pre><code>&lt;Exponent inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"basicFwd\"&gt;\n    &lt;Description&gt;Basic 2.2 Gamma&lt;/Description&gt;\n    &lt;ExponentParams exponent=\"2.2\"/&gt;\n&lt;/Exponent&gt;\n</code></pre></p> <p>Example 8. Using <code>Exponent</code> node for applying a 2.2 gamma.</p> <pre><code>&lt;Exponent inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"monCurveFwd\"&gt;\n    &lt;Description&gt;EOTF (sRGB)&lt;/Description&gt;\n    &lt;ExponentParams exponent=\"2.4\" offset=\"0.055\" /&gt;\n&lt;/Exponent&gt;\n</code></pre> <p>Example 9. Using <code>Exponent</code> node for applying the intended EOTF found in IEC 61966-2-1:1999 (sRGB).</p> <pre><code>&lt;Exponent inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"monCurveRev\"&gt;\n    &lt;Description&gt;CIE L*&lt;/Description&gt;\n    &lt;ExponentParams exponent=\"3.0\" offset=\"0.16\" /&gt;\n&lt;/Exponent&gt;\n</code></pre> <p>Example 10. Using <code>Exponent</code> node to apply CIE L* formula.</p> <pre><code>&lt;Exponent inBitDepth=\"32f\" outBitDepth=\"32f\" style=\"monCurveRev\"&gt;\n    &lt;Description&gt;Rec. 709 OETF&lt;/Description&gt;\n    &lt;ExponentParams exponent=\"2.2222222222222222\" offset=\"0.099\" /&gt;\n&lt;/Exponent&gt;\n</code></pre> <p>Example 11. Using <code>Exponent</code> node to apply Rec. 709 OETF.</p>"},{"location":"specifications/clf/#asc_cdl","title":"<code>ASC_CDL</code>","text":"<p>Description: This node processes values according to the American Society of Cinematographers\u2019 Color Decision List (ASC CDL) equations. Color correction using ASC CDL is an industry-wide method of recording and exchanging basic color correction adjustments via parameters that set particular color processing equations.</p> <p>The ASC CDL equations are designed to work on an input domain of floating-point values of [0 to 1.0] although values greater than 1.0 can be present. The output data may or may not be clamped depending on the processing style used. </p> <p>If the <code>style</code>attribute is not specified, the node shall default to <code>\"Fwd\"</code> - i.e. the classic implementation of the v1.2 ASC-CDL equations.</p> <p>Note</p> <p>Equations 4.31-4.34 assume that \\(in\\) and \\(out\\) are scaled to normalized floating-point range. If the <code>ASC_CDL</code> node has <code>inBitDepth</code> or <code>outBitDepth</code> that are integer types, then the input or output values must be normalized to or from 0-1 scaling. In other words, the slope, offset, power, and saturation values stored in the <code>ProcessNode</code> do not depend on <code>inBitDepth</code> and <code>outBitDepth</code>; they are always interpreted as if the bit depths were float.</p> <p>Attributes:</p> <code>id</code> (optional) This should match the id attribute of the ColorCorrection element in the ASC CDL XML format. <code>style</code> <p>Determines the formula applied by the operator. The valid options are:</p> `\"Fwd\" implementation of v1.2 ASC CDL equation (default) <code>\"Rev\"</code> inverse equation <code>\"FwdNoClamp\"</code> similar to the Fwd equation, but without clamping <code>\"RevNoClamp\"</code> inverse equation, without clamping <p>The first two implement the math provided in version 1.2 of the ASC CDL specification. The second two omit the clamping step and are intended to provide compatibility with the many applications that take that alternative approach.</p> <p>Elements:</p> <code>SOPNode</code> (optional) <p>The <code>SOPNode</code> is optional, and if present, must contain each of the following sub-elements:</p> <code>Slope</code> three decimal values representing the R, G, and B slope values, which is similar to gain, but changes the slope of the transfer function without shifting the black level established by <code>offset</code>  Valid values for slope must be greater than or equal to zero (\\(\\geq\\) 0). The nominal value is 1.0 for all channels. <code>Offset</code> three decimal values representing the R, G, and B offset values, which raise or lower overall brightness of a color component by shifting the transfer function up or down while holding the slope constant  The nominal value is 0.0 for all channels. <code>Power</code> three decimal values representing the R, G, and B power values, which change the intermediate shape of the transfer function  Valid values for power must be greater than zero (\\(\\gt\\) 0).  The nominal value is 1.0 for all channels. <code>SatNode</code> (optional) <p>The <code>SatNode</code> is optional, but if present, must contain one of the following sub-element:</p> <code>Saturation</code> a single decimal value applied to all color channels  Valid values for saturation must be greater than or equal to zero (\\(\\geq\\) 0).  The nominal value is 1.0. <p>Note</p> <p>If either element is not specified, values  should default to the nominal values for each element. If using the <code>\"noClamp\"</code> style, the result of the defaulting to the nominal values is a no-op.</p> <p>Note</p> <p>The structure of this <code>ProcessNode</code> matches the structure of the XML format described in the v1.2 ASC CDL specification. However, unlike the ASC CDL XML format, there are no alternate spellings allowed for these elements.</p> <p>The math for <code>style=\"Fwd\"</code> is:</p> \\[ out_{\\textrm{SOP}} = \\textrm{CLAMP}(in \\times \\textrm{slope} + \\textrm{offset})^{\\textrm{power}} \\] \\[ \\begin{aligned} luma &amp;= 0.2126 \\times out_{\\textrm{SOP,R}} + 0.7152 \\times out_{\\textrm{SOP,G}} + 0.0722 \\times out_{\\textrm{SOP,B}} \\\\ out &amp;= \\textrm{CLAMP}\\Big[luma + \\textrm{saturation} \\times (out_{\\textrm{SOP}} \u2212 luma)\\Big] \\end{aligned} \\] <p>Where:  \\(\\textrm{CLAMP()}\\) clamps the argument to \\([0,1]\\)</p> <p>The math for <code>style=\"FwdNoClamp\"</code> is the same as for <code>\"Fwd\"</code> but the two clamp() functions are omitted.  Also, if \\((in \\times \\textrm{slope} + \\textrm{offset}) &lt; 0\\), then no power function is applied.</p> <p>The math for <code>style=\"Rev\"</code> is:</p> \\[ \\begin{aligned} in_{\\textrm{clamp}} &amp;= \\mathrm{CLAMP}(in) \\\\ luma &amp;= 0.2126 \\times in_{\\textrm{clamp,R}} + 0.7152 \\times in_{\\textrm{clamp,G}} + 0.0722 \\times in_{\\textrm{clamp,B}} \\\\ out_{\\textrm{SAT}} &amp;= luma + \\frac{(in_{\\textrm{clamp}} - luma)}{\\textrm{saturation}} \\end{aligned} \\] \\[ out = \\mathrm{CLAMP}\\left(\\frac{\\mathrm{CLAMP}(out_{\\textrm{SAT}})^{\\frac{1}{\\textrm{power}}} - \\textrm{offset}}{\\textrm{slope}}\\right) \\\\ \\] <p>Where:  \\(\\textrm{CLAMP()}\\) clamps the argument to \\([0,1]\\)</p> <p>The math for <code>style=\"RevNoClamp\"</code> is the same as for <code>\"Rev\"</code> but the \\(\\textrm{CLAMP}()\\) functions are omitted.  Also, if \\(out_{\\textrm{SAT}} \\lt 0\\), then no power function is applied.</p> <p>Examples: <pre><code>&lt;ASC_CDL id=\"cc01234\" inBitDepth=\"16f\" outBitDepth=\"16f\" style=\"Fwd\"&gt;\n    &lt;Description&gt;scene 1 exterior look&lt;/Description&gt;\n    &lt;SOPNode&gt;\n        &lt;Slope&gt;1.000000 1.000000 0.900000&lt;/Slope&gt;\n        &lt;Offset&gt;-0.030000 -0.020000 0.000000&lt;/Offset&gt;\n        &lt;Power&gt;1.2500000 1.000000 1.000000&lt;/Power&gt;\n    &lt;/SOPNode&gt;\n    &lt;SatNode&gt;\n        &lt;Saturation&gt;1.700000&lt;/Saturation&gt;\n    &lt;/SatNode&gt;\n&lt;/ASC_CDL&gt;\n</code></pre></p> <p>Example 12. Example of an <code>ASC_CDL</code> node.</p>"},{"location":"specifications/clf/#implementation-notes","title":"Implementation Notes","text":""},{"location":"specifications/clf/#bit-depth","title":"Bit Depth","text":""},{"location":"specifications/clf/#precision","title":"Processing Precision","text":"<p>All processing shall be performed using 32-bit floating-point values. The values of the <code>inBitDepth</code> and <code>outBitDepth</code> attributes shall not affect the quantization of color values.</p> <p>Note</p> <p>For some hardware devices, 32-bit float processing might not be possible. In such instances, processing should be performed at the highest precision available. Because CLF permits complex series of discrete operations, CLF LUT files are unlikely to run on hardware devices without some form of pre-processing. Any pre-processing to prepare a CLF for more limited hardware applications should adhere to the processing precision requirements.</p>"},{"location":"specifications/clf/#in-out-processlist","title":"Input To and Output From a ProcessList","text":"<p>Applications often support multiple pixel formats (e.g. 8i, 10i, 16f, 32f, etc.). Often the actual pixel format to be processed may not agree with the <code>inBitDepth</code> of the first ProcessNode or the <code>outBitDepth</code> of the last ProcessNode. (Note that the <code>ProcessList</code> element itself does not contain global <code>inBitDepth</code> or <code>outBitDepth</code> attributes.) Therefore, in some cases an application may need to rescale a given <code>ProcessNode</code> to be appropriate for the actual image data being processed.</p> <p>For example, if the last ProcessNode in a ProcessList is a <code>LUT1D</code> with an <code>outBitDepth</code> of 12i, it indicates that the LUT Array values are scaled relative to 4095. If the application wants to produce floating-point pixel values, it should therefore divide the LUT Array values by 4095 before processing the pixels (according to Conversion). Likewise, if the <code>outBitDepth</code> was instead 32f and the application wants to produce 12i pixel values, it should multiply the LUT Array values by 4095. (Note that in this case, since the result of the computations may exceed 4095 and the application wants to produce 12-bit integer output, the application would want to clamp, round, and quantize the value.)</p>"},{"location":"specifications/clf/#in-out-processnode","title":"Input To and Output From a ProcessNode","text":"<p>In order to ensure the scaling of parameter values of all ProcessNodes in a ProcessList are consistent, the <code>inBitDepth</code> of each ProcessNode must match the <code>outBitDepth</code> of the previous ProcessNode (if any).</p> <p>Please note that an integer <code>inBitDepth</code> or <code>outBitDepth</code> of a ProcessNode does not indicate that any clamping or quantization should be done. These attributes are strictly used to indicate the scaling of parameter and array values. As discussed above, processing precision shall be floating-point.</p> <p>Furthermore, because the processing precision is intended to be floating-point, the <code>inBitDepth</code> and <code>outBitDepth</code> only control the scaling of parameter and array values and do not impose range limits. For example, even if the <code>outBitDepth</code> of a LUT Array is 12i, it does not mean that the Array values must be limited to \\([0,4095]\\) or that they must be integer values. It simply means that in order to rescale to 32f that a scale factor of 1/4095 should be used (as per Conversion).</p> <p>Because processing within a ProcessList should be done at floating-point precision, applications may optionally want to rescale the interfaces all ProcessNodes \u201cinterior\u201d to a ProcessList to be 32f according to Conversion. As discussed in Input To and Output From a ProcessList, applications may want to rescale the \u201cexterior\u201d interfaces of the ProcessList based on the type of pixel data being processed.</p> <p>For some applications, it may be easiest to simply rescale all ProcessNodes to 32f input and output bit-depth when parsing the file. That way, the ProcessList may be considered a purely 32f set of operations and the implementation therefore does not need to track or deal with bit-depth differences at the ProcessNode level.</p>"},{"location":"specifications/clf/#scaling","title":"Conversion Between Integer and Normalized Float Scaling","text":"<p>As discussed above, the <code>inBitDepth</code> or <code>outBitDepth</code> of a ProcessNode may need to be rescaled in order to accommodate the pixel data type being processed by the application.</p> <p>The scale factor associated with the bit-depths 8i, 10i, 12i, and 16i is \\(2^n \u2212 1\\), where \\(n\\) is the bit-depth. </p> <p>The scale factor associated with the bit-depths 16f and 32f is 1.0.</p> <p>To rescale Matrix, LUT1D, or LUT3D <code>Array</code> values when the <code>outBitDepth</code> changes, the scale factor is equal to \\(\\frac{\\textrm{newScale}}{\\textrm{oldScale}}\\). For example, to convert from 12i to 10i, multiply array values by \\(1023/4095\\).</p> <p>To rescale Matrix <code>Array</code> values when the <code>inBitDepth</code> changes, the scale factor is equal to \\(\\frac{\\textrm{oldScale}}{\\textrm{newScale}}\\). For example, to convert from 32f to 10i, multiply array values by \\(1/1023\\).</p> <p>To rescale Range parameters when the <code>inBitDepth</code> changes, the scale factor for <code>minInValue</code> and <code>maxInValue</code> is \\(\\frac{\\textrm{newScale}}{\\textrm{oldScale}}\\). To rescale Range parameters when the <code>outBitDepth</code> changes, the scale factor for <code>minOutValue</code> and <code>maxOutValue</code> is \\(\\frac{\\textrm{newScale}}{\\textrm{oldScale}}\\).</p> <p>Please note that in all cases, the conversion shall be only a scale factor. In none of the above cases should clamping or quantization be applied.</p> <p>Aside from the specific cases listed above, changes to <code>inBitDepth</code> and <code>outBitDepth</code> do not affect the parameter or array values of a given ProcessNode.</p> <p>If an application needs to convert between different integer pixel formats or between integer and float (or vice versa) on the way into or out of a ProcessList, the same scale factors should be used. Note that when converting from floating-point to integer at the application level that values should be clamped, rounded, and quantized.</p>"},{"location":"specifications/clf/#required-vs-optional","title":"Required vs Optional","text":"<p>The required or optional indicated in parentheses throughout this specification indicate the requirement for an element or attribute to be present for a valid CLF file. In the spirit of a LUT format to be used commonly across different software and hardware, none of the elements or attributes should be considered optional for implementors to support. All elements and attributes, if present, should be recognized and supported by an implementation.</p> <p>If, due to hardware or software limitations, a particular element or attribute is not able to be supported, a warning should be issued to the user of a LUT that contains one of the offending elements. The focus shall be on the user and maintaining utmost compatibility with the specification so that LUTs can be interchanged seamlessly.</p>"},{"location":"specifications/clf/#efficient-processing","title":"Efficient Processing","text":"<p>The transform engine may merge some ProcessNodes in order to obtain better performance. For example, adjacent <code>Matrix</code> operators may be combined into a single matrix. However, in general, combining operators in a way that preserves accuracy is difficult and should be avoided.</p> <p>Hardware implementations may need to convert all ProcessNodes into some other form that is consistent with what the hardware supports. For example, all ProcessNodes might need to be combined into a single 3D LUT. Using a grid size of 64 or larger is recommended to preserve as much accuracy as possible. Implementors should be aware that the success of such approximations varies greatly with the nature of the input and output color spaces. For example, if the input color space is scene-linear in nature, it may be necessary to use a \u201cshaper LUT\u201d or similar non-linearity before the 3D LUT in order to convert values into a more perceptually uniform representation.</p>"},{"location":"specifications/clf/#extensions","title":"Extensions","text":"<p>It is recommended that implementors of CLF file readers protect against unrecognized elements or attributes that are not defined in this specification. Unrecognized elements that are not children of the <code>Info</code> element should either raise an error or at least provide a warning message to the user to indicate that there is an operator present that is not recognized by the reader. Applications that need to add custom metadata should place it under the <code>Info</code> element rather than at the top level of the ProcessList.</p> <p>One or more <code>Description</code> elements in the ProcessList can and should be used for metadata that does not fit into a provided field in the <code>Info</code> element and/or is unlikely to be recognized by other applications.</p>"},{"location":"specifications/clf/#examples","title":"Examples","text":"<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;ProcessList id=\"ACEScsc.ACES_to_ACEScg.a1.0.3\" name=\"ACES2065-1 to ACEScg\" \n    compCLFversion=\"3.0\"&gt;\n    &lt;Info&gt;\n        &lt;ACEStransformID&gt;ACEScsc.ACES_to_ACEScg.a1.0.3&lt;/ACEStransformID&gt;\n        &lt;ACESuserName&gt;ACES2065-1 to ACEScg&lt;/ACESuserName&gt;\n    &lt;/Info&gt;\n    &lt;Description&gt;ACES2065-1 to ACEScg&lt;/Description&gt;\n    &lt;InputDescriptor&gt;ACES2065-1&lt;/InputDescriptor&gt;\n    &lt;OutputDescriptor&gt;ACEScg&lt;/OutputDescriptor&gt;\n    &lt;Matrix inBitDepth=\"16f\" outBitDepth=\"16f\"&gt;\n        &lt;Array dim=\"3 3\"&gt;\n             1.451439316146 -0.236510746894 -0.214928569252\n            -0.076553773396  1.176229699834 -0.099675926438\n             0.008316148426 -0.006032449791  0.997716301365\n        &lt;/Array&gt;\n    &lt;/Matrix&gt;\n&lt;/ProcessList&gt;\n</code></pre> <p>Example 13. ACES2065-1 to ACEScg</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;ProcessList id=\"ACEScsc.ACES_to_ACEScct.a1.0.3\" name=\"ACES2065-1 to ACEScct\" \n    compCLFversion=\"3.0\"&gt;\n    &lt;Description&gt;ACES2065-1 to ACEScct Log working space&lt;/Description&gt;\n    &lt;InputDescriptor&gt;Academy Color Encoding Specification (ACES2065-1)&lt;/InputDescriptor&gt;\n    &lt;OutputDescriptor&gt;ACEScct Log working space&lt;/OutputDescriptor&gt;\n    &lt;Info&gt;\n        &lt;ACEStransformID&gt;ACEScsc.ACES_to_ACEScct.a1.0.3&lt;/ACEStransformID&gt;\n        &lt;ACESuserName&gt;ACES2065-1 to ACEScct&lt;/ACESuserName&gt;\n    &lt;/Info&gt;\n    &lt;Matrix inBitDepth=\"16f\" outBitDepth=\"16f\"&gt;\n        &lt;Array dim=\"3 3\"&gt;\n             1.451439316146 -0.236510746894 -0.214928569252\n            -0.076553773396  1.176229699834 -0.099675926438\n             0.008316148426 -0.006032449791  0.997716301365\n        &lt;/Array&gt;\n    &lt;/Matrix&gt;\n    &lt;Log inBitDepth=\"16f\" outBitDepth=\"16f\" style=\"cameraLinToLog\"&gt;\n        &lt;LogParams base=\"2\" logSideSlope=\"0.05707762557\" logSideOffset=\"0.5547945205\" \n            linSideBreak=\"0.0078125\" /&gt;\n    &lt;/Log&gt;\n&lt;/ProcessList&gt;\n</code></pre> <p>Example 14. ACES2065-1 to ACEScct</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;ProcessList id=\"5ac02dc7-1e02-4f87-af46-fa5a83d5232d\" compCLFversion=\"3.0\"&gt;\n    &lt;Description&gt;CIE-XYZ D65 to CIELAB L*, a*, b* (scaled by 1/100, neutrals at \n        0.0 chroma)&lt;/Description&gt;\n    &lt;InputDescriptor&gt;CIE-XYZ, D65 white (scaled [0,1])&lt;/InputDescriptor&gt;\n    &lt;OutputDescriptor&gt;CIELAB L*, a*, b* (scaled by 1/100, neutrals at 0.0 \n        chroma)&lt;/OutputDescriptor&gt;\n    &lt;Matrix inBitDepth=\"16f\" outBitDepth=\"16f\"&gt;\n        &lt;Array dim=\"3 3\"&gt;\n            1.052126639 0.000000000 0.000000000\n            0.000000000 1.000000000 0.000000000\n            0.000000000 0.000000000 0.918224951\n        &lt;/Array&gt;\n    &lt;/Matrix&gt;\n    &lt;Exponent inBitDepth=\"16f\" outBitDepth=\"16f\" style=\"monCurveRev\"&gt;\n        &lt;ExponentParams exponent=\"3.0\" offset=\"0.16\" /&gt;\n    &lt;/Exponent&gt;\n    &lt;Matrix inBitDepth=\"16f\" outBitDepth=\"16f\"&gt;\n        &lt;Array dim=\"3 3\"&gt;\n            0.00000000  1.00000000  0.00000000\n            4.31034483 -4.31034483  0.00000000\n            0.00000000  1.72413793 -1.72413793\n        &lt;/Array&gt;\n    &lt;/Matrix&gt;\n&lt;/ProcessList&gt;\n</code></pre> <p>Example 15. CIE XYZ to CIELAB</p>"},{"location":"specifications/clf/#appendices","title":"Appendices","text":""},{"location":"specifications/clf/#appendix-interpolation","title":"Appendix A: Interpolation","text":"<p>When an input value falls between sampled positions in a LUT, the output value must be calculated as a proportion of the distance along some function that connects the nearest surrounding values in the LUT. There are many different types of interpolation possible, but only three types of interpolation are currently specified for use with the Common LUT Format (CLF). </p> <p>The first interpolation type, linear, is specified for use with a <code>LUT1D</code> node. The other two, trilinear and tetrahedral interpolation, are specified for use with a <code>LUT3D</code> node.</p> <p></p>"},{"location":"specifications/clf/#linear-interpolation","title":"Linear Interpolation","text":"<p>With a table of the sampled input values in \\(inValue[i]\\) where \\(i\\) ranges from \\(0\\) to \\((n-1)\\), and a table of the corresponding output values in \\(outValue[j]\\) where \\(j\\) is equal to \\(i\\),</p> index \\(i\\) inValue index \\(j\\) outValue 0 0 0 0 \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(n - 1\\) 1 \\(n - 1\\) 1000 <p>the \\(output\\) resulting from \\(input\\) can be calculated after finding the nearest \\(inValue[i] &lt; input\\). </p> <p>When \\(inValue[i] = input\\), the result is evaluated directly.</p> \\[ output = \\dfrac{input-inValue[i]}{inValue[i+1]-inValue[i]} \\times (outValue[j+1]-outValue[j])+outValue[j] \\] <p></p>"},{"location":"specifications/clf/#trilinear-interpolation","title":"Trilinear Interpolation","text":"<p>Trilinear interpolation implements linear interpolation in three-dimensions by successively interpolating each direction. </p> <p> </p> Figure 3 - Illustration of a sampled point located within a basic 3D LUT mesh grid (left) and the same point but with only the vertices surrounding the sampled point (right). <p> </p> Figure 4 - Labeling the mesh points surrounding the sampled point (r,g,b). <p>Note</p> <p>The convention used for notation is uppercase variables for mesh points and lowercase variables for points on the grid.</p> <p>Consider a sampled point as depicted in Figure 4. Let \\(V(r,g,b)\\) represent the value at the point with coordinate \\((r,g,b)\\). The distance between each node per color coordinate shows the proportion of each mesh point's color coordinate values that contribute to the sampled point.</p> \\[ \\Delta_r = \\frac{r - R_0}{R_1 - R_0} \\hspace{0.25in} \\Delta_g = \\frac{g - G_0}{G_1 - G_0} \\hspace{0.25in} \\Delta_b = \\frac{b - B_0}{B_1 - B_0} \\] <p>The general expression for trilinear interpolation can be expressed as:</p> \\[ V(r,g,b) = c_0 + c_1\\Delta_b + c_2\\Delta_r + c_3\\Delta_g + c_4\\Delta_b\\Delta_r + c_5\\Delta_r\\Delta_g + c_6\\Delta_g\\Delta_b + c_7\\Delta_r\\Delta_g\\Delta_b \\] <p>where: </p> \\[ \\begin{aligned} c_0 &amp;= V(R_0, G_0, B_0) \\\\ c_1 &amp;= V(R_0, G_0, B_1) - V(R_0, G_0, B_0) \\\\ c_2 &amp;= V(R_1, G_0, B_0) - V(R_0, G_0, B_0) \\\\ c_3 &amp;= V(R_0, G_1, B_0) - V(R_0, G_0, B_0) \\\\ c_4 &amp;= V(R_1, G_1, B_1) - V(R_1, G_0, B_0) - V(R_0, G_0, B_1) + V(R_0, G_0, B_0) \\\\ c_5 &amp;= V(R_1, G_1, B_0) - V(R_0, G_1, B_0) - V(R_1, G_0, B_0) + V(R_0, G_0, B_0) \\\\ c_6 &amp;= V(R_0, G_1, B_1) - V(R_1, G_1, B_0) - V(R_0, G_0, B_1) + V(R_0, G_0, B_0) \\\\ c_7 &amp;= V(R_1, G_1, B_1) - V(R_1, G_1, B_0) - V(R_0, G_1, B_1) - V(R_1, G_0, B_1) \\\\  &amp;+ V(R_0, G_0, B_1) + V(R_0, G_1, B_0) + V(R_1, G_0, B_0) - V(R_0, G_0, B_0)  \\end{aligned} \\] <p>Expressed in matrix form:</p> \\[ \\begin{gather} \\mathbf{C} = [c_0 \\enspace c_1 \\enspace c_2 \\enspace c_3 \\enspace c_4 \\enspace c_5 \\enspace c_6 \\enspace c_7]^T \\\\ \\mathbf{\\Delta} = [1 \\quad \\Delta_b \\quad \\Delta_r \\quad \\Delta_g \\quad \\Delta_b\\Delta_r \\quad \\Delta_r\\Delta_g \\quad \\Delta_g\\Delta_b \\quad \\Delta_r\\Delta_g\\Delta_b]^T \\\\ V(r,g,b) = \\mathbf{C}^T \\mathbf{\\Delta} \\end{gather} \\] \\[ \\begin{bmatrix} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ c_4 \\\\ c_5 \\\\ c_6 \\\\ c_7 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 &amp; 0 &amp; -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; -1 &amp; -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; -1 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 1 &amp; 1 &amp; -1 &amp; 1 &amp; -1 &amp; -1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} V(R_0, G_0, B_0) \\\\ V(R_0, G_1, B_0) \\\\ V(R_1, G_0, B_0) \\\\ V(R_1, G_1, B_0) \\\\ V(R_0, G_0, B_1) \\\\ V(R_0, G_1, B_1) \\\\ V(R_1, G_0, B_1) \\\\ V(R_1, G_1, B_1) \\end{bmatrix}     \\] <p>The expression in above can be written as: \\(\\mathbf{C} = \\mathbf{A}\\mathbf{V}\\).</p> <p>Trilinear interpolation shall be done according to \\(V(r,g,b) = \\mathbf{C}^T \\mathbf{\\Delta} = \\mathbf{V}^T \\mathbf{A}^T \\mathbf{\\Delta}\\).</p> <p>Note</p> <p>The term \\(\\mathbf{V}^T \\mathbf{A}^T\\) does not depend on the variable \\((r,g,b)\\) and thus can be computed in advance for optimization. Each sub-cube can have the values of the vector \\(\\mathbf{C}\\) already stored in memory. Therefore the algorithm can be summarized as:</p> <ol> <li>Find the sub-cube containing the point \\((r,g,b)\\)</li> <li>Select the vector \\(\\mathbf{C}\\) corresponding to that sub-cube</li> <li>Compute \\(\\Delta_r\\), \\(\\Delta_g\\), \\(\\Delta_b\\)</li> <li>Return \\(V(r,g,b) = \\mathbf{C}^T \\mathbf{\\Delta}\\)</li> </ol> <p></p>"},{"location":"specifications/clf/#tetrahedral-interpolation","title":"Tetrahedral Interpolation}","text":"<p>Tetrahedral interpolation subdivides the cubelet defined by the vertices surrounding a sampled point into six tetrahedra by segmenting along the main (and usually neutral) diagonal (Figure 5). </p> <p> </p> Figure 5 - Illustration of the six subdivided tetrahedra. <p>To find the tetrahedron containing the point \\((r,g,b)\\):</p> <ul> <li>if \\(\\Delta_b &gt; \\Delta_r &gt; \\Delta_g\\), then use the first tetrahedron, \\(t1\\)</li> <li>if \\(\\Delta_b &gt; \\Delta_g &gt; \\Delta_r\\), then use the first tetrahedron, \\(t2\\)</li> <li>if \\(\\Delta_g &gt; \\Delta_b &gt; \\Delta_r\\), then use the first tetrahedron, \\(t3\\)</li> <li>if \\(\\Delta_r &gt; \\Delta_b &gt; \\Delta_g\\), then use the first tetrahedron, \\(t4\\)</li> <li>if \\(\\Delta_r &gt; \\Delta_g &gt; \\Delta_b\\), then use the first tetrahedron, \\(t5\\)</li> <li>else, use the sixth tetrahedron, \\(t6\\)</li> </ul> <p>The matrix notation is:</p> \\[ \\mathbf{V} = \\begin{bmatrix} V(R_0, G_0, B_0) \\\\ V(R_0, G_1, B_0) \\\\ V(R_1, G_0, B_0) \\\\ V(R_1, G_1, B_0) \\\\ V(R_0, G_0, B_1) \\\\ V(R_0, G_1, B_1) \\\\ V(R_1, G_0, B_1) \\\\ V(R_1, G_1, B_1) \\end{bmatrix}\\\\ \\\\ \\] \\[ \\mathbf{\\Delta_t} = [1 \\enspace \\Delta_b \\enspace \\Delta_r \\enspace \\Delta_g]^T \\] \\[ \\begin{aligned} \\mathbf{T}_1 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 1 \\end{bmatrix} \\hspace{0.5in} \\mathbf{T}_2 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 0 \\end{bmatrix} \\\\ \\mathbf{T}_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\hspace{0.5in} \\mathbf{T}_4 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 1 \\end{bmatrix} \\\\ \\mathbf{T}_5 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\hspace{0.5in} \\mathbf{T}_6 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\end{aligned} \\] <p>Trilinear interpolation shall be done according to:</p> \\[ \\begin{gather} V(r,g,b)_{t1} = \\mathbf{\\Delta}^T_t \\mathbf{T}_1 \\mathbf{V}\\\\ V(r,g,b)_{t2} = \\mathbf{\\Delta}^T_t \\mathbf{T}_2 \\mathbf{V}\\\\ V(r,g,b)_{t3} = \\mathbf{\\Delta}^T_t \\mathbf{T}_3 \\mathbf{V}\\\\ V(r,g,b)_{t4} = \\mathbf{\\Delta}^T_t \\mathbf{T}_4 \\mathbf{V}\\\\ V(r,g,b)_{t5} = \\mathbf{\\Delta}^T_t \\mathbf{T}_5 \\mathbf{V}\\\\ V(r,g,b)_{t6} = \\mathbf{\\Delta}^T_t \\mathbf{T}_6 \\mathbf{V} \\end{gather} \\] <p>Note</p> <p>The vectors \\(\\mathbf{T}_i \\mathbf{V}\\) for \\(i = 1,2,3,4,5,6\\) does not depend on the variable \\((r,g,b)\\) and thus can be computed in advance for optimization.</p>"},{"location":"specifications/clf/#appendix-cineon-style","title":"Appendix B: Cineon-style Log Parameters","text":"<p>When using a <code>Log</code> node, it might be desirable to conform an existing logarithmic function that uses Cineon style parameters to the parameters used by CLF. A translation from Cineon-style parameters to those used by CLF's <code>LogParams</code> element is quite straightforward using the following steps.</p> <p>Traditionally, \\(\\textrm{refWhite}\\) and \\(\\textrm{refBlack}\\) are provided as 10-bit quantities, and if they indeed are, first normalize them to floating point by dividing by 1023:</p> \\[ \\begin{align} \\textrm{refWhite} = \\frac{\\textrm{refWhite}_{10i}}{1023.0} \\\\[12pt] \\textrm{refBlack} = \\frac{\\textrm{refBlack}_{10i}}{1023.0} \\end{align} \\] <p>where subscript 10\\(i\\) indicates a 10-bit quantity.</p> <p>The density range is assumed to be:</p> \\[ \\textrm{range} = 0.002 \\times 1023.0 \\] <p>Then solve the following quantities:</p> \\[ \\begin{align} \\textrm{multFactor} =&amp; \\frac{\\textrm{range}}{\\textrm{gamma}} \\\\ \\textrm{gain} =&amp; \\frac{\\textrm{highlight} - \\textrm{shadow}}{1.0 - 10^{( MIN( \\textrm{multFactor} \\times (\\textrm{refBlack}-\\textrm{refWhite}), -0.0001)}} \\\\[6pt] \\textrm{offset} =&amp; \\ \\textrm{gain} - (\\textrm{highlight} - \\textrm{shadow}) \\\\ \\end{align} \\] <p>Where \\(MIN(x,y)\\) returns \\(x\\) if \\(x&lt;y\\), otherwise returns \\(y\\)</p> <p>The parameters for the <code>LogParams</code> element are then:</p> \\[\\begin{align}     \\texttt{base} =&amp; \\ 10.0 \\\\[6pt]     \\texttt{logSlope} =&amp; \\ \\frac{1}{\\textrm{multFactor}} \\\\[6pt]     \\texttt{logOffset} =&amp; \\ \\textrm{refWhite} \\\\[6pt]     \\texttt{linSlope} =&amp; \\ \\frac{1}{\\textrm{gain}} \\\\[6pt]     \\texttt{linOffset} =&amp; \\ \\frac{\\textrm{offset}-\\textrm{shadow}}{\\textrm{gain}} \\end{align}\\]"},{"location":"specifications/clf/#appendix-c-changes-between-v20-and-v30","title":"Appendix C: Changes between v2.0 and v3.0","text":"<ul> <li>Add <code>Log</code> ProcessNode</li> <li>Add <code>Exponent</code> ProcessNode</li> <li>Revise formulas for defining use of <code>Range</code> ProcessNode to clamp at the low or high end.</li> <li><code>IndexMaps</code> removed. Use a <code>halfDomain</code> LUT to achieve reshaping of input to a LUT.</li> <li>Move <code>ACEStransform</code> elements to <code>Info</code> element of ProcessList in main spec</li> <li>Changed syntax for <code>dim</code> attribute of <code>Array</code> when contained in a <code>Matrix</code>. Two integers are now used to define the dimensions of the matrix instead of the previous three values which defined the dimensions of the matrix and the number of color components.</li> </ul>"},{"location":"specifications/rgc/","title":"Reference Gamut Compression (RGC) - A Look Transform to bring pixel values within AP1","text":""},{"location":"specifications/rgc/#scope","title":"Scope","text":"<p>This document introduces a Reference Gamut Compression (RGC) operator, published in ACES 1.3, which may be applied to ACES image data to \u201cheal\u201d pixel values outside the AP1 gamut. The Reference Gamut Compression algorithm is intended to replace the Blue Light Artifact LMT, which is now deprecated.</p>"},{"location":"specifications/rgc/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>ACES Gamut Mapping Architecture VWG - Technical Documentation Deliverable</li> <li>ISO 17321-1:2012 - Colour characterisation of digital still cameras (DSCs) - Part 1: Stimuli, metrology and test procedures</li> <li>RP 177:1993 - SMPTE Recommended Practice - Derivation of Basic Television Color Equations</li> <li>S-2014-004: ACEScg \u2014 A Working Space for CGI Render and Compositing</li> </ul>"},{"location":"specifications/rgc/#introduction","title":"Introduction","text":"<p>A common complaint from users of ACES has been the artifacts resulting from out of gamut values in source images. These artifacts are most known for appearing in highly saturated, bright LED light sources such as police car lights, stoplights, etc - but also appear frequently in LED sources used to light scenes. In an ACES workflow, these artifacts appear at two stages - first in the conversion from camera raw RGB via an Input Transform (IDT) into ACES AP0 - and second in the conversion from ACES AP0 into ACES AP1 (ACEScg and ACEScct). These out of gamut pixel values are problematic when their negative components cause issues in compositing, and may also produce visual artifacts when viewed through an ACES Output Transform.</p> <p>A Look Modification Transform (LMT) referred to as the blue light artifact fix was created as a temporary solution, but this affected all pixels in the image, rather than just the problem areas. A new solution was needed which preserved colors within a \u201czone of trust\u201d, only altering the most saturated values.</p>"},{"location":"specifications/rgc/#specification","title":"Specification","text":"<p>ACEScg values within the AP1 gamut are positive. Values outside of the AP1 gamut are negative in one or two of their components.</p> <p>The gamut compression algorithm runs per-pixel.</p> <p>ACES RGB values shall be converted to gamut compressed ACES RGB values using the following steps.</p>"},{"location":"specifications/rgc/#step-1-convert-aces-2065-1-rgb-to-acescg-rgb","title":"Step 1 \u2013 Convert ACES 2065-1 RGB to ACEScg RGB","text":"<p>ACES 2065-1 RGB values shall be converted to ACEScg RGB values using the transformation matrix (\\(TRA_1\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 1 shows the relationship between ACES R, G, and B values and ACEScg RGB values. NPM, rounded to 10 decimal places, is derived using the color space chromaticity coordinates specified in SMPTE S 2065-1, Academy Specification S-2014-004, and the methods provided in Section 3.3 of SMPTE RP 177:1993.</p> \\[ \\begin{bmatrix}R_{ACEScg}\\\\G_{ACEScg}\\\\B_{ACEScg}\\end{bmatrix}=TRA_1\\cdot \\begin{bmatrix}R_{ACES}\\\\G_{ACES}\\\\B_{ACES}\\end{bmatrix} \\] \\[ TRA_1=\\begin{bmatrix}\\phantom{-}1.4514393161 &amp; -0.2365107469 &amp; -0.2149285693 \\\\-0.0765537734 &amp; \\phantom{-}1.1762296998 &amp; -0.0996759264\\\\\\phantom{-}0.0083161484 &amp; -0.0060324498 &amp; \\phantom{-}0.9977163014 \\\\\\end{bmatrix} \\] \\[ TRA_1 = {NPM_{AP1}}^{-1}\\cdot NPM_{AP0} \\] Equation 1"},{"location":"specifications/rgc/#step-2-calculate-a","title":"Step 2 \u2013 Calculate A","text":"<p>Calculate \\(A\\) using Equation 2.</p> \\[ A = \\text{MAX}(R_{ACEScg}, G_{ACEScg}, B_{ACEScg}) \\] <p>where:  \\(\\textrm{MAX()}\\) returns the maximum of the arguments</p> Equation 2 <p>Note</p> <p>A is used as representative of the achromatic value of the pixel.</p>"},{"location":"specifications/rgc/#step-3-calculate-dn","title":"Step 3 \u2013 Calculate d<sub>n</sub>","text":"<p>Calculate \\(d_n\\) using Equation 3.</p> \\[ d_n = \\begin{cases}\\frac{A - RGB_{ACEScg}}{\\text{ABS} \\left ( A \\right )},&amp; \\text{if } A\\neq0\\\\0,&amp; \\text{otherwise}\\end{cases} \\] <p>where :     \\(\\textrm{ABS()}\\) returns the absolute value of the argument</p> Equation 3 <p>Note</p> <p>\\(d_n\\) represents the normalized distances of the channels from the achromatic, or \u201cinverse RGB ratios\u201d.</p>"},{"location":"specifications/rgc/#step-4-calculate-dc","title":"Step 4 \u2013 Calculate d<sub>c</sub>","text":"<p>Compress \\(d_n\\) to produce \\(d_c\\)  using Equation 4.</p> \\[ d_c = \\begin{cases}d_n,&amp; \\text{if }d_n &lt; t\\\\t + \\frac{d_n-t}{\\left ( 1+d_p \\right ) ^{\\frac{1}{p}}}\\end{cases} \\] <p>where :</p> \\[ l = \\begin{bmatrix}1.147 \\\\ 1.264 \\\\ 1.312\\end{bmatrix}\\ t = \\begin{bmatrix}0.815 \\\\ 0.803 \\\\ 0.880\\end{bmatrix}\\ p = 1.2 \\] \\[ s = \\frac{l - t}{\\left (\\left (\\frac{1 - t}{l - t} \\right )^{-p} - 1\\right )^\\frac{1}{p}} \\] \\[ d_p = \\left ( \\frac{d_n - t}{s} \\right )^{p} \\] Equation 4 <p>Note</p> <p>The compression function compresses values at limit (\\(l\\)) to 1.0. This results in values beyond this limit remaining outside AP1 after compression. A function which compressed all values to within AP1 would require a significant proportion of the space between threshold (\\(t\\)) and 1.0 to be reserved for values which are highly unlikely to occur, thus compressing other values more than necessary. Limit (\\(l\\)) values have been chosen so that the encoding gamuts of all digital cinema cameras with official ACES IDTs (ARRI, RED, Canon, Sony, Panasonic) will compress to within AP1. Threshold (\\(t\\)) values have been derived from the boundary of the ACEScg values for the ColorChecker Classic 24 (as specified in ISO 17321-1). The exponent (\\(p\\)) has been set to an average value, chosen following user testing, since there is no objective measure of the correctness of this value.</p>"},{"location":"specifications/rgc/#step-5-calculate-rgbc","title":"Step 5 \u2013 Calculate RGB<sub>c</sub>","text":"<p>Calculate \\(RGB_c\\)  using Equation 5.</p> \\[ RGB_c = A - d_c \\times \\text{ABS}\\left ( A \\right ) \\] <p>where :     \\(\\textrm{ABS()}\\) returns the absolute value of the argument</p> Equation 5"},{"location":"specifications/rgc/#step-6-convert-rgbc-to-aces2065-1-rgb","title":"Step 6 \u2013 Convert RGB<sub>c</sub> to ACES2065-1 RGB","text":"<p>Gamut compressed ACEScg RGB values (\\(RGB_c\\)) shall be converted to ACES2065-1 RGB values using the transformation matrix (\\(TRA_2\\)) calculated and applied using the methods provided in Section 4 of SMPTE RP 177:1993.</p> <p>Note</p> <p>Equation 6 shows the relationship between ACEScg R, G, and B values and ACES2065-1 RGB values. NPM, rounded to 10 decimal places, is derived using the color space chromaticity coordinates specified in SMPTE S 2065-1, Academy Specification S-2014-004, and the methods provided in Section 3.3 of SMPTE RP 177.</p> \\[ \\begin{bmatrix}R_{ACES}\\\\G_{ACES}\\\\B_{ACES}\\end{bmatrix}=TRA_2\\cdot \\begin{bmatrix}R_c\\\\G_c\\\\B_c\\end{bmatrix} \\] \\[ TRA_2=\\begin{bmatrix} \\phantom{-}0.6954522414 &amp; \\phantom{-}0.1406786965 &amp; \\phantom{-}0.1638690622 \\\\ \\phantom{-}0.0447945634 &amp; \\phantom{-}0.8596711185 &amp; \\phantom{-}0.0955343182 \\\\ -0.0055258826 &amp; \\phantom{-}0.0040252103 &amp; \\phantom{-}1.0015006723 \\\\\\end{bmatrix} \\] \\[ TRA_2 = {NPM_{AP0}}^{-1} \\cdot NPM_{AP1} \\] Equation 6"},{"location":"specifications/rgc/#gamut-decompression","title":"Gamut Decompression","text":"<p>While the Reference Gamut Compression has a closed form inverse, its use is not normally recommended.</p> <p>Gamut compressed ACES RGB values may be converted back to the original ACES RGB values using the same steps as for compression above, but using Equation 4b in place of Equation 4.</p> \\[ d_c = \\begin{cases}t + s \\times \\left ( \\frac{d_p}{1-d_p} \\right ) ^{\\frac{1}{p}},&amp; \\text{if }t \\leq d_n \\leq t + s\\\\d_n,&amp; \\text{otherwise}\\end{cases} \\] Equation 4b <p>Note</p> <p>\\(l\\), \\(t\\), \\(p\\), \\(s\\) and \\(d_p\\) are as defined in Equation 4.</p>"},{"location":"specifications/rgc/#tracking","title":"Tracking","text":"<p>The Reference Gamut Compression is defined as a Look Transform (LMT) in CTL and has the following ACES Transform ID:</p> <pre><code>&lt;ACEStransformID&gt;urn:ampas:aces:transformId:v1.5:LMT.Academy.GamutCompress.a1.3.0&lt;/ACEStransformID&gt;\n</code></pre> <p>This is trackable via a lookTransform element in an AMF file. If the RCG is used in the viewing pipeline, the lookTransform will be listed in the associated AMF. If the AMF is accompanying rendered media, the applied flag should be used to track whether or not the RGC has been \u201cbaked in\u201d.</p> <p>If using the RGC in a viewing pipeline, this lookTransform should appear directly after the IDT, first in the list of any LMTs, to make sure other operations benefit from the gamut compression. </p> <p>The Transform ID should be included in any exported AMFs, with the applied flag set as appropriate, and the description set to the ACESuserName to enable proper tracking. Currently, only the Reference (i.e. static) Gamut Compression is trackable via AMF.</p>"},{"location":"specifications/rgc/#appendix-a-history-research","title":"Appendix A - History / Research","text":"<p>The Architecture Virtual Working Group, chaired by Carol Payne (Netflix) and Matthias Scharfenberg (ILM), to investigate gamut mapping in ACES began its work in January 2020, with a proposal outlining the main issue as:</p> <p>Users of ACES are experiencing problems with out of gamut colors and the resulting artifacts (loss of texture, intensification of color fringes). This issue occurs at two stages in the pipeline. </p> <ul> <li>Conversion from camera raw RGB or from the manufacturer\u2019s encoding space into ACES AP0 </li> <li>Conversion from ACES AP0 into the working color space ACES AP1</li> </ul> <p>It was acknowledged early on in the group that this artifacting can also occur in VFX/Color grading, as well as the Output Transform stages in the pipeline.</p> <p>The working group chairs set the scope:</p> <ul> <li>Propose transforms between color spaces that avoid or reduce color clipping. Solutions for this may include: </li> <li>Proposing a suitable color encoding space for digital motion-picture cameras.</li> <li>Proposing a suitable working color space.</li> <li>Propose a suitable gamut mapping/compression algorithm that performs well with wide gamut, high dynamic range, scene referred content that is robust and invertible.</li> </ul> <p>The group started out investigating the working and encoding spaces (ACES 2065-1 and ACEScg). However, it was agreed early on that although the possibility of creating a new ACES working space which mitigates common gamut issues should not be discounted, it would require a very strong case as to the benefits. Changing a core component of ACES would potentially introduce backwards compatibility issues, and would also be based only on the situation at the current time. It raised the possibility of having to change the working space repeatedly in future. Thus, the focus moved on to the third option - a suitable algorithm to solve the artifacting while maintaining as much of the current ACES standards and structure as possible. The gamut mapping approach chosen is one of compression. It deals with the ACES image data \u201cas is\u201d, and simply strives to convert that into less problematic image data. </p> <p>Based on the history above, the general working assumptions were:</p> <ul> <li>Samples are relative scene exposure values (i.e. scene-referred linear data) with no assumed min/max value range boundaries</li> <li>The gamut mapping operator is per-pixel only (i.e. not spatial or temporal)</li> </ul> <p>The stated ideals for a gamut compression algorithm were:</p> <ul> <li>Exposure invariance \u2014 \\(f(a \\cdot RGB) = a \\cdot f(RGB)\\)</li> <li>Source gamut agnosticism</li> <li>Monotonicity</li> <li>Simplicity \u2013 suited to a fast shader implementation</li> <li>Invertibility (see caveats in Appendix II)</li> <li>Colors in a \u201czone of trust\u201d will be left unaltered</li> </ul> <p>While a suitable algorithm should be able to map arbitrary gamut A into arbitrary gamut B, it should not be a requirement that all source data must be contained within gamut A. Nor is it necessarily a requirement that the output should be entirely bounded by gamut B. Indeed, allowing extreme source values to be mapped to output values close to, but not within, the target gamut means that the compression function does not need to tend to the horizontal at the boundary. This means that its inverse will not tend to the vertical, which is beneficial for invertibility.</p> <p>Because the unreal colors which occur are a result of the mismatch between a camera and human observer (among other causes) and are outliers in the residual error of a transform optimized for a subset of important \u201cmemory\u201d colors, what they \u201cshould\u201d look like is somewhat undefined. The important thing is to remap them into values which are plausible rather than \u201caccurate\u201d.</p> <p>What was determined to be outside the scope:</p> <ul> <li>Colorimetric accuracy or spectral plausibility of input device transforms (IDTs) </li> <li>Display gamut mapping. (Required modifications to the RRT/ODT will need to be addressed by a subsequent group.)</li> <li>Customizing for specific input/output gamuts</li> <li>Working in bounded or volume-based gamuts</li> <li>Actions which could limit creative choices further down the line (e.g. excessive desaturation)</li> </ul>"},{"location":"specifications/rgc/#user-testing","title":"User Testing","text":"<p>Once the working group settled on the baseline algorithm and its properties a set of targeted, small scale user tests were conducted to ensure the foundations of the work were solid. The testing was composed of two groups - VFX compositors and colorists. Between these two disciplines every major use case for the gamut compression algorithm could be tested and measured. The group gathered an open repository of test images that clearly exhibited the problem to be solved. It then derived a set of test scenarios for each group ranging from keying, blur, grain matching, hue adjustment, and more. The tests were conducted in Nuke and Resolve, on both SDR and HDR monitors. </p> <p> </p> User Testing Footage Examples <p>Overall, the results of the user testing were positive and uncovered no major issues in the algorithm functionality. 75% of compositors and 96% of colorists stated that using the algorithm helped them complete their work and achieve their creative goals. For full user testing results, please refer to the working group historical repository.</p>"},{"location":"specifications/rgc/#appendix-b-implementation-considerations","title":"Appendix B  \u2013 Implementation Considerations","text":""},{"location":"specifications/rgc/#invertibility","title":"Invertibility","text":"<p>Invertibility of the transformation is an aspect that was discussed at length. The consensus was that while an inverse transform should be defined it comes with the caveat that gamut expansion can create undesirable results when used with highly saturated pixel values, such as those added as part of graphics or CG rendered imagery As this operation is considered more of a \u201cpixel healing\u201d technical operation, inversion should not be a required part of the workflow. It is more akin to a despill after pulling a key, or a bit of sharpening on a scale operation.</p>"},{"location":"specifications/rgc/#3d-lut-approximation","title":"3D LUT Approximation","text":"<p>If there is an unavoidable requirement to implement the Reference Gamut Compression on legacy systems using a 3D lookup table (LUT) special consideration must be given to the shaper function. Common functions for implementing ACES transforms as 3D LUTs are the Log2 shaper or ACEScct. However neither of those are designed to cover the negative value range required to map the negative components of out of gamut colors into a normalized 0 to 1 domain. For a successful 3D LUT implementation the normalizing shaper function must also cover a significant range of negative values.</p> <p>Due to the residual error a 3D LUT approximation of the transform should be considered to be non-invertible.</p> <p>Systems used as part of the finishing pipeline should use a mathematical implementation of the RGC, rather than a LUT. But for preview purposes, a well constructed LUT may be acceptable, particularly if it is a concatenation of all the required transforms, such as an on-set preview LUT transforming camera log to Rec.709.</p>"},{"location":"specifications/rgc/#parametric-version","title":"Parametric Version","text":"<p>Some implementations may also choose to offer a parametric variation of the RGC. This is not officially endorsed as part of ACES, and should be treated simply as another grading operator. It should not be used as a replacement for the RGC since its parameters cannot be tracked by AMF and must instead be stored in the project files of the implementing application.</p> <p>Suggested parameter names, and default values for a parametric version are given in Section 9 of the RGC Implementation guide.</p>"},{"location":"specifications/rgc/#appendix-c-illustrations","title":"Appendix C: Illustrations","text":"Distance Limit (CIExy Chromaticity Plot) Compression Threshold and ColorChecker 24 Patches (CIExy Chromaticity Plot) Before Gamut Compression (ACES Rec. 709 Output Transform) Gamut Compression <p>Note that the red value has also been moved slightly, because although it was not negative, its normalized distance was close to 1.0, and beyond the protected threshold for that channel (0.815).</p> <p> </p> After Gamut Compression (ACES Rec. 709 Output Transform) <p> </p> Gamut Compression Polar Chromaticity Grid"},{"location":"specifications/rgc/#appendix-d-ctl-reference-implementation","title":"Appendix D: CTL Reference Implementation","text":"<pre><code>// &lt;ACEStransformID&gt;urn:ampas:aces:transformId:v1.5:LMT.Academy.ReferenceGamutCompress.a1.v1.0&lt;/ACEStransformID&gt;\n// &lt;ACESuserName&gt;ACES 1.3 Look - Reference Gamut Compress&lt;/ACESuserName&gt;\n//\n// Gamut compression algorithm to bring out-of-gamut scene-referred values into AP1\n//\n//\n// Usage:\n//  This transform is intended to be applied to AP0 data, immediately after the IDT, so\n//  that all grading or compositing operations are downstream of the compression, and\n//  therefore work only with positive AP1 values.\n//\n// Note:\n//  It is not recommended to bake the compression into VFX pulls, as it may be beneficial\n//  for compositors to have access to the unmodified image data.\n//\n//\n// Input and output: ACES2065-1\n//\n\n\nimport \"ACESlib.Transform_Common\";\n\n\n/* --- Gamut Compress Parameters --- */\n// Distance from achromatic which will be compressed to the gamut boundary\n// Values calculated to encompass the encoding gamuts of common digital cinema cameras\nconst float LIM_CYAN =  1.147;\nconst float LIM_MAGENTA = 1.264;\nconst float LIM_YELLOW = 1.312;\n// Percentage of the core gamut to protect\n// Values calculated to protect all the colors of the ColorChecker Classic 24 as given by\n// ISO 17321-1 and Ohta (1997)\nconst float THR_CYAN = 0.815;\nconst float THR_MAGENTA = 0.803;\nconst float THR_YELLOW = 0.880;\n// Aggressiveness of the compression curve\nconst float PWR = 1.2;\n\n\n// Calculate compressed distance\nfloat compress(float dist, float lim, float thr, float pwr)\n{\n    float comprDist;\n    float scl;\n    float nd;\n    float p;\n    if (dist &lt; thr) {\n        comprDist = dist; // No compression below threshold\n    }\n    else {\n        // Calculate scale factor for y = 1 intersect\n        scl = (lim - thr) / pow(pow((1.0 - thr) / (lim - thr), -pwr) - 1.0, 1.0 / pwr);\n        // Normalize distance outside threshold by scale factor\n        nd = (dist - thr) / scl;\n        p = pow(nd, pwr);\n        comprDist = thr + scl * nd / (pow(1.0 + p, 1.0 / pwr)); // Compress\n    }\n    return comprDist;\n}\n\n\nvoid main \n(\n    input varying float rIn, \n    input varying float gIn, \n    input varying float bIn, \n    input varying float aIn,\n    output varying float rOut,\n    output varying float gOut,\n    output varying float bOut,\n    output varying float aOut\n) \n{ \n    // Source values\n    float ACES[3] = {rIn, gIn, bIn};\n    // Convert to ACEScg\n    float linAP1[3] = mult_f3_f44(ACES, AP0_2_AP1_MAT);\n    // Achromatic axis\n    float ach = max_f3(linAP1);\n    // Distance from the achromatic axis for each color component aka inverse RGB ratios\n    float dist[3];\n    if (ach == 0.0) {\n        dist[0] = 0.0;\n        dist[1] = 0.0;\n        dist[2] = 0.0;\n    }\n    else {\n        dist[0] = (ach - linAP1[0]) / fabs(ach);\n        dist[1] = (ach - linAP1[1]) / fabs(ach);\n        dist[2] = (ach - linAP1[2]) / fabs(ach);\n    }\n    // Compress distance with parameterized shaper function\n    float comprDist[3] = {\n        compress(dist[0], LIM_CYAN, THR_CYAN, PWR),\n        compress(dist[1], LIM_MAGENTA, THR_MAGENTA, PWR),\n        compress(dist[2], LIM_YELLOW, THR_YELLOW, PWR)\n    };\n    // Recalculate RGB from compressed distance and achromatic\n    float comprLinAP1[3] = {\n        ach - comprDist[0] * fabs(ach),\n        ach - comprDist[1] * fabs(ach),\n        ach - comprDist[2] * fabs(ach)\n    };\n    // Convert back to ACES2065-1\n    ACES = mult_f3_f44(comprLinAP1, AP1_2_AP0_MAT);\n    // Write output\n    rOut = ACES[0];\n    gOut = ACES[1];\n    bOut = ACES[2];\n    aOut = aIn;\n}\n</code></pre>"},{"location":"system-components/organization/","title":"ACES Compenents","text":"<p>The ACES system consists of the many Academy-supplied transforms \u2013 standard color space conversions, inputs, outputs, looks, and utilities \u2013 as well as vendor or user-contributed transforms \u2013 also including color space conversions, inputs, outputs, looks, and utilities \u2013 and finally documentation and metadata files.</p>"},{"location":"system-components/organization/#github","title":"Github","text":"<p>The ACES components are distributed across a few separate Github repositories:</p> <ul> <li><code>aces-core</code><ul> <li>Intended for the core algorithms and math that is ACES </li> </ul> </li> <li><code>aces-colorspace-and-input</code><ul> <li>Intended for color space definitions and their conversion to/from ACES (or another color space)</li> <li>All Input Transforms are Color Space Transforms but not all Color Space Transforms are Input Transforms. Some ACES Color Space transforms can be used as Input Transforms, or if they convert the other direction, as Inverse Input Transforms</li> </ul> </li> <li><code>aces-output</code><ul> <li>Output Transforms structured essentially as \u201cmacros\u201d that call aces-core transforms with preset parameters</li> </ul> </li> <li><code>aces-look</code><ul> <li>Any Transform that modifies the default appearance of images through an ACES pipeline. Can be empirical or analytical Transforms.</li> </ul> </li> <li><code>aces-util</code><ul> <li>Functions that are not core essential to any of the other categories but might have use when building a pipeline or debugging (e.g. Unity). Some of these might also be called with certain preset parameters to be used as Look Transforms (e.g. ASC_CDL, exposure adjustment, etc.)</li> </ul> </li> <li><code>aces-amf</code><ul> <li>schema and example files for ACES Metadata File (AMF)</li> </ul> </li> <li><code>aces-docs</code><ul> <li>ACES documentation hosted in mkdocs</li> </ul> </li> </ul> <p>The components are divided across separate repositories to enforce the notion that the system is flexible and extensible, and that new transforms can be added without breaking compatibility with the rest of the system. The addition of a new input, output, or look transform should have no impact on the ACES System Version number. For example, if a new camera is released, an Input Transform can be developed for that camera and contributed to the aces-input repository. This is done independent of the overall ACES system and thus does not warrant a change to the current version of ACES. </p> <p>In the opposite sense, transforms need not be included in an \u201cofficial\u201d release in order to be deemed usable on an ACES project. New transforms can be considered \u201cofficial\u201d as soon as they have been reviewed and accepted into the repository. Developers can choose to keep up with adding support for new inputs or outputs however best fits into their development cycles. From time to time, we will \u201croll up\u201d these changes into a commit on the main branch to summarize the new additions and provide a milestone marker against which to compare one\u2019s available presets.</p>"},{"location":"system-components/overview/","title":"System Component Documentation Overview","text":""},{"location":"system-components/overview/#introduction","title":"Introduction","text":"<p>The ACES system is composed of many core components: encodings, image files, transforms, and metadata. ACES 1.0 was the first official release of these components. It was anticipated that at some point after initial adoption, the ACES system would require a revision to address user feedback, fix bugs and create a stable release that could have all its components standardized. In addition, ACES would always require updates to add new transforms to support new cameras, displays, or industry standards. Therefore, a specification for versioning of ACES system components and a guide to consistent naming of components is needed.</p> <p>This document describes the versioning of the various engineering components that comprise the ACES System Release. These attributes can be referenced in the ACES Metadata File to describe the exact transforms used when ACES image files were viewed, created, or modified.</p> <p>A separate document (Academy TB-2014-002) addresses naming of transforms and presentation of versioning information to end-users within the context of software or hardware implementing ACES transforms.</p>"},{"location":"tb/component-names/","title":"Preferred ACES 1.0 Component Names","text":""},{"location":"tb/component-names/#introduction","title":"Introduction","text":"<p>ACES component names have technical names that emerged from the engineering and development process. While the names make sense to the scientists, engineers and early adopters that \u201cgrew up\u201d with the system, the larger adoption community targeted for adoption by ACES 1.0 does not have the historical knowledge and context of the ACES pioneers and a large majority of that community does not have the technical training needed to understand many of the existing names.</p> <p>This Technical Bulletin documents the ACES component naming conventions as agreed to by the ACES Project Committee for the ACES 1.0 System Release.</p>"},{"location":"tb/component-names/#scope","title":"Scope","text":"<p>This Technical Bulletin documents ACES 1.0 component names. These names were settled on after extensive discussions at ACES Project Committee meetings, feedback from the field, internal discussions amongst the ACES Leadership Team and the work of the ACES User Experience Working Group. The names documented herein rationalize naming approaches between diverse technical components, are technically correct, sensible for end users, acknowledge terminology that seems to have \u201cstuck\u201d, and accommodate system evolution. Full words instead of acronyms are used where possible, and the ACES prefix was liberally used to promote a system identity.</p>"},{"location":"tb/component-names/#aces-internal-components","title":"ACES Internal Components","text":"<p>The following component groups are components that color engineers, pipeline builders, technical directors, etc. might need to know about, but end users do not need to directly address if the applications they use follow ACES User Experience Guidelines.</p>"},{"location":"tb/component-names/#color-primary-sets","title":"Color Primary Sets","text":"<ol> <li> <p>Pre-release nomenclature: SMPTE 2065-1:2012 primaries, a.k.a. \u201cACES primaries\u201d </p> <p>ACES 1.0 name: \u201cACES Primaries 0\u201d or \u201cAP0\u201d</p> </li> <li> <p>Pre-release nomenclature: ACES \u201cworking space\u201d primaries, a.k.a. \u201cRec.2020+\u201d</p> <p>ACES 1.0 name: \u201cACES Primaries 1\u201d or \u201cAP1\u201d</p> </li> </ol> <p></p> Pre-release nomenclature ACES 1.0 Name Shorthand SMPTE 2065-1:2012 primariesa.k.a. \u201cACES primaries\u201d ACES Primaries 0 AP0 ACES \u201cworking space\u201d primariesa.k.a. \u201cRec.2020+\u201d ACES Primaries 1 AP1 <p>Table 1. Color Primary Sets</p> <p></p>"},{"location":"tb/component-names/#transforms","title":"Transforms","text":"<ol> <li> <p>Pre-release nomenclature: \u201cReference Rendering Transform\u201d or \u201cRRT\u201d</p> <p>ACES 1.0 name: \u201cReference Rendering Transform\u201d or \u201cRRT\u201d</p> </li> </ol> <p>Note</p> <p>Use of Reference Rendering Transform or RRT should be deprecated in end-user documentation in favor of \"ACES Output Transform\". However, plain English explanations of the RRT may still be necessary to explain how \u201cscene referred\u201d images are prepared for viewing.</p> <p></p> Pre-release nomenclature ACES 1.0 Name Shorthand Reference Rendering TransformRRT Reference Rendering Transform RRT <p>Table 2. Reference Rendering Transform</p> <p></p>"},{"location":"tb/component-names/#aces-user-facing-components","title":"ACES User-facing Components","text":""},{"location":"tb/component-names/#encodings","title":"Encodings","text":"<p>There are five possible image encodings that can be used in ACES projects. All encodings might not be used in all workflows. The ACES prefix is used to identify the encodings as ACES components.</p> <ol> <li> <p>Pre-release nomenclature: SMPTE 2065-1:2012, a.k.a. \u201cACES\u201d </p> <p>Use: base encoding, used for exchange of full fidelity images, archiving </p> <p>ACES 1.0 name: \u201cACES2065-1\u201d</p> </li> <li> <p>Pre-release nomenclature: SMPTE 2065-1:2012 with Rec.2020+ primaries, log encoding, floating point encoding, a.k.a. \u201cACES working space\u201d</p> <p>Use: working space for color correctors using a pure logarithmic encoding, fully compatible with ACESproxy</p> <p>ACES 1.0 name: \u201cACEScc\u201d</p> </li> <li> <p>Pre-release nomenclature: N/A</p> <p>Use: alternate working space for color correctors using a logarithmic encoding with a toe, not fully compatible with ACESproxy</p> <p>ACES 1.0 name: \u201cACEScct\u201d</p> </li> <li> <p>Pre-release nomenclature: VFX-friendly encoding, i.e., linear ACES working space with Rec.2020+ primaries</p> <p>Use: working space for rendering and compositor applications</p> <p>ACES 1.0 name: \u201cACEScg\u201d</p> </li> <li> <p>Pre-release nomenclature: \u201cACES wire format\u201d, a.k.a. \u201cACESproxy,\u201d \u201cACESproxy10,\u201d \u201cACESproxy12\u201d</p> <p>Use: lightweight encoding for transmission over HD-SDI (or other production transmission schemes), on-set look management. Not intended to be stored or used in production imagery or for final color grading/master- ing.</p> <p>ACES 1.0 name: \u201cACESproxy\u201d</p> </li> </ol> <p></p> Pre-release nomenclature ACES 1.0 Name SMPTE 2065-1:2012a.k.a. \u201cACES\u201d ACES2065-1 SMPTE 2065-1:2012 with Rec.2020+ primaries, log encoding, floating point encodinga.k.a. \u201cACES working space\u201d ACEScc N/A ACEScct VFX-friendly encoding, i.e., linear ACES working space with Rec.2020+ primaries ACEScg \u201cACES wire format\u201da.k.a. \u201cACESproxy,\u201d \u201cACESproxy10,\u201d \u201cACESproxy12\u201d ACESproxy10ACESproxy12 <p>Table 3. Encodings</p> <p></p>"},{"location":"tb/component-names/#transforms_1","title":"Transforms","text":"<p>There are three basic ACES transforms that end users work with. Although the \u201cpioneers\u201d seem comfortable with the three letter acronyms, ACES 1.0 transitions to simpler terms that describe what these transforms do.</p> <ol> <li> <p>Pre-release nomenclature: Input Device Transform, a.k.a. \u201cIDT\u201d</p> <p>Use: converts digital camera native data to ACES2065</p> <p>ACES 1.0 name: \u201cACES Input Transform\u201d; Shorthand: \u201cInput Transform\u201d</p> </li> <li> <p>Pre-release nomenclature: Look Modification Transform, a.k.a. \u201cLMT\u201d </p> <p>Use: applies a global, pre-RRT look to an ACES project</p> <p>ACES 1.0 name: \u201cACES Look Transform\u201d; Shorthand: \u201cLook Transform\u201d</p> </li> <li> <p>Pre-release nomenclature: \u201cACES Viewing Transform\u201d a.k.a \"RRT+ODT\"</p> <p>Use: converts ACES2065 data to display code values</p> <p>ACES 1.0 name: \u201cACES Output Transform\u201d; Shorthand: \u201cOutput Transform\u201d</p> </li> </ol> <p></p> Pre-release nomenclature ACES 1.0 Name Shorthand Input Device TransformIDT ACES Input Transform Input Transform Look Modification TransformLMT ACES Look Transform Look Transform ACES Viewing TransformRRT + ODT ACES Output Transform Output Transform <p>Table 4. Transforms</p> <p></p>"},{"location":"tb/component-names/#containers","title":"Containers","text":"<p>Containers hold ACES image data, clip-level metadata and LUTs.</p> <ol> <li> <p>Pre-release nomenclature: SMPTE ST2065-4:2013, a.k.a. \u201cACES container,\u201d \u201cexrs\u201d</p> <p>Use: container file format for ACES2065 image data</p> <p>ACES 1.0 name: \u201cACES container\u201d or \u201cACES OpenEXR\u201d; Shorthand: \"ACES EXR\"</p> </li> <li> <p>Pre-release nomenclature: Clip-level Metadata File</p> <p>Use: container for ACES clip-level metadata container</p> <p>ACES 1.0 name: \u201cACES Metadata File\u201d; Alternate: \u201cAMF\u201d</p> </li> <li> <p>Pre-release nomenclature: Academy-ASC Common LUT Format file, a.k.a. \u201cCLF file\u201d</p> <p>Use: container for Academy-ASC Common LUT format data</p> <p>ACES 1.0 name: \u201cAcademy-ASC Common LUT Format\u201d; Alternates: \u201cCommon LUT Format,\u201d \u201cCLF file\u201d</p> </li> </ol> <p></p> Pre-release nomenclature ACES 1.0 Name Shorthand SMPTE ST2065-4:2013a.k.a \"ACES Container\" ACES ContainerACES OpenEXR ACES EXR Clip=level Metadata File ACES Metadata File AMF Academy-ASC Common LUT Format filea.k.a. \u201cCLF file\u201d Academy-ASC Common LUT Format Common LUT FormatCLF <p>Table 4. Containers</p> <p></p>"},{"location":"tb/lmt/","title":"Design, Integration and Use of ACES Look Transforms","text":""},{"location":"tb/lmt/#introduction","title":"Introduction","text":"<p>The ACES Look Transform imparts an image-wide creative \u2018look\u2019 to the appearance of ACES images. It is a component of the ACES viewing pipeline that precedes the selected ACES Output Transform. Look Transforms exist because some color manipulations can be complex, and having a pre-set for a complex look makes a colorist\u2019s work more efficient. In addition, emulation of traditional color reproduction methods such as the projection of film print requires complex interactions of colors that are better modeled in a systematic transform than by requiring a colorist to match \u2018by eye.\u2019</p> <p>The Look Transform is intended to supplement \u2013 not replace \u2013 a colorist\u2019s traditional tools for grading and manipulating images. There are three places in the viewing pipeline where production staff modifies the look of the image from the default rendering of ACES data:</p> <ul> <li> <p>Adjustments to the exposure levels or white balance of a particular shot are often done as a \u2018pre-grade.\u2019</p> </li> <li> <p>A colorist applies a grade to further refine and modify the color appearance to achieve the creative look of a shot.</p> </li> <li> <p>Finally the Look Transform provides an additional, optional tool for the colorist to manipulate ACES images and preview the result.</p> </li> </ul> <p>Thus the pre-grade and the Look Transform bracket the colorist\u2019s grading work.</p> <p>While the colorist\u2019s grading tools allow manipulation of either the overall image or of selected pieces of the image, the Look Transform is designed to work only across the overall image.</p> <p>As part of the ACES viewing pipeline, the Look Transform takes ACES color-encoded values as inputs, and outputs modified ACES-encoded values that may then be immediately processed by an Output Transform (in this case, the Output Transform appropriate for the colorist's display).</p> <p>Outside an immediate ACES viewing pipeline, the Look Transform's output can additionally (or alternatively) be saved, creating a new ACES image container file that has \u2018baked in\u2019 the effect of the Look Transform on the original image. When this new file with \u2018baked in\u2019 changes is viewed using the standard ACES viewing pipeline, the creative intent reflected in the prior application of the Look Transform to the original will be preserved.</p> <p>Look Transforms can be reused across multiple shots or even across an entire production. They are separate from an individual shot\u2019s \u2018grade\u2019 or a particular vendor\u2019s color grade file.</p> <p>Key characteristics of a well-designed Look Transform are portability across applications, and preservation (to the extent that is both possible and practical) of ACES\u2019s high dynamic range and wide color gamut while still imparting a designed, creative, target look.</p>"},{"location":"tb/lmt/#scope","title":"Scope","text":"<p>This document describes the use of ACES Look Transforms for ACES-based color management. It provides several use cases for Look Transforms, defines how Look Transforms are expressed and are carried along with clips and projects, discusses Look Transform use in the context of a workflow employing ACES-based color management, and concludes with design guidelines for Look Transforms. This document also describes optimal use of Look Transforms and suggests several ways in which a Look Transform may be designed to support flexible mastering and archiving workflows.</p>"},{"location":"tb/lmt/#references","title":"References","text":"<p>The following standards, specifications, articles, presentations, and texts are referenced in this text:</p> <ul> <li>ST 2065-1:2021 - SMPTE Standard - Academy Color Encoding Specification (ACES), Society of Motion Picture and Television Engineers, New York, US, Standard, 2021.</li> <li>Academy S-2013-001, ACESproxy - An Integer Log Encoding of ACES Data</li> <li>Academy S-2014-003, ACEScc \u2013 A Logarithmic Encoding of ACES Data for use within Color Grading Systems</li> <li>Academy S-2014-006, A Common File Format for Look-Up Tables</li> <li>Academy S-2019-001, ACES Metadata File (AMF)</li> <li>[ASC Color Decision List (ASC CDL) Transfer Functions and Interchange Syntax, ASC-CDL Release 1.2]</li> </ul>"},{"location":"tb/lmt/#look-transform-use-cases","title":"Look Transform Use Cases","text":"<p>Two styles of image modification are common in post-production: interactive modification, either across the entire frame or in isolated regions of interest, and a preset systematic modification across the entire frame. The interactive image modification is termed \u2018grading.\u2019 The ACES term for preset systematic, full-frame image modification is \u2018look modification.\u2019 Look modification is performed using a Look Transform.</p> <p>Note</p> <p>In early development of the ACES system, Look Transforms were abbreviated LMT (from 'Look Modification Transform'). Occasionally the 'LMT' acronym will still be used although the preferred terminology is 'ACES Look Transform' or simply 'Look Transform'.</p>"},{"location":"tb/lmt/#emulation-of-photochemical-processing","title":"Emulation of photochemical processing","text":"<p>Though modern grading systems are very powerful, some whole-frame color transformations are too complex for even a skilled colorist to accomplish using grading system controls. Often the complexity arises when the creative intent is to simulate, for frames captured with digital cinema cameras, the nonlinear color and exposure relationships used in film laboratory photochemical processing, especially nonstandard photochemical processing. Examples of such color transformations include:</p> <ul> <li>\u2018Bleach Bypass\u2019 emulation: modification of image color values to achieve a unique desaturated appearance mimicking projection of a print that had skipped a normal laboratory bleaching step</li> <li>Technicolor 3-strip emulation: modification of image color values to achieve a saturated, higher-contrast appearance mimicking projection of a print from Technicolor's imbibition dye transfer process (c. 1938)</li> <li>Kodak Vision 3 print film emulation: modification of image color values to achieve a reproduction of the relationship between scene exposure values and projected film imagery resulting from the use of Kodaks latest film stocks</li> </ul> <p>Figure 1 illustrates how a colorist could prepend one or more emulation Look Transforms to the RRT (which itself precedes a selected ODT), so that his or her time could be spent on sequence, shot and/or region-specific color requests from the client. The grade modifies the image first, followed by the process emulation provided by the Look Transform.</p> <p></p> <p> </p> Figure 1 <p></p>"},{"location":"tb/lmt/#systematic-color-correction-and-application-of-asc-cdl","title":"Systematic Color Correction (and application of ASC CDL)","text":"<p>The Look Transform takes as input an image in the ACES color space and yields a modified image that is still in the ACES color space. As a consequence, Look Transforms can be \u2018chained\u2019 together, one after another. Figure 2 shows a grading workflow where, prior to applying the \u2018Kodak Vision 3 emulation\u2019 Look Transform described above, the colorist applies an \u2018ASC CDL\u2019 Look Transform - very likely one whose parameter values were chosen by the cinematographer on-set to modify the default \u2018look\u2019 of emulated Kodak Vision 3 stock.</p> <p></p> <p> </p> Figure 2 <p></p> <p>Note</p> <p>The values of the ASC CDL in this case are only valid in the context of the selected \u2018Kodak Vision 3 emulation\u2019 Look Transform. If this Look Transform were removed, the ASC CDL values would no longer be valid.</p> <p>Note that the ASC CDL Look Transform incorporates a conversion from ACES to ACEScc before the ASC CDL operations will be applied, and likewise incorporates a conversion from ACEScc to ACES after the ASC CDL operations have been applied. This \u2018wrapping\u2019 of ASC CDL operations is a key capability available in the Academy/ASC Common LUT Format (CLF).</p>"},{"location":"tb/lmt/#trim-pass-grading","title":"Trim Pass Grading","text":"<p>Content today is delivered across a wide range of output devices, each of which has their own color space and characteristic brightness. Creative adjustments to the look of shots are often needed to enhance the content\u2019s appearance beyond the original creative intent. The client might desire to accentuate the difference between the results of the viewing pipeline for theatrical exhibition, the results of the viewing pipeline appropriate for home video and the results of the viewing pipeline appropriate for mobile streaming. This could be done by having three workflows that differed only in that the first had no Look Transform \u2018accentuating\u2019 the image for any nonstandard viewing environment, the second had a Look Transform just prior to the application of an Output Transform designated as appropriate for home video viewing, and the third had a Look Transform just prior to the application of an Output Transform designated as appropriate for viewing with content streamed to mobile devices, as shown in Figure 3.</p> <p></p> <p> </p> Figure 3 <p></p>"},{"location":"tb/lmt/#flexible-pre-sets-for-creative-modifications","title":"Flexible pre-sets for creative modifications","text":"<p>Separation of grading and Look Transform(s) allows for a production to make significant changes in creative decisions that affect the entire frame equally, without requiring the colorist to start from scratch, or ideally without even requiring a trim pass. For example, the client might start a production shooting \u2018day for night\u2019 and use a Look Transform to accomplish this result (Figure 4).</p> <p></p> <p> </p> Figure 4 <p></p> <p>A change in creative direction (say, after a test screening) might place the captured action two hours earlier, so \u2018day for night\u2019 might become \u2018day for dusk\u2019. Since the Look Transform is separate from the grade, the change may be made without requiring lengthy and expensive colorist intervention. A new Look Transform is simply swapped into the old Look Transform\u2019s place (Figure 5).</p> <p></p> <p> </p> Figure 5 <p></p>"},{"location":"tb/lmt/#permanent-color-modification-archival","title":"Permanent Color Modification (Archival)","text":"<p>The workflows above all show a \u2018transient\u2019 processing of image file to displayed output, with the display being a calibrated grading monitor or projector. It is also completely valid and correct to archive the input to the RRT as an ACES container file, \u2018baking in\u2019 the grade and any Look Transform application(s), as shown in Figure 6.</p> <p></p> <p> </p> Figure 6 <p></p> <p>A person who retrieves an ACES file need not know about the grades and Look Transform(s) applied to produce the desired creative result; by virtue of being an ACES file, the image \u2018speaks for itself\u2019 when the RRT and a selected ODT are applied.</p> <p>It is extremely important that the Look Transform authors preserve as much of the dynamic range of the Look Transforms input ACES RGB relative exposure values as is possible. This provides the maximum amount of latitude for a colorist grading through the Look Transform. It also preserves the maximum amount of grading latitude for someone who later retrieves stored ACES container files created by baking in the effect of the Look Transform to the graded ACES images, when remastering for a radically different display or viewing environment (e.g. for grading on a higher-dynamic-range display than previously available). While full preservation of dynamic range and gamut is almost never possible, when faced with an engineering decision in which all other considerations are equal, the Look Transform author should choose the option that retains more of the Look Transform input's dynamic range and color gamut.</p> <p>Preserving the integrity of the ACES RGB relative exposure values representing the scene means more than just not clipping highlight luminances or very deep shadow tones. It also means avoiding severe distortions of the distributions of ACES values, such as the distortion caused by a strong \u2018gamma\u2019 operation, e.g. by a very large or very small value for one or more CDL \u2018power\u2019 parameters.</p> <p>Because Look Transforms are customizable and unique, and because it is essential to maintain the portability and archivability of an ACES project, it is always necessary to preserve the Look Transforms within any project where they are used.</p> <p>Note</p> <p>If a production wishes to preserve maximum flexibility for later remastering, it should archive the original ACES images, any clip-level metadata-bearing container encapsulating the original image files, any IDT(s), any pre-grading adjustments (see the following \u2018Look Transforms and pre-grading for Visual Effects\u2019 section), any project-wide and shot-specific grading parameters, and the Look Transform (that is, the set of all Look Transforms employed to achieve the creative result, in their proper sequence).</p>"},{"location":"tb/lmt/#portability","title":"Portability","text":"<p>Look Transforms are expressed and transported using the Common LUT Format (also known as the Academy/ASC Common LUT Format or CLF).</p> <p>The building blocks of an Look Transform include basic arithmetical operations, simple matrix application, 1D LUTs and 3D LUTs. Straightforward color transforms can often be expressed analytically using the first three of these building blocks. More complex (and typically empirically derived) Look Transforms may be conveyed as 3D LUTs. The Common LUT Format was chosen because it can express, in a portable encoding, all of the above-mentioned operations and LUTs.</p> <p>Note</p> <p>Using the floating point ACES RGB relative exposure values directly as 1D LUT indices requires a more complex lookup mechanism than found in traditional 1D LUT implementations. The Common LUT Format supports this type of lookup by using the halfDomain attribute of the LUT1D process node. See the Common LUT Format specification for more information.</p>"},{"location":"tb/lmt/#look-transforms-and-pre-grading-for-visual-effects","title":"Look Transforms and pre-grading for Visual Effects","text":"<p>In some cases, color corrections may be created prior to the colorist session in a scene-balancing \u2018pre-grade.\u2019 This allows for all shots in a sequence to share identical Look Transforms \u2018downstream\u2019 in the color modification pipeline. A motivating case would be a long sequence of daylight shots with varying color temperature. An example of this workflow, with two illustrations, is shown below. The first illustration shows what might happen at a visual effects facility that receives a number of shots that will be edited together to make up a sequence.</p> <p></p> <p> </p> Figure 7 <p></p> <p>When the visual effects are complete, the frames supplied to the colorist have both the pre-grade and the visual effect(s) \u2018baked in.\u2019 The Look Transform is not \u2018baked in\u2019 to this imagery, since it must be applied after the grade, but is instead carried as metadata, and is referenced by the ACES Metadata File.</p> <p></p> <p> </p> Figure 8 <p></p>"},{"location":"tb/lmt/#specification","title":"Specification","text":""},{"location":"tb/lmt/#inputs","title":"Inputs","text":"<p>The inputs to a Look Transform are ACES2065 RGB relative exposure values.</p>"},{"location":"tb/lmt/#outputs","title":"Outputs","text":"<p>The outputs of a Look Transform are ACES2065 RGB relative exposure values.</p>"},{"location":"tb/lmt/#working-space","title":"Working Space","text":"<p>The internal working space of a Look Transform is unrestricted. A Look Transform may transform its inputs internally into a non-ACES2065 space and perform color operations there, as long as it transforms the results of those back to ACES2065 before yielding them as outputs.</p>"},{"location":"tb/lmt/#syntax","title":"Syntax","text":"<p>Look Transforms are specified in Common LUT Format XML.</p>"},{"location":"tb/lmt/#transport","title":"Transport","text":"<p>Look Transform metadata is carried in the ACES Metadata File . Multiple Look Transforms are referenced in the <code>lookTransform</code> element and are applied in the order in which they are referenced.</p>"},{"location":"tb/lmt/#workflow-integration","title":"Workflow Integration","text":""},{"location":"tb/lmt/#application-to-entire-image-pixel-by-individual-pixel","title":"Application to entire image, pixel by individual pixel","text":"<p>As with all other ACES transforms, Look Transforms are applied across the entire image, and are not applied to any subset of the image. Similarly, as with all other ACES transforms, the outputs of the Look Transform depend solely on its inputs, with no contribution from neighboring regions of the image.</p>"},{"location":"tb/lmt/#ubiquity","title":"Ubiquity","text":"<p>The Look Transform is a key transform in the ACES system, and must be supported in any system component supporting ACES workflows that allows for image alteration. (An example of an ACES system component not allowing image alteration would be a display that interpreted its inputs as being ACESproxy values.) The ability to specify, record, transport and/or apply Look Transform(s) can be present in almost any component of an ACES-based workflow, including applications for converting native camera output to ACES imagery, on-set grading and dailies tools, color correctors, displays and CG tools.</p>"},{"location":"tb/lmt/#optimizations","title":"Optimizations","text":"<p>When there are multiple Look Transforms making up a Look Transform, the hardware or software implementing the ACES viewing pipeline may optimize the application of the Look Transform by combining the Look Transforms into a single LUT; it may also combine this Look Transform LUT with the RRT and a selected ODT.</p>"},{"location":"tb/lmt/#composite-transform-sampling-using-a-3d-lut","title":"Composite transform sampling using a 3D LUT","text":"<p>Applications may also send a lattice of sample values through some set of adjacent transforms (anything from two or more adjacent Look Transforms in the Look Transform, to the full concatenation of Look Transform plus RRT and a selected ODT) to derive a single 3D LUT.</p> <p>Artifact-free processing across the large range of ACES RGB relative exposure values requires shaper LUTs before and after the 3D LUT to minimize interpolation error.</p>"},{"location":"tb/lmt/#retention","title":"Retention","text":"<p>Look Transforms need to be saved with every clip or project, and loaded automatically into the user\u2019s application so that the creatively established Look Transform is maintained at every step of production and post-production. Every Look Transform contains, as part of its description, an ACES Transform Identifier. To maintain portability of Look Transforms, the metadata identifying them \u2013 their ACES Transform Identifiers \u2013 must be included within the ACES Metadata File, and the order in which they are referenced in the ACES Metadata File is the order in which they should be applied.</p>"},{"location":"tb/lmt/#nondestructive-preview-of-displayed-image-appearance","title":"Nondestructive preview of displayed image appearance","text":"<p>Look Transforms are one of several types of transform applied to either camera-specific files in proprietary or standard format that have been converted into ACES by an IDT, or to ACES files in OpenEXR containers. In the most general of cases, the input image data will be processed first by zero or more pre-grades, then by the grading transform, then by zero or more Look Transforms, then by the RRT, then by a selected ODT, and then displayed to the user. This most general case is shown in Figure 9.</p> <p></p> <p> </p> Figure 9 <p></p> <p>The syntax and semantics of Look Transforms are given in the Specification section above. The syntax and semantics of pre-grading and grading operations are outside the scope of this document; indeed, they are outside the scope of the ACES project itself.</p>"},{"location":"tb/lmt/#archival-of-aces-imagery-with-and-without-baked-in-grading-andor-look-transform-application","title":"Archival of ACES imagery with and without baked in grading and/or Look Transform application","text":"<p>Every archived ACES image has an implied associated displayed image, namely, the result of processing that archived ACES image with the RRT and the ODT that was used when creative approval was given. Because of this it is critical that the selected ODT (including all relevant versioning information) be archived alongside any archived ACES imagery, whether or not that ACES imagery is the product of \u2018baking in\u2019 grades and/or the application of one or more Look Transforms making up a Look Transform.</p> <p>Since many grading operations and Look Transforms may reduce the color volume in the original image to a smaller color volume that is then delivered to the RRT and a selected ODT, productions wishing to \u2018future-proof\u2019 their assets should store the original ACES files, along with all pre-grading information, grading information, the ordered set of Look Transforms that make up the Look Transform and the ODT that was selected at the time of creative approval. </p> <p>Example</p> <p>A Look Transform applying ASC CDL would be an example of such a color-volume-reducing transformation, as ASC CDL is applied in the ACEScc color space, a smaller color space than ACES.</p>"},{"location":"tb/lmt/#design","title":"Design","text":"<p>Simple Look Transforms are the best. A Look Transform should be as simple as possible while still achieving the desired modification to the displayed image.</p>"},{"location":"tb/lmt/#overall-principles","title":"Overall principles","text":""},{"location":"tb/lmt/#support-high-dynamic-range","title":"Support high dynamic range","text":"<p>ACES image data can represent luminances from far below the detectability threshold of the human visual system all the way up to luminances causing physical pain \u2013 the maximum ACES luminance is about 65,505 times as bright as a diffuse white object in the scene. A Look Transform should endeavor to preserve this luminance range as much as is possible.</p>"},{"location":"tb/lmt/#handle-a-wide-color-gamut","title":"Handle a wide color gamut","text":"<p>Creation of ACES imagery by CGI renderers, or by grading operations on live-action capture, can create ACES image values containing colors not found in nature or colors on the spectral locus that only became readily producible with the advent of the laser. A Look Transform should handle as wide a color gamut as is possible and practical, and avoid imposing arbitrary limits on hue or saturation.</p>"},{"location":"tb/lmt/#use-high-level-constructs-to-enable-optimizations","title":"Use high-level constructs to enable optimizations","text":"<p>The Common LUT Format allows the ACES system release to provide a tool chest of predefined operations. Look Transform authors should use these rather than writing their own for several reasons:</p> <ul> <li>They were developed as part of the ACES project, have well-defined semantics, and are tested.</li> <li>They obey the two principles above (support high dynamic range and handle the full color gamut) to the maximum extent possible.</li> <li>Future releases might introduce performance or quality optimizations that depend on recognition of predefined operations.</li> </ul>"},{"location":"tb/lmt/#analytic-and-empirical-approaches","title":"Analytic and empirical approaches","text":"<p>Broadly speaking, Look Transforms can be characterized as either analytic or empirical.</p>"},{"location":"tb/lmt/#analytic-look-transforms","title":"Analytic Look Transforms","text":"<p>Analytic Look Transforms usually have concise mathematical definitions. The prime example of an analytic Look Transform is probably the ACES-system-provided Look Transform that applies ASC CDL to ACES data. Another (hypothetical) analytic Look Transform might be one that changes saturation with luminance at certain hue angles. Analytic Look Transforms are typically expressed as a set of ordered mathematical operations or 1D LUT lookup operations on colors or color component values.</p>"},{"location":"tb/lmt/#empirical-look-transforms","title":"Empirical Look Transforms","text":"<p>Empirical Look Transforms usually are derived by sampling the results of some other color reproduction process, such as normal or special film processing. Empirical Look Transforms are often provided as 3D LUTs that record a regular subsampling of the results of such processes.</p> <p>A challenge arises when using ACES values to index the 3D LUT, as ACES values are radiometrically linear and have a very wide floating point range. The Common LUT Format provides for forward and inverse \u2018shaper LUT\u2019 operations that (when wrapped around an appropriately constructed 3D LUT) effectively solve this problem. Implementers should review the <code>LUT1D</code> section of the Common LUT Format specification.</p>"},{"location":"tb/lmt/#importing-looks-from-non-aces-color-managed-workflows","title":"Importing \u2018looks\u2019 from non-ACES-color-managed workflows","text":"<p>Importing a \u2018look file\u2019 or LUT from a color system not defined using ACES can be quite difficult. Such look files contain transforms to be applied in color spaces other than ACES or the ACES working space, often presume the relationship between scene and encoded values is encoded with some (possibly proprietary) power function or log function, and likely are designed to supply a display device with code values directly rather than hand off the image to an ACES Output Transform.</p> <p>For all of these reasons, it is typically better (and often much more efficient over the course of a production) to establish new Look Transforms within a workflow that is built around ACES-based color management rather than to try and mathematically translate a \u2018look file\u2019 or LUT intended for use with a workflow based on some other color management system.</p>"},{"location":"tb/white-point/","title":"Derivation of the ACES White Point CIE Chromaticity Coordinates","text":""},{"location":"tb/white-point/#introduction","title":"Introduction","text":"<p>The Academy Color Encoding System is a free, open, device-independent color management and image interchange system that can be applied to almost any current or future workflow. It was developed by hundreds of the industry\u2019s top scientists, engineers, and end users, working together under the auspices of the Academy of Motion Picture Arts and Sciences.</p> <p>The primary color encoding in the Academy Color Encoding System (ACES) is the Academy Color Encoding Specification (ACES2065-1), which is standardized in SMPTE ST 2065-1:2021<sup>1</sup>. The encoding primaries and white point are specified as CIE xy chromaticity coordinates to allow for the transformation of ACES2065-1 RGB values to and from other color spaces including CIE XYZ. Though the CIE xy chromaticity coordinates of encoding red, green, blue and white primaries are only one factor important to unambiguous color interchange<sup>2</sup>, their specification is required for the calculation of a normalized primary matrix used in color space transformations<sup>3</sup>.</p> <p>The white point used in ACES2065-1 was later adopted for use in other ACES encodings such as ACEScg, ACEScc, and ACEScct. For brevity and inclusiveness, the white point used in the various encodings will be referred to as \u201dthe ACES white point\u201d throughout the remainder of this document unless more specificity is required.</p> <p>The derivation of the ACES white point chromaticity coordinates outlined in this document is intended to help technical users of the ACES system calculate transformations to and from the various ACES encodings in as accurate a manner as possible. </p> <p>The white point of the ACES encodings does not limit the choice of sources that may be used to photograph or generate source images, nor does it dictate the white point of the reproduction. Using various techniques beyond the scope of this document, the chromaticity of the reproduction of equal ACES2065-1 red, green and blue values (ACES2065-1 R=G=B) may match the chromaticity of the ACES white point, the display calibration white point, or any other white point preferred for technical or aesthetic reasons.</p>"},{"location":"tb/white-point/#scope","title":"Scope","text":"<p>This document describes the derivation of the ACES white point CIE chromaticity coordinates and details of why the chromaticity coordinates were chosen. This document includes links to an example Python implementation of the derivation and an iPython notebook intended to help readers reproduce the referenced values.</p> <p>This document is primarily intended for those interested in understanding the details of the technical specification of ACES and the history of its development. The definition of a color space encoding\u2019s white point chromaticity coordinates is one important factor in the definition of a color managed system. The white point used in various ACES encodings does not dictate the creative white point of images created or mastered using the ACES system. It exists to enable accurate conversion to and from other color encodings such as CIE XYZ. The proper usage of the ACES white point in conversion, mastering, or reproduction are beyond the scope of this document. For example, the proper usage of the ACES white point in encoding scene colorimetry in ACES2065-1 is detailed in P-2013-001.</p>"},{"location":"tb/white-point/#references","title":"References","text":"<ul> <li>ST 2065-1:2021 - SMPTE Standard - Academy Color Encoding Specification (ACES), Society of Motion Picture and Television Engineers, New York, US, Standard, 2021.</li> <li>RP 177:1993 - SMPTE Recommended Practice - Derivation of Basic Television Color Equations, Society of Motion Picture and Television Engineers, New York, US, Standard, 1993.</li> <li>CIE 015:2018 - Colorimetry, 4th Edition, International Commission on Illumination (CIE), Vienna, Austria, Technical Report, 2018.</li> </ul>"},{"location":"tb/white-point/#derivation-of-cie-chromaticity-coordinates","title":"Derivation of CIE chromaticity coordinates","text":"<p>The CIE xy chromaticity coordinates of the ACES white point are specified in SMPTE ST 2065-1 as:</p> <p> \\(x\\) = 0.32168     \\(\\quad\\quad y\\) = 0.33767 </p> <p>The ACES white point chromaticity coordinates are derived using the following procedure:</p> <ol> <li> <p>Calculate the CIE Daylight spectral power distribution for a Correlated Color Temperature (CCT) of 6000 K over the wavelength intervals 300 nm to 830 nm in 1 nm increments as specified in CIE 15 Section 3.1.</p> </li> <li> <p>Calculate the CIE 1931 XYZ tristimulus values of the spectral power distribution as specified in CIE 15 Section 7.1.</p> </li> <li> <p>Convert the CIE XYZ values to CIE xy chromaticity coordinates as specified in CIE 15 Section 7.3.</p> </li> <li> <p>Round the CIE xy chromaticity coordinates to 5 decimal places.</p> </li> </ol> Calculating ACES white point chromaticities in python <p>A python implementation of the above procedure can be found at: https://github.com/ampas/aces-docs/blob/dev/python/TB-2018-001/aces_wp.py</p>"},{"location":"tb/white-point/#discussion","title":"Discussion","text":""},{"location":"tb/white-point/#comparison-of-the-aces-white-point-and-cie-d60","title":"Comparison of the ACES white point and CIE D<sub>60</sub>","text":"<p>Frequently, the white point associated with various ACES encodings is said to be \u2018D60\u2019<sup>4</sup><sup>5</sup><sup>6</sup>. This short-hand notation has sometimes led to confusion for those familiar with the details of how the chromaticity coordinates of the CIE D series illuminants are calculated<sup>7</sup>. The chromaticity coordinates of any CIE D series illuminant can be calculated using the equations found in Section 3 of CIE 15:2004 and reproduced in Equation 1.</p> <p></p> \\[ y_{D} = \u22123.000x^{2}_{D} + 2.870x_{D} \u2212 0.275  \\] \\[ x_D=\\begin{cases} 0.244063 + 0.09911 \\frac{10^3}{T} + 2.9678 \\frac{10^6}{T^2}  - 4.6070 \\frac{10^9}{T^3} &amp; 4,000 K \\le T \\le 7,000 K \\\\  0.237040 + 0.24748 \\frac{10^3}{T} + 1.9018 \\frac{10^6}{T^2}  - 2.0064 \\frac{10^9}{T^3} &amp; 7,000 K \\lt T \\le 25,000 K  \\end{cases} \\] <p>Equation 1. Calculation of CIE xy from CCT for CIE Daylight</p> <p></p> <p>The CIE has specified four canonical daylight illuminants (D<sub>50</sub>, D<sub>55</sub>, D<sub>65</sub> and D<sub>75</sub>). Contrary to what the names might imply, the correlated color temperature (CCT) values of these four canonical illuminants are not the nominal CCT values of 5000 K, 5500 K, 6500 K, and 7500 K. For instance, CIE D<sub>65</sub> does not have a CCT of 6500 K but rather a CCT temperature of approximately 6504 K<sup>8</sup>. The exact CCT values differ from the nominal CCT values due to a 1968 revision to \\(c2\\), the second radiation constant in Planck\u2019s blackbody radiation formula<sup>9</sup>. When the value of \\(c2\\) was changed from 0.014380 to 0.014388, it altered the CIE xy location of the Planckian locus for a blackbody. This small change to the Planckian locus\u2019s position relative to the chromaticity coordinates of the established CIE daylight locus had the effect of changing the correlated color temperature of the CIE D series illuminants ever so slightly. The precise CCT values for the established canonical CIE D series illuminants can be determined by applying Equation 2 to the nominal CCT values implied by the illuminant name. The exact CCT values of the canonical daylight illuminants are not whole numbers after the correction factor is applied, but it is common to round their values to the nearest Kelvin. The CCT values of the CIE canonical daylight illuminants before the 1968 change to \\(c2\\), after the 1968 change, and rounded to the nearest Kelvin can be found in Table 1.</p> <p></p> \\[ CCT_{new} = CCT \\times \\frac{1.4388}{1.4380} \\] <p>Equation 2. Conversion of nominal pre-1968 CCT to post-1968 CCT </p> <p></p> <p></p> CIE D Illuminant CCT before 1968 CCT current (rounded to 3 decimal places) CCT current (round to 0 decimal places) D<sub>50</sub> 5000 K 5002.782 K 5003 K D<sub>55</sub> 5500 K 5503.060 K 5503 K D<sub>65</sub> 6500 K 6503.616 K 6504 K D<sub>75</sub> 7500 K 7504.172 K 7504 K <p>Table 1. CCT of canonical CIE daylight illuminants</p> <p></p> <p>D<sub>60</sub> is not one of the four CIE canonical daylight illuminants so the exact CCT of such a daylight illuminant could be interpreted to be either approximately 6003 K (\\(6000 \\times \\frac{1.4388}{1.4380}\\)) or 6000 K. Regardless, the ACES white point chromaticity coordinates derived using the method specified in [Section 3] differs from both the chromaticity coordinates of CIE daylight with a CCT of 6003 K and CIE daylight with a CCT of 6000 K. The chromaticity coordinates of each, rounded to 5 decimal places, can be found in Table 2. As illustrated in Figure 1, the chromaticity coordinates of the ACES white point do not fall on the daylight locus nor do they match those of any CIE daylight spectral power distribution. The positions of the chromaticity coordinates in CIE Uniform Color Space (\\({u^\u2032}{v^\u2032}\\)) and the differences from the ACES chromaticity coordinates in \\(\\Delta {u^\u2032}{v^\u2032}\\) can be found in Table 3.</p> <p></p> CIE \\(x\\) CIE \\(y\\) ACES White Point 0.32168 0.33767 CIE Daylight 6000K 0.32169 0.33780 CIE Daylight 6003K 0.32163 0.33774 <p>Table 2. CIE xy chromaticity coordinates rounded to 5 decimal places</p> <p></p> <p></p> CIE \\({u^\u2032}\\) CIE \\({v^\u2032}\\) \\(\\Delta {u^\u2032}{v^\u2032}\\) ACES White Point 0.20078 0.47421 0 CIE Daylight 6000K 0.20074 0.47427 0.00008 CIE Daylight 6003K 0.20072 0.47423 0.00007 <p>Table 3. CIE \\({u^\u2032}{v^\u2032}\\) chromaticity coordinates and  \\(\\Delta {u^\u2032}{v^\u2032}\\) from the ACES white point rounded to 5 decimal places</p> <p></p> <p>Although the ACES white point chromaticity is not on either the Planckian locus or the daylight locus, the CCT of its chromaticity can still be estimated. There are a number of methods for estimating the CCT of any particular set of chromaticity coordinates [16]\u2013[19]. The results of four popular methods can be found in Table 4. Each of the methods estimates the CCT of the ACES white point to be very close to 6000 K.</p> <p></p> <p> </p> Figure 1. CIE UCS diagram with chromaticity  <p></p> <p></p> CCT Estimation Method ACES white point CCT Robertson<sup>10</sup> 5998.98 K Hernandez-Andres<sup>11</sup> 5997.26 K Ohno<sup>12</sup> 6000.04 K McCamy<sup>13</sup> 6000.41 K <p>Table 4. Estimation of the CCT of the ACES white point rounded to 2 decimal places</p> <p></p>"},{"location":"tb/white-point/#reasons-for-the-d60-like-white-point","title":"Reasons for the \u201cD<sub>60</sub>-like\u201d white point","text":"<p>The ACES white point was first specified by the Academy\u2019s ACES Project Committee in 2008 in Academy Specification S-2008-001. The details in S-2008-001 were later standardized in SMPTE ST 2065-1:2012. Prior to the release of the Academy specification the Project Committee debated various aspects of the ACES2065-1 encoding, including the exact white point, for many months. The choice of the \u201cD<sub>60</sub>-like\u201d white point was influenced heavily by discussions centered around viewer adaptation, dark surround viewing conditions, \u201ccinematic look\u201d, and preference. In the end, the Committee decided to go with a white point that was close to that of a daylight illuminant but also familiar to those with a film heritage. The white point would later be adopted for use in other encodings used in the ACES system. </p> <p>It is important to note that the ACES white point does not dictate the chromaticity of the reproduction neutral axis. Using various techniques beyond the scope of this document the chromaticity of the equal red, green and blue (ACES2065-1 \\(R=G=B\\)) may match the ACES white point, the display calibration white point, or any other white point preferred for technical or aesthetic reasons.</p> <p>The Committee felt that a white point with a chromaticity similar to that of daylight was appropriate for ACES2065-1. However, the exact CCT of the daylight was in question. Some felt D<sub>55</sub> was a reasonable choice given its historical use as the design illuminant for daylight color negative films. Others felt D<sub>65</sub> would be good choice given its use in television and computer graphics as a display calibration white point. </p> <p>Because the exact white point chromaticity would not prohibit users from achieving any reproduction white point, the Committee ultimately decided to use the less common CCT of 6000 K. This choice was based on an experiment to determine the reproduction chromaticity of projected color print film, the relative location of the white point compared to other white points commonly used in digital systems, and the general belief that imagery reproduced with the white point felt aesthetically \u201ccinematic\u201d.</p> <p>The projected color print film experiment involved simulating the exposure of a spectrally non-selective (neutral) gray scale onto color negative film, printing that negative onto a color print film, then projecting the color film onto a motion picture screen with a xenon-based film projector and measuring the colorimetry off the screen. The result of the experiment found that the CIE xy chromaticity coordinates of a projected LAD patch<sup>14</sup><sup>15</sup> through a film system were approximately \\(x = 0.32170\\) \\(y = 0.33568\\). Figure 2 shows a plot of the CIE \\({u^\u2032}{v^\u2032}\\) chromaticity coordinates of a scene neutral as reproduced by a film system compared to the CIE daylight locus and the ACES white point. </p> <p></p> <p> </p> Figure 2.  Film system print-through color reproduction of original scene neutral scale  <p>The chromaticity of the film system LAD reproduction was determined to be closest to CIE daylight with the CCT of 6000 K when the differences were calculated in CIE \\({u^\u2032}{v^\u2032}\\). A summary of the CIE \\({u^\u2032}{v^\u2032}\\) differences between CIE daylight at various CCTs and the LAD patch chromaticity are summarized in Table 5.</p> <p></p> Daylight CCT \\(\\Delta {u^\u2032}{v^\u2032}\\) from LAD chromaticity 5500 K 0.008183 5600 K 0.006619 5700 K 0.005112 5800 K 0.003676 5900 K 0.002354 6000 K 0.001360 6100 K 0.001448 6200 K 0.002442 6300 K 0.003627 6400 K 0.004836 6500 K 0.006035 <p>Table 5. CIE \\(\\Delta {u^\u2032}{v^\u2032}\\) difference between projected LAD patch and CIE Daylight CCT chromaticity coordinates round to 6 decimal places</p> <p></p> Generating the table values with python <p>An ipython notebook used to generate the values in Tables 1-5 is available at: https://github.com/ampas/aces-docs/tree/main/python/TB-2018-001/TB-2018-001_table_vals.ipynb</p>"},{"location":"tb/white-point/#reasons-why-the-aces-white-point-doesnt-match-the-cie-d60-chromaticity-coordinates","title":"Reasons why the ACES white point doesn\u2019t match the CIE D<sub>60</sub> chromaticity coordinates","text":"<p>As discussed in Section 5.2, the ACES white point was chosen to be very close to that of CIE Daylight with a CCT of 6000 K. This raises the question why the CIE chromaticity coordinates of \\(x = 0.32169\\) \\(y = 0.33780\\) were not used. The reasoning is somewhat precautionary; at the time, the exact chromaticity coordinates for the ACES white point were being debated, the ACES Project Committee was concerned about the implications the choice of any particular set of chromaticity coordinates could suggest.</p> <p>Those new to ACES can often misinterpret the specification of a set of ACES encoding white point chromaticity coordinates as a requirement that the final reproduction neutral axis chromaticity is limited to only that white point chromaticity. However, the ACES encoding white point does not dictate the chromaticity of the reproduction neutral axis. Therefore,  regardless of the chosen ACES white point chromaticity, the reproduction neutral axis may match the ACES white point, the display calibration white point, or any other white point preferred for technical or aesthetic reasons. The ACES white point chromaticity coordinates serve to aid in the understanding and, if desired, conversion of the colorimetry of ACES encoded images to any other encoding including those with a different white point.</p> <p>Just as the implication of the ACES encoding white point on reproduction can be misunderstood, the ACES Project Committee was also concerned that the ACES encoding white point might have unintended implications for image creators. Specifically, the Committee was concerned that the choice of a set of chromaticity coordinates that corresponded to a source with a defined spectral power distribution might be misunderstood to suggest that only that source could be used to illuminate the scene. For example, the Committee was concerned if the ACES white point chromaticity was chosen to match that of CIE Daylight with a CCT of 6000 K then only scenes photographed under CIE Daylight with a CCT of 6000 K would be compatible with the ACES system. In reality, ACES does not dictate the source under which movies or television shows can be photographed. ACES Input Transforms handle the re-encoding of camera images to ACES2065-1 and preserve all the technical and artistic intent behind on-set lighting choices.</p> <p>For these reasons as well as an abundance of caution, the ACES Project Committee decided it would be best to use a set of chromaticity coordinates very near those of CIE Daylight with a CCT of 6000 K but not exactly those of any easily calculated spectral power distribution.</p> <ol> <li> <p>\u201cSMPTE ST 2065-1:2012 \u2013 Academy Color Encoding Specification (ACES),\u201d Society of Motion Picture and Television Engineers, New York, US, Standard, 2012\u00a0\u21a9</p> </li> <li> <p>E. J. Giorgianni and T. E. Madden, Digital Color Management: Encoding Solutions, Second Edition. Addison-Wesley Longman Publishing Co., Inc., 2008, ISBN: 978-0-470-51244-9\u00a0\u21a9</p> </li> <li> <p>\u201cSMPTE RP 177-1993 \u2013 Derivation of Basic Television Color Equations,\u201d Society of Motion Picture and Television Engineers, New York, US, Recommended Procedure, 1993 Beverly Hills, CA, Academy Procedure, Mar. 2016\u00a0\u21a9</p> </li> <li> <p>AutoDesk. (2016). The aces workflow, [Online]. Available: https://knowledge.autodesk.com/support/maya/learn-explore/caas/CloudHelp/cloudhelp/2016/ENU/Maya/files/GUID-24E4143D-6FD1-4210-B677-3D5EEF3D3F29-htm.html (visited on 04/30/2018)\u00a0\u21a9</p> </li> <li> <p>BlackMagic Forum. (2014). Aces and color space transform white point shift problem, [Online]. Available: https://forum.blackmagicdesign.com/viewtopic.php?f=21&amp;t=66681 (visited on 04/30/2018)\u00a0\u21a9</p> </li> <li> <p>ACEScentral Forum. (2017). D60 issue - dci white gamut violated by projectors with \u201cuse white clip\u201d feature, [Online]. Available: http://acescentral.com/t/d60-issue-dci-white-gamut-violated-by-projectors-with-use-white-clip-feature/1306 (visited on 04/30/2018)\u00a0\u21a9</p> </li> <li> <p>Thomas Mansencal. (2018). \u2018\u2018D60\u201d chromaticity coordinates and spectral power distribution are incorrect., [Online]. Available: https://github.com/colour-science/colour/issues/394 (visited on 04/30/2018)\u00a0\u21a9</p> </li> <li> <p>G. Wyszecki and W. Stiles, Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition. Wiley New York, 2000\u00a0\u21a9</p> </li> <li> <p>M. Durieux, \u201cThe international practical temperature scale of 1968,\u201d Progress in Low Temperature Physics, vol. 6, no. C, pp. 405\u2013425, 1970\u00a0\u21a9</p> </li> <li> <p>A. R. Robertson, \u201cComputation of correlated color temperature and distribution temperature,\u201d Journal of the Optical Society of America, vol. 58, no. 11, pp. 1528\u20131535, 1968\u00a0\u21a9</p> </li> <li> <p>C. S. McCamy, \u201cCorrelated color temperature as an explicit function of chromaticity coordinates,\u201d Color Research &amp; Application, vol. 17, no. 2, pp. 142\u2013144, 1992\u00a0\u21a9</p> </li> <li> <p>J. Hernandez-Andres, R. L. Lee, and J. Romero, \u201cCalculating correlated color temperatures across the entire gamut of daylight and skylight chromaticities,\u201d Applied Optics, vol. 38, no. 27, pp. 5703\u20135709, 1999\u00a0\u21a9</p> </li> <li> <p>Y. Ohno, \u201cPractical use and calculation of CCT and Duv,\u201d LEUKOS - Journal of the Illuminating Engi- neering Society of North America, vol. 10, no. 1, pp. 47\u201355, 2014, ISSN: 15502716. DOI: 10.1080/15502724.2014.839020\u00a0\u21a9</p> </li> <li> <p>J. P. Pytlak and A. W. Fleischer, \u201cA simplified motion-picture laboratory control method for improved color duplication,\u201d SMPTE Journal, vol. 85, no. 10, pp. 781\u2013786, 1976, ISSN: 0036-1682. DOI: 10.5594/J07544\u00a0\u21a9</p> </li> <li> <p>Eastman Kodak Company. (2018). Laboratory aim density, [Online]. Available: https://www.kodak.com/en/motion/page/laboratory-tools-and-techniques (visited on 06/10/2022)\u00a0\u21a9</p> </li> </ol>"},{"location":"tools/","title":"Production Tools Guides","text":""},{"location":"workflows/","title":"Workflow Guides","text":""}]}